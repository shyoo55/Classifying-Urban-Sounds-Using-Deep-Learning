{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zo3kh86dzCP"
   },
   "source": [
    "# Classifying Urban sounds using Deep Learning\n",
    "**By:**\n",
    "- Shaikha Bin Ateeq \n",
    "- Alanoud Alosaimi \n",
    "- Raghad Althanyan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcV91S5OdzCS"
   },
   "source": [
    "### OverView  :\n",
    "\n",
    "Following on from the previous notebook, we identifed the following audio properties that need preprocessing to ensure consistency across the whole dataset:\n",
    "- Audio Channels\n",
    "- Sample rate\n",
    "- Bit-depth\n",
    "\n",
    "also We use Mel-Frequency Cepstral Coefficients(MFCC) and extract it from audio samples.now lets move on to split and Build our model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Sb0MQqSdzCT"
   },
   "source": [
    "**Import Libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "pkrP-ivXdzCU"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from keras.layers import Convolution2D, MaxPooling2D\n",
    "#from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout,Activation\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from sklearn.metrics import classification_report,confusion_matrix \n",
    "from keras.layers import Dense, Embedding, LSTM, Input, Flatten, Dropout, Activation, Conv1D, Conv2D,MaxPooling1D, AveragePooling1D, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "FMQ9qGE9dzCV",
    "outputId": "f698404b-36c5-4bf0-c042-93453a45b720"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCC</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -0.6859621, -0.4066782, -0.27337...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -0.92530745, -0.8827135, -0.83456916, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.5573074, -0.52561057, -0.62860227, -0.678...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.9489181, -0.8168003, -0.7987346, -0.78233...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.21790166, -0.15364559, -0.22467631, -0.20...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                MFCC        class_name\n",
       "0  [[-1.0, -1.0, -0.6859621, -0.4066782, -0.27337...          dog_bark\n",
       "1  [[-1.0, -0.92530745, -0.8827135, -0.83456916, ...  children_playing\n",
       "2  [[-0.5573074, -0.52561057, -0.62860227, -0.678...  children_playing\n",
       "3  [[-0.9489181, -0.8168003, -0.7987346, -0.78233...  children_playing\n",
       "4  [[-0.21790166, -0.15364559, -0.22467631, -0.20...  children_playing"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Df_Final.pickle','rb') as read_file:\n",
    "    df5 = pickle.load(read_file)\n",
    "    \n",
    "df5.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t64XbW2edzCW",
    "outputId": "1f077838-a10f-47fc-c4dd-a19c3986c61a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10903, 2)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "JE0yrMESbhx7"
   },
   "outputs": [],
   "source": [
    "df5 = shuffle(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "option=['Scissors','Bus','Computer_keyboard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5[~df5.class_name.isin(option)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "street_music        1000\n",
      "children_playing    1000\n",
      "jackhammer          1000\n",
      "dog_bark            1000\n",
      "air_conditioner     1000\n",
      "engine_idling       1000\n",
      "drilling            1000\n",
      "siren                929\n",
      "car_horn             429\n",
      "gun_shot             374\n",
      "Applause             300\n",
      "Fireworks            300\n",
      "Tearing              300\n",
      "Laughter             300\n",
      "Cough                243\n",
      "Microwave_oven       146\n",
      "Keys_jangling        139\n",
      "Telephone            120\n",
      "Name: class_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df5.class_name.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df5.groupby('class_name',as_index = False,group_keys=False).apply(lambda s: s.sample(120,replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df5.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "SefRsdJedzCb"
   },
   "outputs": [],
   "source": [
    "df5 = df5.rename(columns={'MFCC': 'MFCC_Padded'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "z0b7vVyhdzCc",
    "outputId": "d84179de-fbcd-49dd-b3c5-85571282f8ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCC_Padded</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.24786279, -0.24490526, -0.27155763, -0.30...</td>\n",
       "      <td>street_music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.24621098, 0.31810847, 0.3041705, 0.0594650...</td>\n",
       "      <td>air_conditioner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.6048181, 0.33277223, -0.098323286, -0.3606...</td>\n",
       "      <td>jackhammer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.87349004, -0.78541213, -0.87545437, -0.90...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.66386485, -0.39963248, -0.54435503, -1.0,...</td>\n",
       "      <td>jackhammer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10575</th>\n",
       "      <td>[[-0.56632936, -0.60961926, -0.6482681, -0.632...</td>\n",
       "      <td>siren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10576</th>\n",
       "      <td>[[-0.6008589, -0.49897188, -0.49449846, -0.513...</td>\n",
       "      <td>engine_idling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10577</th>\n",
       "      <td>[[-0.18406941, -0.18236142, -0.25557747, -0.13...</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10578</th>\n",
       "      <td>[[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10579</th>\n",
       "      <td>[[0.23915121, 0.31070983, 0.2881079, 0.2574947...</td>\n",
       "      <td>drilling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10580 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             MFCC_Padded        class_name\n",
       "0      [[-0.24786279, -0.24490526, -0.27155763, -0.30...      street_music\n",
       "1      [[0.24621098, 0.31810847, 0.3041705, 0.0594650...   air_conditioner\n",
       "2      [[0.6048181, 0.33277223, -0.098323286, -0.3606...        jackhammer\n",
       "3      [[-0.87349004, -0.78541213, -0.87545437, -0.90...  children_playing\n",
       "4      [[-0.66386485, -0.39963248, -0.54435503, -1.0,...        jackhammer\n",
       "...                                                  ...               ...\n",
       "10575  [[-0.56632936, -0.60961926, -0.6482681, -0.632...             siren\n",
       "10576  [[-0.6008589, -0.49897188, -0.49449846, -0.513...     engine_idling\n",
       "10577  [[-0.18406941, -0.18236142, -0.25557747, -0.13...          car_horn\n",
       "10578  [[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0...  children_playing\n",
       "10579  [[0.23915121, 0.31070983, 0.2881079, 0.2574947...          drilling\n",
       "\n",
       "[10580 rows x 2 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[-0.24786279, -0.24490526, -0.27155763, -0.30...\n",
       "1        [[0.24621098, 0.31810847, 0.3041705, 0.0594650...\n",
       "2        [[0.6048181, 0.33277223, -0.098323286, -0.3606...\n",
       "3        [[-0.87349004, -0.78541213, -0.87545437, -0.90...\n",
       "4        [[-0.66386485, -0.39963248, -0.54435503, -1.0,...\n",
       "                               ...                        \n",
       "10575    [[-0.56632936, -0.60961926, -0.6482681, -0.632...\n",
       "10576    [[-0.6008589, -0.49897188, -0.49449846, -0.513...\n",
       "10577    [[-0.18406941, -0.18236142, -0.25557747, -0.13...\n",
       "10578    [[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0...\n",
       "10579    [[0.23915121, 0.31070983, 0.2881079, 0.2574947...\n",
       "Name: MFCC_Padded, Length: 10580, dtype: object"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check5 = df5['MFCC_Padded']\n",
    "check5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Df_Final2.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df5, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = shuffle(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "6UlvO_tQdzCd"
   },
   "outputs": [],
   "source": [
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(df5.MFCC_Padded.tolist())\n",
    "y = np.array(df5.class_name.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhTtNNVcdzCd",
    "outputId": "4417cbd9-72e4-4f11-8da3-d6ebe906cddd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10580, 40, 174)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMFpSaLbdzCd",
    "outputId": "7f8f19b4-48c6-4f06-f942-8893ce6c1281"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10580,)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "HRAqBxUNdzCd",
    "outputId": "f38dcf95-067e-4660-eecb-57af7cb3ab1b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>street_music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drilling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>engine_idling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10575</th>\n",
       "      <td>engine_idling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10576</th>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10577</th>\n",
       "      <td>engine_idling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10578</th>\n",
       "      <td>air_conditioner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10579</th>\n",
       "      <td>siren</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10580 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "0                 siren\n",
       "1          street_music\n",
       "2              drilling\n",
       "3              car_horn\n",
       "4         engine_idling\n",
       "...                 ...\n",
       "10575     engine_idling\n",
       "10576  children_playing\n",
       "10577     engine_idling\n",
       "10578   air_conditioner\n",
       "10579             siren\n",
       "\n",
       "[10580 rows x 1 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y.head()\n",
    "aa = pd.DataFrame(y)\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "e1TwoSiSweMH"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3kuHwcgdzCd",
    "outputId": "45e872ee-811b-42b0-bedb-789af189ca9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpIFEJlHdzCe",
    "outputId": "34d76721-466b-4d6e-968a-0b0b7f36d2d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 18\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHu8HZi6dzCe"
   },
   "source": [
    "### Split the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "cV9D35yidzCe"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "X_train , X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=.25, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "euwmgqdD57KY",
    "outputId": "42625efa-d59a-4d24-e90d-d86b2f267d41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6348, 40, 174),\n",
       " (2116, 40, 174),\n",
       " (6348, 18),\n",
       " (2116, 18),\n",
       " (2116, 40, 174),\n",
       " (2116, 18))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eavw9DyExzv9",
    "outputId": "63fd8064-d654-4e5c-911f-87f4661a0b70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "LM5HPt4LdzCe"
   },
   "outputs": [],
   "source": [
    "#reshaping to shape required by CNN\n",
    "X_train2=np.reshape(X_train,(X_train.shape[0], 40,174,1))\n",
    "X_val2=np.reshape(X_val,(X_val.shape[0], 40,174,1))\n",
    "X_test2=np.reshape(X_test,(X_test.shape[0], 40,174,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBjdHqhu2Jb4",
    "outputId": "fafe32ed-4f68-4110-c77b-540820fca79c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6348, 40, 174, 1), (2116, 40, 174, 1), (6348, 18), (2116, 18))"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape, X_val2.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=X_train2.shape[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HM7GYY0DdzCf"
   },
   "source": [
    "# Experiment 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "E8X7KCk3-xnG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import InputLayer, BatchNormalization, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "lY7o0NSt0_d3"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "input_shape=X_train2.shape[1:]\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(padding=\"same\"))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=5,strides=1,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(padding=\"same\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(512,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units = num_labels, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEV7ORjLeUXp",
    "outputId": "53d54e1e-abae-4a39-99c5-a592c812fe4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 40, 174, 64)       1664      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 20, 87, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 20, 87, 128)       204928    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 44, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 44, 128)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 56320)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               14418176  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 21)                10773     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,767,125\n",
      "Trainable params: 14,767,125\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "CkL1u6wZehsd"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'],\n",
    "              optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZUjinUBendt",
    "outputId": "6ab4c6f6-a453-4f24-bc01-a462e48806b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.8458 - accuracy: 0.1028\n",
      "Epoch 00001: val_loss improved from inf to 2.55411, saving model to weights3.best.hdf5\n",
      "9/9 [==============================] - 67s 7s/step - loss: 2.8458 - accuracy: 0.1028 - val_loss: 2.5541 - val_accuracy: 0.1345\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.4930 - accuracy: 0.1873\n",
      "Epoch 00002: val_loss improved from 2.55411 to 2.33189, saving model to weights3.best.hdf5\n",
      "9/9 [==============================] - 65s 7s/step - loss: 2.4930 - accuracy: 0.1873 - val_loss: 2.3319 - val_accuracy: 0.2131\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.2436 - accuracy: 0.2675\n",
      "Epoch 00003: val_loss improved from 2.33189 to 2.08715, saving model to weights3.best.hdf5\n",
      "9/9 [==============================] - 74s 8s/step - loss: 2.2436 - accuracy: 0.2675 - val_loss: 2.0872 - val_accuracy: 0.3238\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.0469 - accuracy: 0.3270\n",
      "Epoch 00004: val_loss improved from 2.08715 to 2.00539, saving model to weights3.best.hdf5\n",
      "9/9 [==============================] - 66s 7s/step - loss: 2.0469 - accuracy: 0.3270 - val_loss: 2.0054 - val_accuracy: 0.3226\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.9000 - accuracy: 0.3802\n",
      "Epoch 00005: val_loss improved from 2.00539 to 1.90437, saving model to weights3.best.hdf5\n",
      "9/9 [==============================] - 63s 7s/step - loss: 1.9000 - accuracy: 0.3802 - val_loss: 1.9044 - val_accuracy: 0.3810\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.7559 - accuracy: 0.4298\n",
      "Epoch 00006: val_loss improved from 1.90437 to 1.83838, saving model to weights3.best.hdf5\n",
      "9/9 [==============================] - 63s 7s/step - loss: 1.7559 - accuracy: 0.4298 - val_loss: 1.8384 - val_accuracy: 0.4262\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.6165 - accuracy: 0.4754\n",
      "Epoch 00007: val_loss improved from 1.83838 to 1.81515, saving model to weights3.best.hdf5\n",
      "9/9 [==============================] - 67s 7s/step - loss: 1.6165 - accuracy: 0.4754 - val_loss: 1.8151 - val_accuracy: 0.4286\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5278 - accuracy: 0.5071\n",
      "Epoch 00008: val_loss improved from 1.81515 to 1.77287, saving model to weights3.best.hdf5\n",
      "9/9 [==============================] - 64s 7s/step - loss: 1.5278 - accuracy: 0.5071 - val_loss: 1.7729 - val_accuracy: 0.4429\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4135 - accuracy: 0.5440\n",
      "Epoch 00009: val_loss did not improve from 1.77287\n",
      "9/9 [==============================] - 69s 8s/step - loss: 1.4135 - accuracy: 0.5440 - val_loss: 1.8061 - val_accuracy: 0.4500\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.3189 - accuracy: 0.5667\n",
      "Epoch 00010: val_loss improved from 1.77287 to 1.68456, saving model to weights3.best.hdf5\n",
      "9/9 [==============================] - 69s 8s/step - loss: 1.3189 - accuracy: 0.5667 - val_loss: 1.6846 - val_accuracy: 0.5000\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.2137 - accuracy: 0.6119\n",
      "Epoch 00011: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 68s 8s/step - loss: 1.2137 - accuracy: 0.6119 - val_loss: 1.7813 - val_accuracy: 0.4833\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.1537 - accuracy: 0.6310\n",
      "Epoch 00012: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 76s 9s/step - loss: 1.1537 - accuracy: 0.6310 - val_loss: 1.7140 - val_accuracy: 0.5095\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.0472 - accuracy: 0.6571\n",
      "Epoch 00013: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 65s 7s/step - loss: 1.0472 - accuracy: 0.6571 - val_loss: 1.7373 - val_accuracy: 0.5190\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.9857 - accuracy: 0.6813\n",
      "Epoch 00014: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 63s 7s/step - loss: 0.9857 - accuracy: 0.6813 - val_loss: 1.8029 - val_accuracy: 0.5274\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.9175 - accuracy: 0.6929\n",
      "Epoch 00015: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 63s 7s/step - loss: 0.9175 - accuracy: 0.6929 - val_loss: 1.7903 - val_accuracy: 0.5286\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.8681 - accuracy: 0.7190\n",
      "Epoch 00016: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 70s 8s/step - loss: 0.8681 - accuracy: 0.7190 - val_loss: 1.8773 - val_accuracy: 0.5321\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.8416 - accuracy: 0.7234\n",
      "Epoch 00017: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 71s 8s/step - loss: 0.8416 - accuracy: 0.7234 - val_loss: 1.8639 - val_accuracy: 0.5179\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7809 - accuracy: 0.7389\n",
      "Epoch 00018: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 69s 8s/step - loss: 0.7809 - accuracy: 0.7389 - val_loss: 1.9573 - val_accuracy: 0.5155\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7533 - accuracy: 0.7425\n",
      "Epoch 00019: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 70s 7s/step - loss: 0.7533 - accuracy: 0.7425 - val_loss: 1.9390 - val_accuracy: 0.5440\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7521 - accuracy: 0.7409\n",
      "Epoch 00020: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 75s 8s/step - loss: 0.7521 - accuracy: 0.7409 - val_loss: 1.9622 - val_accuracy: 0.5357\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7114 - accuracy: 0.7627\n",
      "Epoch 00021: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 83s 9s/step - loss: 0.7114 - accuracy: 0.7627 - val_loss: 1.9979 - val_accuracy: 0.5417\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.7647\n",
      "Epoch 00022: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 82s 9s/step - loss: 0.6817 - accuracy: 0.7647 - val_loss: 2.0226 - val_accuracy: 0.5369\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7045 - accuracy: 0.7603\n",
      "Epoch 00023: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 80s 9s/step - loss: 0.7045 - accuracy: 0.7603 - val_loss: 1.9701 - val_accuracy: 0.5524\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6665 - accuracy: 0.7675\n",
      "Epoch 00024: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 72s 8s/step - loss: 0.6665 - accuracy: 0.7675 - val_loss: 2.0815 - val_accuracy: 0.5333\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6405 - accuracy: 0.7798\n",
      "Epoch 00025: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 74s 8s/step - loss: 0.6405 - accuracy: 0.7798 - val_loss: 2.0539 - val_accuracy: 0.5274\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6221 - accuracy: 0.7833\n",
      "Epoch 00026: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 69s 8s/step - loss: 0.6221 - accuracy: 0.7833 - val_loss: 2.0859 - val_accuracy: 0.5571\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6156 - accuracy: 0.7810\n",
      "Epoch 00027: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 76s 9s/step - loss: 0.6156 - accuracy: 0.7810 - val_loss: 2.1059 - val_accuracy: 0.5298\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6026 - accuracy: 0.7821\n",
      "Epoch 00028: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 64s 7s/step - loss: 0.6026 - accuracy: 0.7821 - val_loss: 2.0426 - val_accuracy: 0.5369\n",
      "Epoch 29/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6092 - accuracy: 0.7821\n",
      "Epoch 00029: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 61s 7s/step - loss: 0.6092 - accuracy: 0.7821 - val_loss: 2.1246 - val_accuracy: 0.5393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.7782\n",
      "Epoch 00030: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 62s 7s/step - loss: 0.5719 - accuracy: 0.7782 - val_loss: 2.1938 - val_accuracy: 0.5345\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5763 - accuracy: 0.7869\n",
      "Epoch 00031: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 61s 7s/step - loss: 0.5763 - accuracy: 0.7869 - val_loss: 2.1265 - val_accuracy: 0.5512\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.7988\n",
      "Epoch 00032: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.5542 - accuracy: 0.7988 - val_loss: 2.2218 - val_accuracy: 0.5274\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5651 - accuracy: 0.7893\n",
      "Epoch 00033: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 60s 7s/step - loss: 0.5651 - accuracy: 0.7893 - val_loss: 2.1066 - val_accuracy: 0.5488\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5675 - accuracy: 0.7810\n",
      "Epoch 00034: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.5675 - accuracy: 0.7810 - val_loss: 2.1414 - val_accuracy: 0.5512\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.8028\n",
      "Epoch 00035: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.5269 - accuracy: 0.8028 - val_loss: 2.3117 - val_accuracy: 0.5381\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5585 - accuracy: 0.7909\n",
      "Epoch 00036: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.5585 - accuracy: 0.7909 - val_loss: 2.1703 - val_accuracy: 0.5464\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5301 - accuracy: 0.7897\n",
      "Epoch 00037: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.5301 - accuracy: 0.7897 - val_loss: 2.1665 - val_accuracy: 0.5524\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.7917\n",
      "Epoch 00038: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.5285 - accuracy: 0.7917 - val_loss: 2.1273 - val_accuracy: 0.5571\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.7968\n",
      "Epoch 00039: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 59s 7s/step - loss: 0.5112 - accuracy: 0.7968 - val_loss: 2.2282 - val_accuracy: 0.5476\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5474 - accuracy: 0.7857\n",
      "Epoch 00040: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 66s 7s/step - loss: 0.5474 - accuracy: 0.7857 - val_loss: 2.2758 - val_accuracy: 0.5440\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5317 - accuracy: 0.7821\n",
      "Epoch 00041: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 62s 7s/step - loss: 0.5317 - accuracy: 0.7821 - val_loss: 2.1421 - val_accuracy: 0.5417\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.7944\n",
      "Epoch 00042: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.5155 - accuracy: 0.7944 - val_loss: 2.3123 - val_accuracy: 0.5488\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5093 - accuracy: 0.7933\n",
      "Epoch 00043: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.5093 - accuracy: 0.7933 - val_loss: 2.2967 - val_accuracy: 0.5345\n",
      "Epoch 44/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5099 - accuracy: 0.7893\n",
      "Epoch 00044: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 62s 7s/step - loss: 0.5099 - accuracy: 0.7893 - val_loss: 2.2424 - val_accuracy: 0.5500\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5117 - accuracy: 0.7917\n",
      "Epoch 00045: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 61s 7s/step - loss: 0.5117 - accuracy: 0.7917 - val_loss: 2.3229 - val_accuracy: 0.5440\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.4929 - accuracy: 0.7972\n",
      "Epoch 00046: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 59s 7s/step - loss: 0.4929 - accuracy: 0.7972 - val_loss: 2.3174 - val_accuracy: 0.5440\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.4988 - accuracy: 0.7976\n",
      "Epoch 00047: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.4988 - accuracy: 0.7976 - val_loss: 2.3129 - val_accuracy: 0.5560\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.7964\n",
      "Epoch 00048: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.4996 - accuracy: 0.7964 - val_loss: 2.2516 - val_accuracy: 0.5488\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.7853\n",
      "Epoch 00049: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.5025 - accuracy: 0.7853 - val_loss: 2.2525 - val_accuracy: 0.5548\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.7996\n",
      "Epoch 00050: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 61s 7s/step - loss: 0.4791 - accuracy: 0.7996 - val_loss: 2.3192 - val_accuracy: 0.5607\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.4884 - accuracy: 0.7964\n",
      "Epoch 00051: val_loss did not improve from 1.68456\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.4884 - accuracy: 0.7964 - val_loss: 2.3275 - val_accuracy: 0.5536\n",
      "Epoch 00051: early stopping\n",
      "Training completed in time:  0:55:27.740769\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 25, verbose= 1, mode='auto')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights3.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "\n",
    "model.fit(X_train2, y_train,\n",
    "          batch_size = 300, \n",
    "          epochs = 300,\n",
    "          validation_data = (X_val2, y_val), \n",
    "          callbacks=[checkpointer, es])\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training-Validation Accuracy')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#saving the model history\n",
    "loss = pd.DataFrame(model.history.history)\n",
    "\n",
    "\n",
    "#plotting the loss and accuracy \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(loss[\"loss\"], label =\"Loss\")\n",
    "plt.plot(loss[\"val_loss\"], label = \"Validation_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(loss['accuracy'],label = \"Training Accuracy\")\n",
    "plt.plot(loss['val_accuracy'], label =\"Validation_ Accuracy \")\n",
    "plt.legend()\n",
    "plt.title(\"Training-Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20  5 13 17 18 10 11 16 18 20 16  1 16  3 10  5  0 18 20 11 11  2  0 15\n",
      " 12  1  9  4 19  5  5 16  2 17 15 11 10  4  0  7  1 16 10  7 19 13  5 13\n",
      "  5 16  7  8  8 12  8  9 19  7 18  2  3 20 11  8 15  0 11 12  0  4  5  1\n",
      "  3 18  7 18  7  6  9 15 10 17  5 15 10 18  2 14 10 11  4 19 11 19  1 12\n",
      "  9  2 11  9 14  6 16  5 20 19  2  1 19  4  5  4 20 10 16  3 15 18  5 10\n",
      " 11 16 16 11 12 10 16  5  7 13 17 19 14 10  0  3 10  2  8  6  3  6  6 19\n",
      "  3  3 16 13  2  5  8  8 18  8 14 17 20 12 18 19 18 16 20  8  8  5 20  8\n",
      "  8  3 20 12  4 18  6  6  5 20 18 16  1 15 17  5 19 14 15 18  1  3  8 13\n",
      " 11  4  7 12  3  6 20 13 17  2 20 14 12  5  3  3 12  8  6  1 12 10 15  1\n",
      " 17 14  4 18 18  3 11 18 17  6 12 10  2 12 18  3 20  5 13  8 10 15  6 16\n",
      " 16  6  3 17 18 13 19 19 15 17 15 12 18 15 13  1 20  6 19 16  5 19 19  5\n",
      "  2 13  5  2 13 10 16  0 14 16  5  7 18  6  5  2 20  4  6 20  8 15 11  3\n",
      " 16  1 18 11  8 13  6 16  2 16 14  8 19  2 16  6 15  6 14 16 11  8  8  8\n",
      "  1  5 20  7  5 14  6 11 12 16 20  1  4 17  2 19  7 11 12 16 19 16 18 11\n",
      " 17  2  2 18  5  7  2 17  9  2 15 13  0  2 16 16 16 10  8 15  3  2  3 11\n",
      " 10 11  3 20  3 11  2 18  4 11  4  2  3 10 18 19 16 18 14 14 16 17 19 12\n",
      " 16  2  3  1  9 17 20 13  0 12  3  3 10  5  3  2 15 18 17 20  5  0 16 14\n",
      "  3 15  5 19  8 11  9 10  5 12  5  4 14 17  9 12 16 10 20 12  8 13  7 11\n",
      "  9  9 17  0 19 12 15 14 16 15 20 16 18  4 18  1 19 20  2 18  8 13  9 12\n",
      "  1 20  5 18  5  3  2  2 18  3 13  1  2 11 19 14  1 18  5  3 17  7  9  8\n",
      "  9 10  8 12 14 10  2  3  3 14  8  5 11  4 15 15  1  6 20 20  8 20 16 20\n",
      "  5 11  9  8  5  9  3  0  2  4  8  5  8 17 18 14 20  8  7  8  2 19  7 20\n",
      " 14 18  8  2  2 17  2  7 20  8 15 14 17 13  3  8 16 16  7  4  8 12  4  6\n",
      "  2  7  2  5  2  1  4 13 20  2  1  5 20 13  2 10 12 19  9  5  8  8 17 20\n",
      " 11  6 14  9  1 18  5  8  4  1 16 10  7 17  3  2 16 11  9 11  5  4  1 13\n",
      "  8  2  9 19  0  6  2 11 13  4  9 20 16 11  8  0 15  5  6 16 20 10  9  2\n",
      " 15  5 18 12  7 20 14 10  9  4 20 15  4 13  8 12  3  7  8 18  8 14  8  7\n",
      "  1 12  4  7  2  8 14 12  5 14 11  6 18 13 16  9  4 10  0 12 16 11 10  0\n",
      " 14  4  3  0  6 13 14 17  3  6  8  8 10  2 14 17 11 19 15 20 18 20 17 13\n",
      "  3  1  4 14 10  5 14 20 11 13  7  8  9  3  5  1  8 19 18  5 16  0 13 20\n",
      " 14 20  2 14  8 14  8 18  3  2  4 10 10 18  6  2 11  0 20  6 18  6 12  2\n",
      "  4  6 10  2  7 16  2  3 12  8  8 14 10 12  3  1 17  1  1  1 15 14 18  1\n",
      "  9 17  8  5 18 10 13 11  3 15  9 20  9 16 15 20  4 14  6 17 19 10  4  5\n",
      "  0 10 14  6  6 16 11  7 19 18 15  7 20 12 20  9  4 18 10 19  6 15 16  6\n",
      "  8 15  1  5 13 17  6 20 17  7  4 15  1 11 19  5  1  7 14 15  8  7  9  6]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_val2)\n",
    "\n",
    "# finding class with larget predicted probability using argmax of numpy \n",
    "y_pred = np.argmax(prediction, axis = 1)  # prediction using model \n",
    "y_val_orig = np.argmax(y_val, axis = 1) # original y_val\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.04      0.06        45\n",
      "           1       0.56      0.45      0.50        44\n",
      "           2       0.36      0.47      0.41        40\n",
      "           3       0.30      0.34      0.32        38\n",
      "           4       0.29      0.24      0.26        42\n",
      "           5       0.25      0.45      0.32        29\n",
      "           6       0.24      0.24      0.24        37\n",
      "           7       0.48      0.33      0.39        45\n",
      "           8       0.25      0.54      0.34        28\n",
      "           9       0.50      0.29      0.37        52\n",
      "          10       0.33      0.35      0.34        37\n",
      "          11       0.68      0.82      0.75        34\n",
      "          12       0.91      0.97      0.94        33\n",
      "          13       0.61      0.54      0.58        35\n",
      "          14       0.78      0.63      0.70        49\n",
      "          15       0.86      0.67      0.76        46\n",
      "          16       0.90      0.85      0.88        54\n",
      "          17       0.94      0.94      0.94        33\n",
      "          18       0.88      0.91      0.90        47\n",
      "          19       0.76      0.81      0.79        32\n",
      "          20       0.67      0.85      0.75        40\n",
      "\n",
      "    accuracy                           0.55       840\n",
      "   macro avg       0.55      0.56      0.55       840\n",
      "weighted avg       0.56      0.55      0.55       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_orig, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "model2.add(Conv2D(128, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\n",
    "model2.add(MaxPooling2D(padding=\"same\"))\n",
    "\n",
    "model2.add(Conv2D(128,kernel_size=5,strides=1,padding=\"same\",activation=\"relu\"))\n",
    "model2.add(MaxPooling2D(padding=\"same\"))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "model2.add(Dense(512,activation=\"relu\"))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(512,activation=\"relu\"))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model2.add(Dense(512,activation=\"relu\"))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model2.add(Dense(units = num_labels, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 40, 174, 128)      3328      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 20, 87, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 20, 87, 128)       409728    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 10, 44, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 10, 44, 128)       0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 56320)             0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 512)               28836352  \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 21)                10773     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,785,493\n",
      "Trainable params: 29,785,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'],\n",
    "              optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.7756 - accuracy: 0.1198 \n",
      "Epoch 00001: val_loss improved from inf to 2.63168, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 120s 13s/step - loss: 2.7756 - accuracy: 0.1198 - val_loss: 2.6317 - val_accuracy: 0.1869\n",
      "Epoch 2/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.6487 - accuracy: 0.1389 \n",
      "Epoch 00002: val_loss improved from 2.63168 to 2.46612, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 120s 13s/step - loss: 2.6487 - accuracy: 0.1389 - val_loss: 2.4661 - val_accuracy: 0.2060\n",
      "Epoch 3/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.5071 - accuracy: 0.1575 \n",
      "Epoch 00003: val_loss improved from 2.46612 to 2.31954, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 117s 13s/step - loss: 2.5071 - accuracy: 0.1575 - val_loss: 2.3195 - val_accuracy: 0.2131\n",
      "Epoch 4/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.3870 - accuracy: 0.2067 \n",
      "Epoch 00004: val_loss improved from 2.31954 to 2.27171, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 116s 13s/step - loss: 2.3870 - accuracy: 0.2067 - val_loss: 2.2717 - val_accuracy: 0.2452\n",
      "Epoch 5/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.2820 - accuracy: 0.2337 \n",
      "Epoch 00005: val_loss improved from 2.27171 to 2.18695, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 117s 13s/step - loss: 2.2820 - accuracy: 0.2337 - val_loss: 2.1870 - val_accuracy: 0.2964\n",
      "Epoch 6/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.1992 - accuracy: 0.2655 \n",
      "Epoch 00006: val_loss improved from 2.18695 to 2.08607, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 118s 13s/step - loss: 2.1992 - accuracy: 0.2655 - val_loss: 2.0861 - val_accuracy: 0.3131\n",
      "Epoch 7/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.0704 - accuracy: 0.3151 \n",
      "Epoch 00007: val_loss improved from 2.08607 to 2.03036, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 119s 13s/step - loss: 2.0704 - accuracy: 0.3151 - val_loss: 2.0304 - val_accuracy: 0.3131\n",
      "Epoch 8/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.9826 - accuracy: 0.3496 \n",
      "Epoch 00008: val_loss improved from 2.03036 to 1.93243, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 120s 13s/step - loss: 1.9826 - accuracy: 0.3496 - val_loss: 1.9324 - val_accuracy: 0.3464\n",
      "Epoch 9/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.9121 - accuracy: 0.3583 \n",
      "Epoch 00009: val_loss improved from 1.93243 to 1.92020, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 119s 13s/step - loss: 1.9121 - accuracy: 0.3583 - val_loss: 1.9202 - val_accuracy: 0.3726\n",
      "Epoch 10/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.8040 - accuracy: 0.3956 \n",
      "Epoch 00010: val_loss improved from 1.92020 to 1.83915, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 116s 13s/step - loss: 1.8040 - accuracy: 0.3956 - val_loss: 1.8391 - val_accuracy: 0.3857\n",
      "Epoch 11/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.7373 - accuracy: 0.4210 \n",
      "Epoch 00011: val_loss improved from 1.83915 to 1.82566, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 118s 13s/step - loss: 1.7373 - accuracy: 0.4210 - val_loss: 1.8257 - val_accuracy: 0.4155\n",
      "Epoch 12/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.6696 - accuracy: 0.4429 \n",
      "Epoch 00012: val_loss improved from 1.82566 to 1.78025, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 118s 13s/step - loss: 1.6696 - accuracy: 0.4429 - val_loss: 1.7802 - val_accuracy: 0.4143\n",
      "Epoch 13/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5765 - accuracy: 0.4651 \n",
      "Epoch 00013: val_loss improved from 1.78025 to 1.76464, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 120s 13s/step - loss: 1.5765 - accuracy: 0.4651 - val_loss: 1.7646 - val_accuracy: 0.4060\n",
      "Epoch 14/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5197 - accuracy: 0.4937 \n",
      "Epoch 00014: val_loss improved from 1.76464 to 1.74867, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 118s 13s/step - loss: 1.5197 - accuracy: 0.4937 - val_loss: 1.7487 - val_accuracy: 0.4357\n",
      "Epoch 15/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356 - accuracy: 0.5349 \n",
      "Epoch 00015: val_loss did not improve from 1.74867\n",
      "9/9 [==============================] - 117s 13s/step - loss: 1.4356 - accuracy: 0.5349 - val_loss: 1.7689 - val_accuracy: 0.4702\n",
      "Epoch 16/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4006 - accuracy: 0.5425 \n",
      "Epoch 00016: val_loss improved from 1.74867 to 1.68630, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 117s 13s/step - loss: 1.4006 - accuracy: 0.5425 - val_loss: 1.6863 - val_accuracy: 0.4952\n",
      "Epoch 17/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.3317 - accuracy: 0.5639 \n",
      "Epoch 00017: val_loss did not improve from 1.68630\n",
      "9/9 [==============================] - 118s 13s/step - loss: 1.3317 - accuracy: 0.5639 - val_loss: 1.7681 - val_accuracy: 0.4774\n",
      "Epoch 18/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.2313 - accuracy: 0.5873 \n",
      "Epoch 00018: val_loss did not improve from 1.68630\n",
      "9/9 [==============================] - 116s 13s/step - loss: 1.2313 - accuracy: 0.5873 - val_loss: 1.7058 - val_accuracy: 0.5214\n",
      "Epoch 19/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.1922 - accuracy: 0.6159 \n",
      "Epoch 00019: val_loss did not improve from 1.68630\n",
      "9/9 [==============================] - 121s 13s/step - loss: 1.1922 - accuracy: 0.6159 - val_loss: 1.7259 - val_accuracy: 0.5131\n",
      "Epoch 20/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.1577 - accuracy: 0.6226 \n",
      "Epoch 00020: val_loss improved from 1.68630 to 1.67937, saving model to weights4.best.hdf5\n",
      "9/9 [==============================] - 116s 13s/step - loss: 1.1577 - accuracy: 0.6226 - val_loss: 1.6794 - val_accuracy: 0.5119\n",
      "Epoch 21/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.0965 - accuracy: 0.6552 \n",
      "Epoch 00021: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 121s 14s/step - loss: 1.0965 - accuracy: 0.6552 - val_loss: 1.7596 - val_accuracy: 0.5262\n",
      "Epoch 22/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.0610 - accuracy: 0.6560 \n",
      "Epoch 00022: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 120s 13s/step - loss: 1.0610 - accuracy: 0.6560 - val_loss: 1.7545 - val_accuracy: 0.5321\n",
      "Epoch 23/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.0143 - accuracy: 0.6730 \n",
      "Epoch 00023: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 117s 13s/step - loss: 1.0143 - accuracy: 0.6730 - val_loss: 1.7290 - val_accuracy: 0.5381\n",
      "Epoch 24/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.9991 - accuracy: 0.6817 \n",
      "Epoch 00024: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 116s 13s/step - loss: 0.9991 - accuracy: 0.6817 - val_loss: 1.7799 - val_accuracy: 0.5262\n",
      "Epoch 25/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.9345 - accuracy: 0.7044 \n",
      "Epoch 00025: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 116s 13s/step - loss: 0.9345 - accuracy: 0.7044 - val_loss: 1.8624 - val_accuracy: 0.5286\n",
      "Epoch 26/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.9398 - accuracy: 0.6980 \n",
      "Epoch 00026: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 116s 13s/step - loss: 0.9398 - accuracy: 0.6980 - val_loss: 1.7897 - val_accuracy: 0.5250\n",
      "Epoch 27/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.9328 - accuracy: 0.7083 \n",
      "Epoch 00027: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 118s 13s/step - loss: 0.9328 - accuracy: 0.7083 - val_loss: 1.8229 - val_accuracy: 0.5381\n",
      "Epoch 28/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.9238 - accuracy: 0.7179 \n",
      "Epoch 00028: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 117s 13s/step - loss: 0.9238 - accuracy: 0.7179 - val_loss: 1.8932 - val_accuracy: 0.5298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.8853 - accuracy: 0.7079 \n",
      "Epoch 00029: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 119s 13s/step - loss: 0.8853 - accuracy: 0.7079 - val_loss: 1.8639 - val_accuracy: 0.5310\n",
      "Epoch 30/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.8736 - accuracy: 0.7317 \n",
      "Epoch 00030: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 118s 13s/step - loss: 0.8736 - accuracy: 0.7317 - val_loss: 1.8507 - val_accuracy: 0.5440\n",
      "Epoch 31/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.8508 - accuracy: 0.7238 \n",
      "Epoch 00031: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 117s 13s/step - loss: 0.8508 - accuracy: 0.7238 - val_loss: 1.8489 - val_accuracy: 0.5464\n",
      "Epoch 32/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.8397 - accuracy: 0.7194 \n",
      "Epoch 00032: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 118s 13s/step - loss: 0.8397 - accuracy: 0.7194 - val_loss: 1.9667 - val_accuracy: 0.5333\n",
      "Epoch 33/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7865 - accuracy: 0.7393 \n",
      "Epoch 00033: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 120s 13s/step - loss: 0.7865 - accuracy: 0.7393 - val_loss: 1.8433 - val_accuracy: 0.5488\n",
      "Epoch 34/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7750 - accuracy: 0.7448 \n",
      "Epoch 00034: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 120s 13s/step - loss: 0.7750 - accuracy: 0.7448 - val_loss: 1.9794 - val_accuracy: 0.5417\n",
      "Epoch 35/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7596 - accuracy: 0.7591 \n",
      "Epoch 00035: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 119s 13s/step - loss: 0.7596 - accuracy: 0.7591 - val_loss: 1.9064 - val_accuracy: 0.5333\n",
      "Epoch 36/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7618 - accuracy: 0.7492 \n",
      "Epoch 00036: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 117s 13s/step - loss: 0.7618 - accuracy: 0.7492 - val_loss: 2.0021 - val_accuracy: 0.5560\n",
      "Epoch 37/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7553 - accuracy: 0.7556 \n",
      "Epoch 00037: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 119s 13s/step - loss: 0.7553 - accuracy: 0.7556 - val_loss: 1.9459 - val_accuracy: 0.5405\n",
      "Epoch 38/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7381 - accuracy: 0.7540 \n",
      "Epoch 00038: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 118s 13s/step - loss: 0.7381 - accuracy: 0.7540 - val_loss: 1.9054 - val_accuracy: 0.5417\n",
      "Epoch 39/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7134 - accuracy: 0.7730 \n",
      "Epoch 00039: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 120s 13s/step - loss: 0.7134 - accuracy: 0.7730 - val_loss: 2.0723 - val_accuracy: 0.5452\n",
      "Epoch 40/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.7243 - accuracy: 0.7556 \n",
      "Epoch 00040: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 121s 13s/step - loss: 0.7243 - accuracy: 0.7556 - val_loss: 1.9720 - val_accuracy: 0.5488\n",
      "Epoch 41/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6972 - accuracy: 0.7679 \n",
      "Epoch 00041: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 119s 13s/step - loss: 0.6972 - accuracy: 0.7679 - val_loss: 2.0266 - val_accuracy: 0.5393\n",
      "Epoch 42/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6833 - accuracy: 0.7718 \n",
      "Epoch 00042: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 119s 13s/step - loss: 0.6833 - accuracy: 0.7718 - val_loss: 2.1331 - val_accuracy: 0.5262\n",
      "Epoch 43/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.7694 \n",
      "Epoch 00043: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 119s 13s/step - loss: 0.6951 - accuracy: 0.7694 - val_loss: 2.0061 - val_accuracy: 0.5357\n",
      "Epoch 44/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6989 - accuracy: 0.7698  \n",
      "Epoch 00044: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 1936s 240s/step - loss: 0.6989 - accuracy: 0.7698 - val_loss: 2.1375 - val_accuracy: 0.5417\n",
      "Epoch 45/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.7679 \n",
      "Epoch 00045: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 130s 15s/step - loss: 0.6884 - accuracy: 0.7679 - val_loss: 1.9279 - val_accuracy: 0.5345\n",
      "Epoch 46/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6769 - accuracy: 0.7710 \n",
      "Epoch 00046: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 144s 16s/step - loss: 0.6769 - accuracy: 0.7710 - val_loss: 2.0489 - val_accuracy: 0.5214\n",
      "Epoch 47/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6830 - accuracy: 0.7659 \n",
      "Epoch 00047: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 146s 16s/step - loss: 0.6830 - accuracy: 0.7659 - val_loss: 2.0956 - val_accuracy: 0.5298\n",
      "Epoch 48/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6776 - accuracy: 0.7694  \n",
      "Epoch 00048: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 1124s 138s/step - loss: 0.6776 - accuracy: 0.7694 - val_loss: 2.0956 - val_accuracy: 0.5250\n",
      "Epoch 49/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6739 - accuracy: 0.7631  \n",
      "Epoch 00049: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 2005s 129s/step - loss: 0.6739 - accuracy: 0.7631 - val_loss: 2.0083 - val_accuracy: 0.5452\n",
      "Epoch 50/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6459 - accuracy: 0.7786 \n",
      "Epoch 00050: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 110s 12s/step - loss: 0.6459 - accuracy: 0.7786 - val_loss: 2.1067 - val_accuracy: 0.5357\n",
      "Epoch 51/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6515 - accuracy: 0.7698 \n",
      "Epoch 00051: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 110s 12s/step - loss: 0.6515 - accuracy: 0.7698 - val_loss: 2.0514 - val_accuracy: 0.5369\n",
      "Epoch 52/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6308 - accuracy: 0.7829 \n",
      "Epoch 00052: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 112s 13s/step - loss: 0.6308 - accuracy: 0.7829 - val_loss: 2.0562 - val_accuracy: 0.5429\n",
      "Epoch 53/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6463 - accuracy: 0.7798 \n",
      "Epoch 00053: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 113s 13s/step - loss: 0.6463 - accuracy: 0.7798 - val_loss: 2.1085 - val_accuracy: 0.5345\n",
      "Epoch 54/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6337 - accuracy: 0.7885 \n",
      "Epoch 00054: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 120s 13s/step - loss: 0.6337 - accuracy: 0.7885 - val_loss: 2.1783 - val_accuracy: 0.5429\n",
      "Epoch 55/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6130 - accuracy: 0.7877 \n",
      "Epoch 00055: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 115s 13s/step - loss: 0.6130 - accuracy: 0.7877 - val_loss: 2.2045 - val_accuracy: 0.5310\n",
      "Epoch 56/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6316 - accuracy: 0.7821 \n",
      "Epoch 00056: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 114s 13s/step - loss: 0.6316 - accuracy: 0.7821 - val_loss: 2.0978 - val_accuracy: 0.5333\n",
      "Epoch 57/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6094 - accuracy: 0.7913 \n",
      "Epoch 00057: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 115s 13s/step - loss: 0.6094 - accuracy: 0.7913 - val_loss: 2.0440 - val_accuracy: 0.5512\n",
      "Epoch 58/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6219 - accuracy: 0.7810 \n",
      "Epoch 00058: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 117s 13s/step - loss: 0.6219 - accuracy: 0.7810 - val_loss: 2.1685 - val_accuracy: 0.5583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.7853 \n",
      "Epoch 00059: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 114s 13s/step - loss: 0.6045 - accuracy: 0.7853 - val_loss: 2.1427 - val_accuracy: 0.5476\n",
      "Epoch 60/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.6003 - accuracy: 0.7861 \n",
      "Epoch 00060: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 116s 13s/step - loss: 0.6003 - accuracy: 0.7861 - val_loss: 2.1551 - val_accuracy: 0.5560\n",
      "Epoch 61/300\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.5873 - accuracy: 0.7881 \n",
      "Epoch 00061: val_loss did not improve from 1.67937\n",
      "9/9 [==============================] - 120s 13s/step - loss: 0.5873 - accuracy: 0.7881 - val_loss: 2.2042 - val_accuracy: 0.5405\n",
      "Epoch 00061: early stopping\n",
      "Training completed in time:  3:19:11.086896\n"
     ]
    }
   ],
   "source": [
    "ess = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 25, verbose= 1, mode='auto')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights4.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "\n",
    "model2.fit(X_train2, y_train,\n",
    "          batch_size = 300, \n",
    "          epochs = 300,\n",
    "          validation_data = (X_val2, y_val), \n",
    "          callbacks=[checkpointer, ess])\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training-Validation Accuracy')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#saving the model history\n",
    "loss = pd.DataFrame(model2.history.history)\n",
    "\n",
    "\n",
    "#plotting the loss and accuracy \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(loss[\"loss\"], label =\"Loss\")\n",
    "plt.plot(loss[\"val_loss\"], label = \"Validation_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(loss['accuracy'],label = \"Training Accuracy\")\n",
    "plt.plot(loss['val_accuracy'], label =\"Validation_ Accuracy \")\n",
    "plt.legend()\n",
    "plt.title(\"Training-Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20  5  7 17 18  3 11 11 18 20 16  9 16  3 10  5  0  4 13 11 11  2  5 15\n",
      " 12  7  9  4 19  5  5 16  2 17 15 15 10  4  0  7  1 16 10  7 19 16  5 11\n",
      "  5 16  3  8  5 12 14  9 19  7 16  1  3 20 11  8 15  0 11 12  0 10  5  1\n",
      "  3 18  3 11  7  6  9 15 10 17  5 15 10 18  2  6 10 11  6 19 11 19  1 12\n",
      "  9  2 11  9 14  6 16  3 20 19  2  1 19  4  5  4 20 10 16  3  3 18  5 10\n",
      " 11 11 16 11 12 10 11  5 10 13 17 19 14 10  3 10 14  2  8  6  3  6  6 19\n",
      "  3  3 16 11 17  5  8  8 18  8 10 17 20 12 15 19 16 16 20  8  8  5 20  7\n",
      "  8  3 20 12  4 18 15  6  5 20 18 16  1 15 17  5 19 14 15 18  1  6  5 19\n",
      " 11 14  7 12  3  6 19 13 17  2 20 14 12  5  3  3 12  8  6  1 12  0 15  1\n",
      " 17 14  3 20 20  3 11 18 17  6 12 10  2 12 18  7 20  5 18  8 10 15  6 16\n",
      " 16  6  3 17 18 14  3 19 15 17 15 12 18 15 14  4 20  6 19 16  5 19 19  5\n",
      "  2 13  5  2 13 10 16  0 14 12  5  7 15  5  5  9 20  4  6 20  5 15 11  3\n",
      " 16  1 15 11  8 20  6 16  2 16 14  8 19  1 16  0 15  6 14 16 11  8  8  8\n",
      "  1  5 20  7  5 14  6 11 12 16 11  1  4 17  2 19  7 20 12 16 19 16 18 11\n",
      " 17  4  1 18  5  6  2 17  9  2 15  7  3  5 16 16 16  3  8 15  3  1  3 11\n",
      "  9 11  9 20  3 11  2 18  3 11  5  2  3 10 18 19 16 18 14 14 16 17 19 12\n",
      " 11  2  7  1  9 17 20 13  6 12  3  6 10  5  3  1  9 18 13 20  5  0 11 10\n",
      "  3 15  5 19  8 11  9  3  5 12  7  4 14 17  2 12 16 10 20 12  8 13  7 20\n",
      "  9  9 17  0 19 12 15 14 16 15 20 11 18  4 18  1 19 20  2 18  8 13  9 12\n",
      "  1 20  5 18  5  3  2  1 15  3 20  1  9 13 15 14  1 18  5  3 17  8  6  6\n",
      "  2 10  8 16 14 10  4  3  3 14 19  1 20  5 15 15  1  6 20 20  8 20 16 20\n",
      "  5 11  9  8  1  9  3  0  7  4  8  5  8 17 18 14 20 14  7  8  5 19  7 20\n",
      " 14 18  8  2  2 17  2  7 20  8 15 14 17 13  6  8 16 16  7  6  8 12  0  6\n",
      "  2  5  9  5  2  1  4 13 20  2 17  5 20 16  2 10 12 19  9  5  8  9 17 20\n",
      " 11  6 11  9  1 18  5  8  4  3 16 10  7 17  3  2 16 11  3 11  5  8  9 19\n",
      "  8  2  9 19  0  6  2 20 19  4  9 13 16 11  2  3 15  5  6 16 20 10  9  4\n",
      " 15  4 18 12  7 20 14 10  5  4 20 15  4 19  8 12  3 10  5 18  8 11  5  7\n",
      "  1 12  4  7  2  9 14 12  5 14 11  6 18 13 16  9 10  3  0 12 16 11 10  7\n",
      " 14  4  3  6  6 13 14 17  3  6  8  8 10  2 14 17 11 19 15 20 18 16 17 13\n",
      "  6  1  4 14  3  5 14 15 11 20  7 19  9  3  5  5  8 19 20  5 16  0 13 20\n",
      " 14 20  9 14  8 11  8 18  6  2  4 10 10 18  6  2 11  0 20  6 18  5 12  2\n",
      "  4  6 10  2  7 16  9  3 12  8  8 14 10 12  3 14 17  5  1  1 15 14 18  4\n",
      "  9 17  8  5 18  3 13 11  3 15  3 20  1 16 15 20  4 14  6 17 19 10  4  5\n",
      "  0 10 14  6  6 16 11  7 19 18 15  7 20 12 20  9  3 18 15 19  6 15 16  6\n",
      "  8 15  4  5 13 17  6 20 17  7  6 15  1 11 19  5  9  7 14 15  8  3  9  3]\n"
     ]
    }
   ],
   "source": [
    "prediction = model2.predict(X_val2)\n",
    "\n",
    "# finding class with larget predicted probability using argmax of numpy \n",
    "y_pred = np.argmax(prediction, axis = 1)  # prediction using model \n",
    "y_val_orig = np.argmax(y_val, axis = 1) # original y_val\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.04      0.07        45\n",
      "           1       0.55      0.41      0.47        44\n",
      "           2       0.42      0.40      0.41        40\n",
      "           3       0.30      0.45      0.36        38\n",
      "           4       0.33      0.24      0.28        42\n",
      "           5       0.22      0.48      0.30        29\n",
      "           6       0.19      0.24      0.21        37\n",
      "           7       0.44      0.31      0.36        45\n",
      "           8       0.28      0.46      0.35        28\n",
      "           9       0.42      0.29      0.34        52\n",
      "          10       0.38      0.38      0.38        37\n",
      "          11       0.60      0.85      0.71        34\n",
      "          12       0.91      0.97      0.94        33\n",
      "          13       0.79      0.43      0.56        35\n",
      "          14       0.83      0.69      0.76        49\n",
      "          15       0.74      0.70      0.72        46\n",
      "          16       0.84      0.78      0.81        54\n",
      "          17       0.91      0.94      0.93        33\n",
      "          18       0.90      0.74      0.81        47\n",
      "          19       0.72      0.88      0.79        32\n",
      "          20       0.62      0.85      0.72        40\n",
      "\n",
      "    accuracy                           0.54       840\n",
      "   macro avg       0.55      0.55      0.54       840\n",
      "weighted avg       0.56      0.54      0.54       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_orig, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_11 (Conv2D)          (None, 38, 172, 32)       320       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 19, 86, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 17, 84, 128)       36992     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 9, 42, 128)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 9, 42, 128)        0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 7, 40, 128)        147584    \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 4, 20, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 4, 20, 128)        0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 512)               66048     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 21)                10773     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 261,717\n",
      "Trainable params: 261,717\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "model =  models.Sequential([\n",
    "    \n",
    "                          layers.Conv2D(32 , (3,3),activation = 'relu',padding='valid', input_shape = input_shape),  \n",
    "                          layers.MaxPooling2D(2, padding='same'),\n",
    "                          layers.Conv2D(128, (3,3), activation='relu',padding='valid'),\n",
    "                          layers.MaxPooling2D(2, padding='same'),\n",
    "                          layers.Dropout(0.3),\n",
    "                          layers.Conv2D(128, (3,3), activation='relu',padding='valid'),\n",
    "                          layers.MaxPooling2D(2, padding='same'),\n",
    "                          layers.Dropout(0.3),\n",
    "                          layers.GlobalAveragePooling2D(),\n",
    "                          layers.Dense(512 , activation = 'relu'),\n",
    "                          layers.Dense(21 , activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = 'acc')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 2.9230 - acc: 0.0718\n",
      "Epoch 00001: val_loss improved from inf to 2.74034, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 20s 360ms/step - loss: 2.9230 - acc: 0.0718 - val_loss: 2.7403 - val_acc: 0.0833\n",
      "Epoch 2/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 2.5375 - acc: 0.1599\n",
      "Epoch 00002: val_loss improved from 2.74034 to 2.50144, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 20s 395ms/step - loss: 2.5375 - acc: 0.1599 - val_loss: 2.5014 - val_acc: 0.1857\n",
      "Epoch 3/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 2.3955 - acc: 0.2048\n",
      "Epoch 00003: val_loss improved from 2.50144 to 2.35949, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 20s 393ms/step - loss: 2.3955 - acc: 0.2048 - val_loss: 2.3595 - val_acc: 0.2429\n",
      "Epoch 4/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 2.2891 - acc: 0.2345\n",
      "Epoch 00004: val_loss improved from 2.35949 to 2.34796, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 19s 363ms/step - loss: 2.2891 - acc: 0.2345 - val_loss: 2.3480 - val_acc: 0.2488\n",
      "Epoch 5/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 2.1999 - acc: 0.2651\n",
      "Epoch 00005: val_loss improved from 2.34796 to 2.24925, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 19s 368ms/step - loss: 2.1999 - acc: 0.2651 - val_loss: 2.2493 - val_acc: 0.2464\n",
      "Epoch 6/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 2.1305 - acc: 0.2893\n",
      "Epoch 00006: val_loss improved from 2.24925 to 2.15074, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 19s 380ms/step - loss: 2.1305 - acc: 0.2893 - val_loss: 2.1507 - val_acc: 0.2893\n",
      "Epoch 7/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 2.0659 - acc: 0.3190\n",
      "Epoch 00007: val_loss improved from 2.15074 to 2.10317, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 19s 364ms/step - loss: 2.0659 - acc: 0.3190 - val_loss: 2.1032 - val_acc: 0.3131\n",
      "Epoch 8/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 2.0195 - acc: 0.3298\n",
      "Epoch 00008: val_loss did not improve from 2.10317\n",
      "51/51 [==============================] - 18s 362ms/step - loss: 2.0195 - acc: 0.3298 - val_loss: 2.1174 - val_acc: 0.3190\n",
      "Epoch 9/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.9265 - acc: 0.3671\n",
      "Epoch 00009: val_loss improved from 2.10317 to 2.03749, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 19s 368ms/step - loss: 1.9265 - acc: 0.3671 - val_loss: 2.0375 - val_acc: 0.3345\n",
      "Epoch 10/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.9018 - acc: 0.3671\n",
      "Epoch 00010: val_loss did not improve from 2.03749\n",
      "51/51 [==============================] - 19s 372ms/step - loss: 1.9018 - acc: 0.3671 - val_loss: 2.0410 - val_acc: 0.3250\n",
      "Epoch 11/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.8415 - acc: 0.4016\n",
      "Epoch 00011: val_loss improved from 2.03749 to 1.94654, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 20s 386ms/step - loss: 1.8415 - acc: 0.4016 - val_loss: 1.9465 - val_acc: 0.3762\n",
      "Epoch 12/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.8066 - acc: 0.3952\n",
      "Epoch 00012: val_loss improved from 1.94654 to 1.90268, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 19s 367ms/step - loss: 1.8066 - acc: 0.3952 - val_loss: 1.9027 - val_acc: 0.3917\n",
      "Epoch 13/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.7503 - acc: 0.4238\n",
      "Epoch 00013: val_loss improved from 1.90268 to 1.85549, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 19s 367ms/step - loss: 1.7503 - acc: 0.4238 - val_loss: 1.8555 - val_acc: 0.4060\n",
      "Epoch 14/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.7407 - acc: 0.4286\n",
      "Epoch 00014: val_loss did not improve from 1.85549\n",
      "51/51 [==============================] - 21s 414ms/step - loss: 1.7407 - acc: 0.4286 - val_loss: 1.9266 - val_acc: 0.4012\n",
      "Epoch 15/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.6672 - acc: 0.4567\n",
      "Epoch 00015: val_loss improved from 1.85549 to 1.84154, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 18s 356ms/step - loss: 1.6672 - acc: 0.4567 - val_loss: 1.8415 - val_acc: 0.4155\n",
      "Epoch 16/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.6268 - acc: 0.4615\n",
      "Epoch 00016: val_loss did not improve from 1.84154\n",
      "51/51 [==============================] - 24s 472ms/step - loss: 1.6268 - acc: 0.4615 - val_loss: 1.8687 - val_acc: 0.4179\n",
      "Epoch 17/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.6012 - acc: 0.4690\n",
      "Epoch 00017: val_loss improved from 1.84154 to 1.80358, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 19s 371ms/step - loss: 1.6012 - acc: 0.4690 - val_loss: 1.8036 - val_acc: 0.4321\n",
      "Epoch 18/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.5856 - acc: 0.4734\n",
      "Epoch 00018: val_loss did not improve from 1.80358\n",
      "51/51 [==============================] - 21s 406ms/step - loss: 1.5856 - acc: 0.4734 - val_loss: 1.8747 - val_acc: 0.4036\n",
      "Epoch 19/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.5428 - acc: 0.5012\n",
      "Epoch 00019: val_loss improved from 1.80358 to 1.76298, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 27s 529ms/step - loss: 1.5428 - acc: 0.5012 - val_loss: 1.7630 - val_acc: 0.4619\n",
      "Epoch 20/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.5120 - acc: 0.5071\n",
      "Epoch 00020: val_loss did not improve from 1.76298\n",
      "51/51 [==============================] - 24s 463ms/step - loss: 1.5120 - acc: 0.5071 - val_loss: 1.8187 - val_acc: 0.4381\n",
      "Epoch 21/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.5028 - acc: 0.5115\n",
      "Epoch 00021: val_loss did not improve from 1.76298\n",
      "51/51 [==============================] - 20s 388ms/step - loss: 1.5028 - acc: 0.5115 - val_loss: 1.8486 - val_acc: 0.4179\n",
      "Epoch 22/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.4677 - acc: 0.5262\n",
      "Epoch 00022: val_loss did not improve from 1.76298\n",
      "51/51 [==============================] - 21s 415ms/step - loss: 1.4677 - acc: 0.5262 - val_loss: 1.8072 - val_acc: 0.4643\n",
      "Epoch 23/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.4343 - acc: 0.5310\n",
      "Epoch 00023: val_loss did not improve from 1.76298\n",
      "51/51 [==============================] - 22s 436ms/step - loss: 1.4343 - acc: 0.5310 - val_loss: 1.7662 - val_acc: 0.4571\n",
      "Epoch 24/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.3911 - acc: 0.5492\n",
      "Epoch 00024: val_loss did not improve from 1.76298\n",
      "51/51 [==============================] - 20s 395ms/step - loss: 1.3911 - acc: 0.5492 - val_loss: 1.7714 - val_acc: 0.4560\n",
      "Epoch 25/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.3803 - acc: 0.5460\n",
      "Epoch 00025: val_loss improved from 1.76298 to 1.73898, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 20s 393ms/step - loss: 1.3803 - acc: 0.5460 - val_loss: 1.7390 - val_acc: 0.4714\n",
      "Epoch 26/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.3481 - acc: 0.5567\n",
      "Epoch 00026: val_loss did not improve from 1.73898\n",
      "51/51 [==============================] - 18s 358ms/step - loss: 1.3481 - acc: 0.5567 - val_loss: 1.7544 - val_acc: 0.4595\n",
      "Epoch 27/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.3369 - acc: 0.5575\n",
      "Epoch 00027: val_loss did not improve from 1.73898\n",
      "51/51 [==============================] - 24s 465ms/step - loss: 1.3369 - acc: 0.5575 - val_loss: 1.7625 - val_acc: 0.4714\n",
      "Epoch 28/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.2964 - acc: 0.5790\n",
      "Epoch 00028: val_loss did not improve from 1.73898\n",
      "51/51 [==============================] - 20s 386ms/step - loss: 1.2964 - acc: 0.5790 - val_loss: 1.7639 - val_acc: 0.4881\n",
      "Epoch 29/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.2948 - acc: 0.5679\n",
      "Epoch 00029: val_loss improved from 1.73898 to 1.73163, saving model to weights5.best.hdf5\n",
      "51/51 [==============================] - 22s 438ms/step - loss: 1.2948 - acc: 0.5679 - val_loss: 1.7316 - val_acc: 0.4750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.2472 - acc: 0.5897\n",
      "Epoch 00030: val_loss did not improve from 1.73163\n",
      "51/51 [==============================] - 21s 406ms/step - loss: 1.2472 - acc: 0.5897 - val_loss: 1.7730 - val_acc: 0.4869\n",
      "Epoch 31/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.2494 - acc: 0.5948\n",
      "Epoch 00031: val_loss did not improve from 1.73163\n",
      "51/51 [==============================] - 19s 377ms/step - loss: 1.2494 - acc: 0.5948 - val_loss: 1.7392 - val_acc: 0.4881\n",
      "Epoch 32/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.2199 - acc: 0.5893\n",
      "Epoch 00032: val_loss did not improve from 1.73163\n",
      "51/51 [==============================] - 20s 401ms/step - loss: 1.2199 - acc: 0.5893 - val_loss: 1.8339 - val_acc: 0.4607\n",
      "Epoch 33/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.1943 - acc: 0.6044\n",
      "Epoch 00033: val_loss did not improve from 1.73163\n",
      "51/51 [==============================] - 28s 558ms/step - loss: 1.1943 - acc: 0.6044 - val_loss: 1.7470 - val_acc: 0.4893\n",
      "Epoch 34/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.1946 - acc: 0.6095\n",
      "Epoch 00034: val_loss did not improve from 1.73163\n",
      "51/51 [==============================] - 23s 455ms/step - loss: 1.1946 - acc: 0.6095 - val_loss: 1.7752 - val_acc: 0.4929\n",
      "Epoch 35/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.1833 - acc: 0.6123\n",
      "Epoch 00035: val_loss did not improve from 1.73163\n",
      "51/51 [==============================] - 23s 445ms/step - loss: 1.1833 - acc: 0.6123 - val_loss: 1.8136 - val_acc: 0.4845\n",
      "Epoch 36/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.1776 - acc: 0.6099\n",
      "Epoch 00036: val_loss did not improve from 1.73163\n",
      "51/51 [==============================] - 23s 453ms/step - loss: 1.1776 - acc: 0.6099 - val_loss: 1.7486 - val_acc: 0.4929\n",
      "Epoch 37/300\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.1398 - acc: 0.6278\n",
      "Epoch 00037: val_loss did not improve from 1.73163\n",
      "51/51 [==============================] - 20s 384ms/step - loss: 1.1398 - acc: 0.6278 - val_loss: 1.8392 - val_acc: 0.5012\n",
      "Training completed in time:  0:12:48.017179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights5.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "\n",
    "model.fit(X_train2, y_train,\n",
    "          batch_size = 50, \n",
    "          epochs = 300,\n",
    "          validation_data = (X_val2, y_val), \n",
    "          callbacks=[checkpointer, es])\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training-Validation Accuracy')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#saving the model history\n",
    "loss = pd.DataFrame(model.history.history)\n",
    "\n",
    "\n",
    "#plotting the loss and accuracy \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(loss[\"loss\"], label =\"Loss\")\n",
    "plt.plot(loss[\"val_loss\"], label = \"Validation_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(loss['acc'],label = \"Training Accuracy\")\n",
    "plt.plot(loss['val_acc'], label =\"Validation_ Accuracy \")\n",
    "plt.legend()\n",
    "plt.title(\"Training-Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20  9  0 17 18  3 14 16 18 20 16  9 16  3  9  6  0  3 11 11 11  3  0 15\n",
      " 12  7  9  4 19  5  5 16  0 17 14 14 10  8  0  7  7 14 10  7 19 13  9 11\n",
      "  5 16  3  7  8 12 14 17 19  9 11  9  3 14 11  8 15  0 11 20  0 10  9  2\n",
      "  3 18  3 16  7 10  9 17 10 17  5 15 10 18  9 10 14 11  3 19 11 19  1  6\n",
      "  3  7 11  9 14  3 16  0 20 19  3  9 19  4  9  9 20 10 16  3  3 18  5 10\n",
      " 11 16 16 11 19  0 16  5  3 13 17 19 14 10 17  3 14  2  8  3  3  0  6 19\n",
      "  3  3 16  3 17  5 10 10 18 10 10 17 20 12 18 19 18 11 20 10  9  7 20  8\n",
      "  9  9 20  6  0 18  3  6  5 20 18 11 17 15 17  5 19 14 15 18  0  0  5 14\n",
      " 16 14  3 12  3  2 19 14 17  9 20 14 12  5  4  3 12  3  2  9 12 10  9  1\n",
      " 17 14  0 19 18  3 16 15 17  6 12 10  3 12 18  7 13  5 19  8 10 15  6 11\n",
      " 16 10 14 17 18 13 14 19 15 17 15 12 18 14 14 10 20  3 19 16  5 14 19  0\n",
      "  9 13  5 10 13 10 16  3 14 16  7  7 15  3  5  9 20 17 10 20  5  5 11  3\n",
      " 16  5 18 11 10 13  0 16  0 16 14  9 14  9 16  6 15 17 14 16 11  4  8  5\n",
      "  1  5 20  9  5 14  0 11 14 11 11  5  9 17 10 14  7 13 14 16 19 16 18 16\n",
      " 14  9  9 15  5  3  0 17  9  9 15  0  3 14 16 16 13  3  8 15  9  3  3 11\n",
      "  5 11  3 20  3 11  0 18  3 11  5  9  3 10 18 14 16 11 14 14 16 17 19 12\n",
      " 16 10  7  8  9 17 13 13  6 20  3  3 10  5  3  3  9 18 17 20  5  0 18  3\n",
      "  3 15  9 20 10 15  9  3  5 12  2 14 14 17  9 12 16 17 11 12  4 13  7 11\n",
      "  9  7 17  0 19 12 15  4 11 15 20 16 11 10 18  1 19 20  2 18  8 14  9 12\n",
      "  8 11  9 18 10  3  9  9 18  7 13  5  9 13 14 14 10 18  5  3 17  7  3  6\n",
      "  9 10  8 13 14 10  4  3  3 14 14  5 11  5 15 15  0  6 13 20  7 20 16 20\n",
      "  5 11  9 10  5  9  9  0  9  6  8  5  9 17 18 14 20  0  7  8  5 19  9 20\n",
      " 14 18  9 10  4 17 10  0 20 10 15 14 17 13  3  0 16 16  0  3  8 20  3  6\n",
      "  3  3  9  8  2  2  4 13 20  9 17  5 20 14  0 10 12 19  9  5  4  9 17 20\n",
      " 11  3 10  9  1 11  0 14 14  3 16 10  7 17  3  8 16 11  9 11  5  4  9 14\n",
      "  8  2  9 19  0  6 10 15 13  8  3  3 16 11  9 17 15  9 17 16 20 10  9  9\n",
      " 14  4  0 12  7 20 14  0 10  3 20  4  9 18 10 12  3  3  8 18  9 14  8  2\n",
      "  1 12  9  9  9  9 14 12  7 14 11  6 18  3 16  9 10  3  7  6 16 11 10  9\n",
      " 14  4  3  5  3  9 14 17  9  6 10  8 10  2 14 17 11 15 15 20 18 16 17 14\n",
      "  3  1  0 14  3  3 14 11 11 13  9 14  5  3  5  5  8 19 15  7 16  0 13 20\n",
      " 14 20  9 14  4 14  9 18  3  2  5 10 10 18 10  5 11  3 20 10 18  3 12  3\n",
      "  4  6 10  8  7 16  9  3 12  9  9 14 10  2  3  1 17  5  9 17  9 14 18  4\n",
      "  9 17  3  9 18  3 14 11 10 15 17 11  8 16  9 20 14 14  2 17 19 10  4  5\n",
      "  0  3 14  7  9 16 11  7 19 18 15  0 13 10 14  3  3 18 15 13  3 15 16 10\n",
      " 10 15 10  5 13 17  9 20 14  7  3 15  1 11 19  5  9  7 14 15 10  3  9  7]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_val2)\n",
    "\n",
    "# finding class with larget predicted probability using argmax of numpy \n",
    "y_pred = np.argmax(prediction, axis = 1)  # prediction using model \n",
    "y_val_orig = np.argmax(y_val, axis = 1) # original y_val\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.13      0.14        45\n",
      "           1       0.44      0.09      0.15        44\n",
      "           2       0.21      0.07      0.11        40\n",
      "           3       0.30      0.68      0.41        38\n",
      "           4       0.39      0.17      0.23        42\n",
      "           5       0.14      0.24      0.18        29\n",
      "           6       0.28      0.14      0.18        37\n",
      "           7       0.50      0.33      0.40        45\n",
      "           8       0.24      0.21      0.23        28\n",
      "           9       0.28      0.48      0.36        52\n",
      "          10       0.23      0.38      0.28        37\n",
      "          11       0.65      0.91      0.76        34\n",
      "          12       1.00      0.70      0.82        33\n",
      "          13       0.67      0.46      0.54        35\n",
      "          14       0.55      0.82      0.66        49\n",
      "          15       0.85      0.61      0.71        46\n",
      "          16       0.85      0.76      0.80        54\n",
      "          17       0.73      0.97      0.83        33\n",
      "          18       0.92      0.77      0.84        47\n",
      "          19       0.90      0.81      0.85        32\n",
      "          20       0.75      0.75      0.75        40\n",
      "\n",
      "    accuracy                           0.50       840\n",
      "   macro avg       0.53      0.50      0.49       840\n",
      "weighted avg       0.53      0.50      0.49       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_orig, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_26 (Conv2D)          (None, 39, 173, 16)       80        \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 19, 86, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 19, 86, 16)        0         \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 18, 85, 32)        2080      \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPoolin  (None, 9, 42, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 9, 42, 32)         0         \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 8, 41, 64)         8256      \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPoolin  (None, 4, 20, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 4, 20, 64)         0         \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 3, 19, 128)        32896     \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 1, 9, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 1, 9, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_4   (None, 128)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 21)                2709      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,021\n",
      "Trainable params: 46,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Conv2D(filters=16, kernel_size=2, input_shape=input_shape, activation='relu'))\n",
    "model_relu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_relu.add(Dropout(0.2))\n",
    "\n",
    "model_relu.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model_relu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_relu.add(Dropout(0.2))\n",
    "\n",
    "model_relu.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model_relu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_relu.add(Dropout(0.2))\n",
    "\n",
    "model_relu.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model_relu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_relu.add(Dropout(0.2))\n",
    "model_relu.add(GlobalAveragePooling2D())\n",
    "model_relu.add(Flatten())\n",
    "model_relu.add(Dense(21, activation='softmax'))\n",
    "\n",
    "\n",
    "model_relu.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.9813 - accuracy: 0.0611\n",
      "Epoch 00001: val_loss improved from inf to 2.90763, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 8s 273ms/step - loss: 2.9813 - accuracy: 0.0611 - val_loss: 2.9076 - val_accuracy: 0.1357\n",
      "Epoch 2/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.7527 - accuracy: 0.1190\n",
      "Epoch 00002: val_loss improved from 2.90763 to 2.63735, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 7s 286ms/step - loss: 2.7527 - accuracy: 0.1190 - val_loss: 2.6374 - val_accuracy: 0.1321\n",
      "Epoch 3/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.5491 - accuracy: 0.1643\n",
      "Epoch 00003: val_loss improved from 2.63735 to 2.53595, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 2.5491 - accuracy: 0.1643 - val_loss: 2.5359 - val_accuracy: 0.1738\n",
      "Epoch 4/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.4458 - accuracy: 0.1972\n",
      "Epoch 00004: val_loss improved from 2.53595 to 2.45352, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 2.4458 - accuracy: 0.1972 - val_loss: 2.4535 - val_accuracy: 0.2179\n",
      "Epoch 5/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.3886 - accuracy: 0.2091\n",
      "Epoch 00005: val_loss improved from 2.45352 to 2.39996, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 238ms/step - loss: 2.3886 - accuracy: 0.2091 - val_loss: 2.4000 - val_accuracy: 0.2619\n",
      "Epoch 6/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.3482 - accuracy: 0.2341\n",
      "Epoch 00006: val_loss improved from 2.39996 to 2.38602, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 2.3482 - accuracy: 0.2341 - val_loss: 2.3860 - val_accuracy: 0.2405\n",
      "Epoch 7/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.3088 - accuracy: 0.2373\n",
      "Epoch 00007: val_loss improved from 2.38602 to 2.35689, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 2.3088 - accuracy: 0.2373 - val_loss: 2.3569 - val_accuracy: 0.2476\n",
      "Epoch 8/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.2830 - accuracy: 0.2540\n",
      "Epoch 00008: val_loss improved from 2.35689 to 2.32110, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 2.2830 - accuracy: 0.2540 - val_loss: 2.3211 - val_accuracy: 0.2595\n",
      "Epoch 9/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.2464 - accuracy: 0.2563\n",
      "Epoch 00009: val_loss improved from 2.32110 to 2.29905, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 2.2464 - accuracy: 0.2563 - val_loss: 2.2990 - val_accuracy: 0.2524\n",
      "Epoch 10/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.2101 - accuracy: 0.2679\n",
      "Epoch 00010: val_loss improved from 2.29905 to 2.25672, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 2.2101 - accuracy: 0.2679 - val_loss: 2.2567 - val_accuracy: 0.2798\n",
      "Epoch 11/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.2179 - accuracy: 0.2750\n",
      "Epoch 00011: val_loss improved from 2.25672 to 2.24238, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 7s 288ms/step - loss: 2.2179 - accuracy: 0.2750 - val_loss: 2.2424 - val_accuracy: 0.2738\n",
      "Epoch 12/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.1591 - accuracy: 0.2980\n",
      "Epoch 00012: val_loss improved from 2.24238 to 2.23202, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 2.1591 - accuracy: 0.2980 - val_loss: 2.2320 - val_accuracy: 0.3024\n",
      "Epoch 13/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.1385 - accuracy: 0.2944\n",
      "Epoch 00013: val_loss improved from 2.23202 to 2.20173, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 2.1385 - accuracy: 0.2944 - val_loss: 2.2017 - val_accuracy: 0.2881\n",
      "Epoch 14/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.1194 - accuracy: 0.3131\n",
      "Epoch 00014: val_loss improved from 2.20173 to 2.17953, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 2.1194 - accuracy: 0.3131 - val_loss: 2.1795 - val_accuracy: 0.3155\n",
      "Epoch 15/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.0971 - accuracy: 0.3210\n",
      "Epoch 00015: val_loss did not improve from 2.17953\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 2.0971 - accuracy: 0.3210 - val_loss: 2.1804 - val_accuracy: 0.2940\n",
      "Epoch 16/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.0937 - accuracy: 0.3099\n",
      "Epoch 00016: val_loss improved from 2.17953 to 2.17628, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 7s 255ms/step - loss: 2.0937 - accuracy: 0.3099 - val_loss: 2.1763 - val_accuracy: 0.2940\n",
      "Epoch 17/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.0745 - accuracy: 0.3187\n",
      "Epoch 00017: val_loss improved from 2.17628 to 2.12220, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 8s 307ms/step - loss: 2.0745 - accuracy: 0.3187 - val_loss: 2.1222 - val_accuracy: 0.3500\n",
      "Epoch 18/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.0270 - accuracy: 0.3385\n",
      "Epoch 00018: val_loss improved from 2.12220 to 2.09934, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 8s 316ms/step - loss: 2.0270 - accuracy: 0.3385 - val_loss: 2.0993 - val_accuracy: 0.3155\n",
      "Epoch 19/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.0044 - accuracy: 0.3488\n",
      "Epoch 00019: val_loss improved from 2.09934 to 2.09402, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 9s 335ms/step - loss: 2.0044 - accuracy: 0.3488 - val_loss: 2.0940 - val_accuracy: 0.3381\n",
      "Epoch 20/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.0021 - accuracy: 0.3488\n",
      "Epoch 00020: val_loss improved from 2.09402 to 2.09308, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 8s 323ms/step - loss: 2.0021 - accuracy: 0.3488 - val_loss: 2.0931 - val_accuracy: 0.3226\n",
      "Epoch 21/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.9705 - accuracy: 0.3575\n",
      "Epoch 00021: val_loss did not improve from 2.09308\n",
      "26/26 [==============================] - 7s 250ms/step - loss: 1.9705 - accuracy: 0.3575 - val_loss: 2.0931 - val_accuracy: 0.3405\n",
      "Epoch 22/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.9470 - accuracy: 0.3730\n",
      "Epoch 00022: val_loss improved from 2.09308 to 2.04622, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 7s 266ms/step - loss: 1.9470 - accuracy: 0.3730 - val_loss: 2.0462 - val_accuracy: 0.3524\n",
      "Epoch 23/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.9293 - accuracy: 0.3714\n",
      "Epoch 00023: val_loss did not improve from 2.04622\n",
      "26/26 [==============================] - 10s 379ms/step - loss: 1.9293 - accuracy: 0.3714 - val_loss: 2.0529 - val_accuracy: 0.3429\n",
      "Epoch 24/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.9166 - accuracy: 0.3833\n",
      "Epoch 00024: val_loss did not improve from 2.04622\n",
      "26/26 [==============================] - 9s 340ms/step - loss: 1.9166 - accuracy: 0.3833 - val_loss: 2.0523 - val_accuracy: 0.3440\n",
      "Epoch 25/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.9095 - accuracy: 0.3881\n",
      "Epoch 00025: val_loss improved from 2.04622 to 2.02534, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 7s 256ms/step - loss: 1.9095 - accuracy: 0.3881 - val_loss: 2.0253 - val_accuracy: 0.3571\n",
      "Epoch 26/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.8823 - accuracy: 0.3849\n",
      "Epoch 00026: val_loss did not improve from 2.02534\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 1.8823 - accuracy: 0.3849 - val_loss: 2.0255 - val_accuracy: 0.3417\n",
      "Epoch 27/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.8825 - accuracy: 0.3829\n",
      "Epoch 00027: val_loss improved from 2.02534 to 2.00403, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 7s 253ms/step - loss: 1.8825 - accuracy: 0.3829 - val_loss: 2.0040 - val_accuracy: 0.3619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.8489 - accuracy: 0.4032\n",
      "Epoch 00028: val_loss improved from 2.00403 to 1.99469, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 7s 277ms/step - loss: 1.8489 - accuracy: 0.4032 - val_loss: 1.9947 - val_accuracy: 0.3583\n",
      "Epoch 29/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.8586 - accuracy: 0.3901\n",
      "Epoch 00029: val_loss improved from 1.99469 to 1.98405, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 8s 296ms/step - loss: 1.8586 - accuracy: 0.3901 - val_loss: 1.9840 - val_accuracy: 0.3714\n",
      "Epoch 30/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.8598 - accuracy: 0.3857\n",
      "Epoch 00030: val_loss did not improve from 1.98405\n",
      "26/26 [==============================] - 6s 236ms/step - loss: 1.8598 - accuracy: 0.3857 - val_loss: 2.0310 - val_accuracy: 0.3417\n",
      "Epoch 31/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.8290 - accuracy: 0.4071\n",
      "Epoch 00031: val_loss improved from 1.98405 to 1.98019, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 1.8290 - accuracy: 0.4071 - val_loss: 1.9802 - val_accuracy: 0.3726\n",
      "Epoch 32/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7908 - accuracy: 0.4298\n",
      "Epoch 00032: val_loss improved from 1.98019 to 1.96784, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.7908 - accuracy: 0.4298 - val_loss: 1.9678 - val_accuracy: 0.3786\n",
      "Epoch 33/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7948 - accuracy: 0.4163\n",
      "Epoch 00033: val_loss did not improve from 1.96784\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 1.7948 - accuracy: 0.4163 - val_loss: 1.9835 - val_accuracy: 0.3631\n",
      "Epoch 34/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7938 - accuracy: 0.4206\n",
      "Epoch 00034: val_loss improved from 1.96784 to 1.96186, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 1.7938 - accuracy: 0.4206 - val_loss: 1.9619 - val_accuracy: 0.3786\n",
      "Epoch 35/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7620 - accuracy: 0.4321\n",
      "Epoch 00035: val_loss did not improve from 1.96186\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 1.7620 - accuracy: 0.4321 - val_loss: 1.9827 - val_accuracy: 0.3786\n",
      "Epoch 36/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7502 - accuracy: 0.4345\n",
      "Epoch 00036: val_loss improved from 1.96186 to 1.94317, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 7s 269ms/step - loss: 1.7502 - accuracy: 0.4345 - val_loss: 1.9432 - val_accuracy: 0.3869\n",
      "Epoch 37/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7431 - accuracy: 0.4444\n",
      "Epoch 00037: val_loss did not improve from 1.94317\n",
      "26/26 [==============================] - 7s 258ms/step - loss: 1.7431 - accuracy: 0.4444 - val_loss: 1.9453 - val_accuracy: 0.3833\n",
      "Epoch 38/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7250 - accuracy: 0.4409\n",
      "Epoch 00038: val_loss improved from 1.94317 to 1.93191, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 1.7250 - accuracy: 0.4409 - val_loss: 1.9319 - val_accuracy: 0.3976\n",
      "Epoch 39/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7230 - accuracy: 0.4492\n",
      "Epoch 00039: val_loss did not improve from 1.93191\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.7230 - accuracy: 0.4492 - val_loss: 1.9580 - val_accuracy: 0.3988\n",
      "Epoch 40/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7220 - accuracy: 0.4484\n",
      "Epoch 00040: val_loss improved from 1.93191 to 1.92775, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 1.7220 - accuracy: 0.4484 - val_loss: 1.9278 - val_accuracy: 0.4036\n",
      "Epoch 41/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7055 - accuracy: 0.4536\n",
      "Epoch 00041: val_loss did not improve from 1.92775\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 1.7055 - accuracy: 0.4536 - val_loss: 1.9479 - val_accuracy: 0.3940\n",
      "Epoch 42/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7142 - accuracy: 0.4544\n",
      "Epoch 00042: val_loss improved from 1.92775 to 1.90264, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.7142 - accuracy: 0.4544 - val_loss: 1.9026 - val_accuracy: 0.4060\n",
      "Epoch 43/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6726 - accuracy: 0.4627\n",
      "Epoch 00043: val_loss improved from 1.90264 to 1.89621, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.6726 - accuracy: 0.4627 - val_loss: 1.8962 - val_accuracy: 0.4179\n",
      "Epoch 44/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6437 - accuracy: 0.4774\n",
      "Epoch 00044: val_loss did not improve from 1.89621\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 1.6437 - accuracy: 0.4774 - val_loss: 1.9565 - val_accuracy: 0.3798\n",
      "Epoch 45/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6681 - accuracy: 0.4595\n",
      "Epoch 00045: val_loss improved from 1.89621 to 1.88441, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 1.6681 - accuracy: 0.4595 - val_loss: 1.8844 - val_accuracy: 0.4202\n",
      "Epoch 46/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6537 - accuracy: 0.4734\n",
      "Epoch 00046: val_loss did not improve from 1.88441\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.6537 - accuracy: 0.4734 - val_loss: 1.9417 - val_accuracy: 0.3798\n",
      "Epoch 47/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6546 - accuracy: 0.4754\n",
      "Epoch 00047: val_loss did not improve from 1.88441\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 1.6546 - accuracy: 0.4754 - val_loss: 1.8859 - val_accuracy: 0.4190\n",
      "Epoch 48/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6140 - accuracy: 0.4853\n",
      "Epoch 00048: val_loss improved from 1.88441 to 1.86006, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 7s 290ms/step - loss: 1.6140 - accuracy: 0.4853 - val_loss: 1.8601 - val_accuracy: 0.4345\n",
      "Epoch 49/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6013 - accuracy: 0.4913\n",
      "Epoch 00049: val_loss did not improve from 1.86006\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 1.6013 - accuracy: 0.4913 - val_loss: 1.8817 - val_accuracy: 0.4131\n",
      "Epoch 50/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5979 - accuracy: 0.4972\n",
      "Epoch 00050: val_loss did not improve from 1.86006\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 1.5979 - accuracy: 0.4972 - val_loss: 1.8968 - val_accuracy: 0.4202\n",
      "Epoch 51/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5977 - accuracy: 0.4881\n",
      "Epoch 00051: val_loss did not improve from 1.86006\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 1.5977 - accuracy: 0.4881 - val_loss: 1.8882 - val_accuracy: 0.4036\n",
      "Epoch 52/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5747 - accuracy: 0.5091\n",
      "Epoch 00052: val_loss did not improve from 1.86006\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.5747 - accuracy: 0.5091 - val_loss: 1.8785 - val_accuracy: 0.4226\n",
      "Epoch 53/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5675 - accuracy: 0.5040\n",
      "Epoch 00053: val_loss did not improve from 1.86006\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 1.5675 - accuracy: 0.5040 - val_loss: 1.8624 - val_accuracy: 0.4226\n",
      "Epoch 54/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5602 - accuracy: 0.4905\n",
      "Epoch 00054: val_loss improved from 1.86006 to 1.83564, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 7s 270ms/step - loss: 1.5602 - accuracy: 0.4905 - val_loss: 1.8356 - val_accuracy: 0.4321\n",
      "Epoch 55/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5651 - accuracy: 0.4980\n",
      "Epoch 00055: val_loss did not improve from 1.83564\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 1.5651 - accuracy: 0.4980 - val_loss: 1.8737 - val_accuracy: 0.4167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5474 - accuracy: 0.5032\n",
      "Epoch 00056: val_loss did not improve from 1.83564\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.5474 - accuracy: 0.5032 - val_loss: 1.8514 - val_accuracy: 0.4381\n",
      "Epoch 57/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5261 - accuracy: 0.4996\n",
      "Epoch 00057: val_loss did not improve from 1.83564\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 1.5261 - accuracy: 0.4996 - val_loss: 1.8557 - val_accuracy: 0.4250\n",
      "Epoch 58/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5252 - accuracy: 0.5103\n",
      "Epoch 00058: val_loss did not improve from 1.83564\n",
      "26/26 [==============================] - 6s 239ms/step - loss: 1.5252 - accuracy: 0.5103 - val_loss: 1.8497 - val_accuracy: 0.4238\n",
      "Epoch 59/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5168 - accuracy: 0.5095\n",
      "Epoch 00059: val_loss did not improve from 1.83564\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 1.5168 - accuracy: 0.5095 - val_loss: 1.8667 - val_accuracy: 0.4119\n",
      "Epoch 60/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5065 - accuracy: 0.5198\n",
      "Epoch 00060: val_loss did not improve from 1.83564\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.5065 - accuracy: 0.5198 - val_loss: 1.8484 - val_accuracy: 0.4417\n",
      "Epoch 61/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5154 - accuracy: 0.5171\n",
      "Epoch 00061: val_loss did not improve from 1.83564\n",
      "26/26 [==============================] - 6s 239ms/step - loss: 1.5154 - accuracy: 0.5171 - val_loss: 1.8493 - val_accuracy: 0.4464\n",
      "Epoch 62/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4921 - accuracy: 0.5202\n",
      "Epoch 00062: val_loss improved from 1.83564 to 1.83271, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 1.4921 - accuracy: 0.5202 - val_loss: 1.8327 - val_accuracy: 0.4429\n",
      "Epoch 63/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4870 - accuracy: 0.5222\n",
      "Epoch 00063: val_loss improved from 1.83271 to 1.82245, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 1.4870 - accuracy: 0.5222 - val_loss: 1.8225 - val_accuracy: 0.4405\n",
      "Epoch 64/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4797 - accuracy: 0.5278\n",
      "Epoch 00064: val_loss did not improve from 1.82245\n",
      "26/26 [==============================] - 7s 278ms/step - loss: 1.4797 - accuracy: 0.5278 - val_loss: 1.8489 - val_accuracy: 0.4262\n",
      "Epoch 65/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4541 - accuracy: 0.5369\n",
      "Epoch 00065: val_loss did not improve from 1.82245\n",
      "26/26 [==============================] - 7s 267ms/step - loss: 1.4541 - accuracy: 0.5369 - val_loss: 1.8559 - val_accuracy: 0.4250\n",
      "Epoch 66/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4794 - accuracy: 0.5317\n",
      "Epoch 00066: val_loss did not improve from 1.82245\n",
      "26/26 [==============================] - 7s 257ms/step - loss: 1.4794 - accuracy: 0.5317 - val_loss: 1.8445 - val_accuracy: 0.4452\n",
      "Epoch 67/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4319 - accuracy: 0.5417\n",
      "Epoch 00067: val_loss improved from 1.82245 to 1.82077, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 8s 323ms/step - loss: 1.4319 - accuracy: 0.5417 - val_loss: 1.8208 - val_accuracy: 0.4548\n",
      "Epoch 68/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4506 - accuracy: 0.5306\n",
      "Epoch 00068: val_loss did not improve from 1.82077\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.4506 - accuracy: 0.5306 - val_loss: 1.8222 - val_accuracy: 0.4262\n",
      "Epoch 69/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4357 - accuracy: 0.5444\n",
      "Epoch 00069: val_loss improved from 1.82077 to 1.81136, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.4357 - accuracy: 0.5444 - val_loss: 1.8114 - val_accuracy: 0.4583\n",
      "Epoch 70/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4161 - accuracy: 0.5417\n",
      "Epoch 00070: val_loss did not improve from 1.81136\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 1.4161 - accuracy: 0.5417 - val_loss: 1.8256 - val_accuracy: 0.4452\n",
      "Epoch 71/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4330 - accuracy: 0.5329\n",
      "Epoch 00071: val_loss did not improve from 1.81136\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 1.4330 - accuracy: 0.5329 - val_loss: 1.8274 - val_accuracy: 0.4619\n",
      "Epoch 72/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4232 - accuracy: 0.5508\n",
      "Epoch 00072: val_loss improved from 1.81136 to 1.79574, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.4232 - accuracy: 0.5508 - val_loss: 1.7957 - val_accuracy: 0.4655\n",
      "Epoch 73/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3941 - accuracy: 0.5429\n",
      "Epoch 00073: val_loss improved from 1.79574 to 1.79390, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.3941 - accuracy: 0.5429 - val_loss: 1.7939 - val_accuracy: 0.4679\n",
      "Epoch 74/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3800 - accuracy: 0.5488\n",
      "Epoch 00074: val_loss did not improve from 1.79390\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 1.3800 - accuracy: 0.5488 - val_loss: 1.8156 - val_accuracy: 0.4595\n",
      "Epoch 75/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3797 - accuracy: 0.5560\n",
      "Epoch 00075: val_loss did not improve from 1.79390\n",
      "26/26 [==============================] - 7s 251ms/step - loss: 1.3797 - accuracy: 0.5560 - val_loss: 1.8099 - val_accuracy: 0.4595\n",
      "Epoch 76/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3988 - accuracy: 0.5536\n",
      "Epoch 00076: val_loss did not improve from 1.79390\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 1.3988 - accuracy: 0.5536 - val_loss: 1.8002 - val_accuracy: 0.4655\n",
      "Epoch 77/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3997 - accuracy: 0.5524\n",
      "Epoch 00077: val_loss did not improve from 1.79390\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.3997 - accuracy: 0.5524 - val_loss: 1.8129 - val_accuracy: 0.4464\n",
      "Epoch 78/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3731 - accuracy: 0.5575\n",
      "Epoch 00078: val_loss improved from 1.79390 to 1.78970, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 245ms/step - loss: 1.3731 - accuracy: 0.5575 - val_loss: 1.7897 - val_accuracy: 0.4595\n",
      "Epoch 79/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3642 - accuracy: 0.5607\n",
      "Epoch 00079: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 1.3642 - accuracy: 0.5607 - val_loss: 1.7986 - val_accuracy: 0.4595\n",
      "Epoch 80/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3650 - accuracy: 0.5714\n",
      "Epoch 00080: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.3650 - accuracy: 0.5714 - val_loss: 1.8137 - val_accuracy: 0.4643\n",
      "Epoch 81/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3495 - accuracy: 0.5587\n",
      "Epoch 00081: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.3495 - accuracy: 0.5587 - val_loss: 1.8408 - val_accuracy: 0.4512\n",
      "Epoch 82/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3251 - accuracy: 0.5679\n",
      "Epoch 00082: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 1.3251 - accuracy: 0.5679 - val_loss: 1.8110 - val_accuracy: 0.4690\n",
      "Epoch 83/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3425 - accuracy: 0.5667\n",
      "Epoch 00083: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 1.3425 - accuracy: 0.5667 - val_loss: 1.7943 - val_accuracy: 0.4655\n",
      "Epoch 84/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3114 - accuracy: 0.5766\n",
      "Epoch 00084: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 1.3114 - accuracy: 0.5766 - val_loss: 1.8028 - val_accuracy: 0.4607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3050 - accuracy: 0.5738\n",
      "Epoch 00085: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 6s 244ms/step - loss: 1.3050 - accuracy: 0.5738 - val_loss: 1.8050 - val_accuracy: 0.4690\n",
      "Epoch 86/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3180 - accuracy: 0.5706\n",
      "Epoch 00086: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 7s 270ms/step - loss: 1.3180 - accuracy: 0.5706 - val_loss: 1.8410 - val_accuracy: 0.4595\n",
      "Epoch 87/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3062 - accuracy: 0.5806\n",
      "Epoch 00087: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 8s 318ms/step - loss: 1.3062 - accuracy: 0.5806 - val_loss: 1.8233 - val_accuracy: 0.4619\n",
      "Epoch 88/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2981 - accuracy: 0.5770\n",
      "Epoch 00088: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 8s 308ms/step - loss: 1.2981 - accuracy: 0.5770 - val_loss: 1.8029 - val_accuracy: 0.4619\n",
      "Epoch 89/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2823 - accuracy: 0.5841\n",
      "Epoch 00089: val_loss did not improve from 1.78970\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.2823 - accuracy: 0.5841 - val_loss: 1.8163 - val_accuracy: 0.4571\n",
      "Epoch 90/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2874 - accuracy: 0.5845\n",
      "Epoch 00090: val_loss improved from 1.78970 to 1.78656, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 246ms/step - loss: 1.2874 - accuracy: 0.5845 - val_loss: 1.7866 - val_accuracy: 0.4595\n",
      "Epoch 91/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2803 - accuracy: 0.5964\n",
      "Epoch 00091: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 1.2803 - accuracy: 0.5964 - val_loss: 1.7970 - val_accuracy: 0.4821\n",
      "Epoch 92/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2756 - accuracy: 0.5865\n",
      "Epoch 00092: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 1.2756 - accuracy: 0.5865 - val_loss: 1.8234 - val_accuracy: 0.4714\n",
      "Epoch 93/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2840 - accuracy: 0.5869\n",
      "Epoch 00093: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 7s 279ms/step - loss: 1.2840 - accuracy: 0.5869 - val_loss: 1.8563 - val_accuracy: 0.4774\n",
      "Epoch 94/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2898 - accuracy: 0.5833\n",
      "Epoch 00094: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 8s 287ms/step - loss: 1.2898 - accuracy: 0.5833 - val_loss: 1.8492 - val_accuracy: 0.4607\n",
      "Epoch 95/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2658 - accuracy: 0.5881\n",
      "Epoch 00095: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 1.2658 - accuracy: 0.5881 - val_loss: 1.8220 - val_accuracy: 0.4619\n",
      "Epoch 96/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2384 - accuracy: 0.6071\n",
      "Epoch 00096: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 1.2384 - accuracy: 0.6071 - val_loss: 1.8199 - val_accuracy: 0.4679\n",
      "Epoch 97/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2560 - accuracy: 0.5786\n",
      "Epoch 00097: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 7s 282ms/step - loss: 1.2560 - accuracy: 0.5786 - val_loss: 1.7999 - val_accuracy: 0.4952\n",
      "Epoch 98/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2362 - accuracy: 0.6044\n",
      "Epoch 00098: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 1.2362 - accuracy: 0.6044 - val_loss: 1.7871 - val_accuracy: 0.4929\n",
      "Epoch 99/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2307 - accuracy: 0.6020\n",
      "Epoch 00099: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 6s 240ms/step - loss: 1.2307 - accuracy: 0.6020 - val_loss: 1.8066 - val_accuracy: 0.4690\n",
      "Epoch 100/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2338 - accuracy: 0.6016\n",
      "Epoch 00100: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 1.2338 - accuracy: 0.6016 - val_loss: 1.8261 - val_accuracy: 0.4643\n",
      "Epoch 101/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2196 - accuracy: 0.6147\n",
      "Epoch 00101: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 7s 254ms/step - loss: 1.2196 - accuracy: 0.6147 - val_loss: 1.8114 - val_accuracy: 0.4964\n",
      "Epoch 102/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2308 - accuracy: 0.5944\n",
      "Epoch 00102: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 6s 241ms/step - loss: 1.2308 - accuracy: 0.5944 - val_loss: 1.8072 - val_accuracy: 0.4833\n",
      "Epoch 103/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1992 - accuracy: 0.6111\n",
      "Epoch 00103: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.1992 - accuracy: 0.6111 - val_loss: 1.8017 - val_accuracy: 0.4845\n",
      "Epoch 104/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2155 - accuracy: 0.6000\n",
      "Epoch 00104: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.2155 - accuracy: 0.6000 - val_loss: 1.7955 - val_accuracy: 0.4774\n",
      "Epoch 105/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2161 - accuracy: 0.6099\n",
      "Epoch 00105: val_loss did not improve from 1.78656\n",
      "26/26 [==============================] - 7s 266ms/step - loss: 1.2161 - accuracy: 0.6099 - val_loss: 1.7900 - val_accuracy: 0.5036\n",
      "Epoch 106/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1971 - accuracy: 0.6083\n",
      "Epoch 00106: val_loss improved from 1.78656 to 1.78580, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 1.1971 - accuracy: 0.6083 - val_loss: 1.7858 - val_accuracy: 0.4929\n",
      "Epoch 107/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1890 - accuracy: 0.6167\n",
      "Epoch 00107: val_loss did not improve from 1.78580\n",
      "26/26 [==============================] - 6s 242ms/step - loss: 1.1890 - accuracy: 0.6167 - val_loss: 1.8190 - val_accuracy: 0.4821\n",
      "Epoch 108/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1942 - accuracy: 0.6135\n",
      "Epoch 00108: val_loss did not improve from 1.78580\n",
      "26/26 [==============================] - 7s 258ms/step - loss: 1.1942 - accuracy: 0.6135 - val_loss: 1.7882 - val_accuracy: 0.4845\n",
      "Epoch 109/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1726 - accuracy: 0.6194\n",
      "Epoch 00109: val_loss did not improve from 1.78580\n",
      "26/26 [==============================] - 7s 265ms/step - loss: 1.1726 - accuracy: 0.6194 - val_loss: 1.7954 - val_accuracy: 0.4786\n",
      "Epoch 110/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1938 - accuracy: 0.6036\n",
      "Epoch 00110: val_loss did not improve from 1.78580\n",
      "26/26 [==============================] - 7s 267ms/step - loss: 1.1938 - accuracy: 0.6036 - val_loss: 1.8110 - val_accuracy: 0.4905\n",
      "Epoch 111/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1745 - accuracy: 0.6119\n",
      "Epoch 00111: val_loss did not improve from 1.78580\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 1.1745 - accuracy: 0.6119 - val_loss: 1.8126 - val_accuracy: 0.4929\n",
      "Epoch 112/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1885 - accuracy: 0.6210\n",
      "Epoch 00112: val_loss improved from 1.78580 to 1.78341, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 1.1885 - accuracy: 0.6210 - val_loss: 1.7834 - val_accuracy: 0.5036\n",
      "Epoch 113/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1662 - accuracy: 0.6254\n",
      "Epoch 00113: val_loss did not improve from 1.78341\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.1662 - accuracy: 0.6254 - val_loss: 1.8213 - val_accuracy: 0.4964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1673 - accuracy: 0.6167\n",
      "Epoch 00114: val_loss did not improve from 1.78341\n",
      "26/26 [==============================] - 7s 253ms/step - loss: 1.1673 - accuracy: 0.6167 - val_loss: 1.8342 - val_accuracy: 0.4655\n",
      "Epoch 115/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1571 - accuracy: 0.6222\n",
      "Epoch 00115: val_loss did not improve from 1.78341\n",
      "26/26 [==============================] - 9s 332ms/step - loss: 1.1571 - accuracy: 0.6222 - val_loss: 1.8118 - val_accuracy: 0.5095\n",
      "Epoch 116/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1972 - accuracy: 0.6187\n",
      "Epoch 00116: val_loss improved from 1.78341 to 1.78217, saving model to weights6.best.hdf5\n",
      "26/26 [==============================] - 8s 310ms/step - loss: 1.1972 - accuracy: 0.6187 - val_loss: 1.7822 - val_accuracy: 0.4964\n",
      "Epoch 117/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1340 - accuracy: 0.6298\n",
      "Epoch 00117: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 238ms/step - loss: 1.1340 - accuracy: 0.6298 - val_loss: 1.8025 - val_accuracy: 0.4893\n",
      "Epoch 118/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1307 - accuracy: 0.6234\n",
      "Epoch 00118: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 237ms/step - loss: 1.1307 - accuracy: 0.6234 - val_loss: 1.8186 - val_accuracy: 0.5036\n",
      "Epoch 119/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1308 - accuracy: 0.6357\n",
      "Epoch 00119: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 261ms/step - loss: 1.1308 - accuracy: 0.6357 - val_loss: 1.8153 - val_accuracy: 0.5119\n",
      "Epoch 120/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1350 - accuracy: 0.6325\n",
      "Epoch 00120: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 8s 320ms/step - loss: 1.1350 - accuracy: 0.6325 - val_loss: 1.8009 - val_accuracy: 0.5048\n",
      "Epoch 121/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1550 - accuracy: 0.6254\n",
      "Epoch 00121: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 231ms/step - loss: 1.1550 - accuracy: 0.6254 - val_loss: 1.7884 - val_accuracy: 0.5143\n",
      "Epoch 122/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1140 - accuracy: 0.6373\n",
      "Epoch 00122: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 233ms/step - loss: 1.1140 - accuracy: 0.6373 - val_loss: 1.8165 - val_accuracy: 0.5048\n",
      "Epoch 123/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1079 - accuracy: 0.6325\n",
      "Epoch 00123: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 235ms/step - loss: 1.1079 - accuracy: 0.6325 - val_loss: 1.7906 - val_accuracy: 0.5048\n",
      "Epoch 124/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0982 - accuracy: 0.6357\n",
      "Epoch 00124: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 235ms/step - loss: 1.0982 - accuracy: 0.6357 - val_loss: 1.8469 - val_accuracy: 0.5036\n",
      "Epoch 125/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1226 - accuracy: 0.6333\n",
      "Epoch 00125: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 234ms/step - loss: 1.1226 - accuracy: 0.6333 - val_loss: 1.8161 - val_accuracy: 0.5131\n",
      "Epoch 126/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1189 - accuracy: 0.6373\n",
      "Epoch 00126: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 243ms/step - loss: 1.1189 - accuracy: 0.6373 - val_loss: 1.8334 - val_accuracy: 0.5167\n",
      "Epoch 127/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0974 - accuracy: 0.6401\n",
      "Epoch 00127: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 288ms/step - loss: 1.0974 - accuracy: 0.6401 - val_loss: 1.8400 - val_accuracy: 0.4964\n",
      "Epoch 128/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0949 - accuracy: 0.6393\n",
      "Epoch 00128: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 251ms/step - loss: 1.0949 - accuracy: 0.6393 - val_loss: 1.8183 - val_accuracy: 0.5131\n",
      "Epoch 129/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0759 - accuracy: 0.6429\n",
      "Epoch 00129: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 270ms/step - loss: 1.0759 - accuracy: 0.6429 - val_loss: 1.8466 - val_accuracy: 0.5000\n",
      "Epoch 130/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0950 - accuracy: 0.6393\n",
      "Epoch 00130: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 253ms/step - loss: 1.0950 - accuracy: 0.6393 - val_loss: 1.8478 - val_accuracy: 0.5000\n",
      "Epoch 131/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.6298\n",
      "Epoch 00131: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 257ms/step - loss: 1.0986 - accuracy: 0.6298 - val_loss: 1.8517 - val_accuracy: 0.4940\n",
      "Epoch 132/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0785 - accuracy: 0.6433\n",
      "Epoch 00132: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 1.0785 - accuracy: 0.6433 - val_loss: 1.8381 - val_accuracy: 0.5024\n",
      "Epoch 133/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0851 - accuracy: 0.6357\n",
      "Epoch 00133: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 260ms/step - loss: 1.0851 - accuracy: 0.6357 - val_loss: 1.8599 - val_accuracy: 0.4976\n",
      "Epoch 134/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0586 - accuracy: 0.6563\n",
      "Epoch 00134: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 1.0586 - accuracy: 0.6563 - val_loss: 1.8874 - val_accuracy: 0.5036\n",
      "Epoch 135/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0827 - accuracy: 0.6413\n",
      "Epoch 00135: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 253ms/step - loss: 1.0827 - accuracy: 0.6413 - val_loss: 1.8446 - val_accuracy: 0.5000\n",
      "Epoch 136/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0676 - accuracy: 0.6544\n",
      "Epoch 00136: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 248ms/step - loss: 1.0676 - accuracy: 0.6544 - val_loss: 1.8186 - val_accuracy: 0.5024\n",
      "Epoch 137/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0539 - accuracy: 0.6560\n",
      "Epoch 00137: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 6s 247ms/step - loss: 1.0539 - accuracy: 0.6560 - val_loss: 1.8393 - val_accuracy: 0.5036\n",
      "Epoch 138/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0568 - accuracy: 0.6575\n",
      "Epoch 00138: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 8s 291ms/step - loss: 1.0568 - accuracy: 0.6575 - val_loss: 1.8414 - val_accuracy: 0.5143\n",
      "Epoch 139/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0758 - accuracy: 0.6393\n",
      "Epoch 00139: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 8s 320ms/step - loss: 1.0758 - accuracy: 0.6393 - val_loss: 1.8723 - val_accuracy: 0.5036\n",
      "Epoch 140/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0655 - accuracy: 0.6444\n",
      "Epoch 00140: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 8s 301ms/step - loss: 1.0655 - accuracy: 0.6444 - val_loss: 1.8535 - val_accuracy: 0.4976\n",
      "Epoch 141/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0328 - accuracy: 0.6611\n",
      "Epoch 00141: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 278ms/step - loss: 1.0328 - accuracy: 0.6611 - val_loss: 1.8830 - val_accuracy: 0.5083\n",
      "Epoch 142/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0572 - accuracy: 0.6480\n",
      "Epoch 00142: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 262ms/step - loss: 1.0572 - accuracy: 0.6480 - val_loss: 1.8618 - val_accuracy: 0.5071\n",
      "Epoch 143/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - ETA: 0s - loss: 1.0638 - accuracy: 0.6456\n",
      "Epoch 00143: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 251ms/step - loss: 1.0638 - accuracy: 0.6456 - val_loss: 1.8913 - val_accuracy: 0.5012\n",
      "Epoch 144/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0210 - accuracy: 0.6647\n",
      "Epoch 00144: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 252ms/step - loss: 1.0210 - accuracy: 0.6647 - val_loss: 1.8895 - val_accuracy: 0.4988\n",
      "Epoch 145/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0347 - accuracy: 0.6540\n",
      "Epoch 00145: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 263ms/step - loss: 1.0347 - accuracy: 0.6540 - val_loss: 1.8848 - val_accuracy: 0.5131\n",
      "Epoch 146/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0426 - accuracy: 0.6548\n",
      "Epoch 00146: val_loss did not improve from 1.78217\n",
      "26/26 [==============================] - 7s 262ms/step - loss: 1.0426 - accuracy: 0.6548 - val_loss: 1.8745 - val_accuracy: 0.4905\n",
      "Training completed in time:  0:16:18.090665\n"
     ]
    }
   ],
   "source": [
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights6.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "\n",
    "model_relu.fit(X_train2, y_train,\n",
    "          batch_size = 100, \n",
    "          epochs = 300,\n",
    "          validation_data = (X_val2, y_val), \n",
    "          callbacks=[checkpointer, es])\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training-Validation Accuracy')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#saving the model history\n",
    "loss = pd.DataFrame(model_relu.history.history)\n",
    "\n",
    "\n",
    "#plotting the loss and accuracy \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(loss[\"loss\"], label =\"Loss\")\n",
    "plt.plot(loss[\"val_loss\"], label = \"Validation_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(loss['accuracy'],label = \"Training Accuracy\")\n",
    "plt.plot(loss['val_accuracy'], label =\"Validation_ Accuracy \")\n",
    "plt.legend()\n",
    "plt.title(\"Training-Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20  5 11 17 18  3 11 16 15 20 18  1 16  3 10  6  0 18 20 11 11  2  0 18\n",
      " 12  7  9  5 19  5  5 16  2 17 14 19 10  8  0  7  5 16 10  7 19 13  9 13\n",
      "  5 16  3  8  6 14  0 17 19  8 18  1  3  7 11  8 15  0 11 12  5 10  5  1\n",
      "  3 18  3 16  7  6  3 17 10 17  5 15 10 18  2 14 10 11  3 19 11 19  1 12\n",
      "  8  9 11  4  6  6 16  3 20 19  3  9 19  4  5  8  7 10 16  3 15 18  5 10\n",
      " 11 11 18 11 12 10 11 10  7 13 17 19 17 10 17 10 14  2  8  3  3 19  6 19\n",
      "  3  3 18  0 17  5  5  8 18  5 10 17 20 12 18 19 18 16 20  5  3 17 20  8\n",
      "  8  3 13 12  0 18  6  6  5 13 18 16  1 15 17  5 19 14 15 18  0  6  5 13\n",
      " 11 14  3 12  3  2 19 13 17  2 20  2 12  5  6  3 12  8  2  0 12  0  9  1\n",
      " 17 14 10 19 18  3 18 18 17  6 12 10  3 12 18  3 18  5 19  8  3 15  6 16\n",
      " 16  1 17 17 15 13  3 19 18 17 16 12 18 17 14  1 13  6 19 16  5 19 19  2\n",
      "  2 13  5  2 13 10 11  0  8 18 17  7 18  6  5  9 20  1  6 13  5 15 11  3\n",
      " 16  5 18 11 17 20 19 16  2 16 14  8 16  1 16  0 15  4 14 16  1  8  5  0\n",
      "  1  5 20  8  5 14  6 16 12 16 13  5  2 17  2 19  7 13 12 16 19 16 18 16\n",
      "  3  2  1 19  5  3  2 17  9  7 15 11  3  2 11 16 16  3  8 15  3  1  3 11\n",
      "  9 11  8 20  3 11  2 18  3 11  5  7  3 10 18 18 16 18 14 14 16 17 18 12\n",
      " 16  2  3  7  9 17 13 13  0 12  3 14 10  5  3  1  9 18 17 20  5  5 18  3\n",
      "  3 15  5 16  8 15  9  3  5 12  6  3 14 17  5 12 16 17 20 17  8 13  7 19\n",
      "  9  5 17  5 19 12 15  8 11 15 20 16 18  7 18  1 19 20  2 18  8 14  9 12\n",
      "  1 11  5 18  9  3  2  2 18  3 13  5  9 18 15 14 17 18  5  3 17  7  9 12\n",
      "  5 10  8 11 14  3  9  3  3 17 17  1 20  1 15 15  0  6 13 20  8 20 16 20\n",
      "  5 11  2 10  1  9  3  0 10  4  8  5  3 17 18  3 20 14  7  8  5 19  7 13\n",
      " 14 18  8  2  2 17  2  1 20  5 15 14 17 13 14  8 16 11  1  3  8 12  0  6\n",
      "  3  5  9  5  2  1  5 13 11  2  1  5 13 20  2 10 12 19  9  5  8  8 17 20\n",
      " 11  3  1  9  1 11  5 15  3  1 16 10  7 17  3  8 16 11  3 11  5  8  1 19\n",
      "  8  2  9 19  0  6  2 15 13  8  8 10 16 11  1 17 15  5  4 16 20 10  8  2\n",
      " 15 17 11 12  7 20 14 17  5  3 20 15  0 18  8 12 18  7  6 18  8 14  6  2\n",
      "  1 12  5  7  2  8 14 12 17 14 11 17 18 13 16  9 10 10  2 12 13 11 10  5\n",
      " 14  5  3  5  6 13  1 17  3  6 10  4  9  2 14 17 11 18 15 13 18 17 17 13\n",
      " 14  1  5 14 10  5 14 18 11 19  7 17  9  3  5  5  8 19 18 17 16  0 13 13\n",
      " 14 20  9 14  8 14  8 18  3  2  5 10 10 18 17  5 11  3 20 17 18  6 12  6\n",
      "  4  9 10  5  8 16  9  3 12  8  8 14 10 12  3  1 17  5  4  1  9 14 18  4\n",
      "  9 17  3  5 18 10 13 11  3 15 17 11  8 16  9 20  3 14  2 17 19 10 17  5\n",
      " 10  0 14  7  2 16 11 17 19 18 15  2 13 16 20  8  3 18 15 19  6 15 11 17\n",
      " 10 15  1  5 13 17  2 13 14  7  3 15 11 11 19  5  1  7 13 15  8  3  8  9]\n"
     ]
    }
   ],
   "source": [
    "prediction = model_relu.predict(X_val2)\n",
    "\n",
    "# finding class with larget predicted probability using argmax of numpy \n",
    "y_pred = np.argmax(prediction, axis = 1)  # prediction using model \n",
    "y_val_orig = np.argmax(y_val, axis = 1) # original y_val\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.04      0.06        45\n",
      "           1       0.39      0.32      0.35        44\n",
      "           2       0.32      0.33      0.32        40\n",
      "           3       0.35      0.66      0.46        38\n",
      "           4       0.33      0.07      0.12        42\n",
      "           5       0.17      0.41      0.24        29\n",
      "           6       0.30      0.22      0.25        37\n",
      "           7       0.36      0.20      0.26        45\n",
      "           8       0.12      0.21      0.15        28\n",
      "           9       0.38      0.23      0.29        52\n",
      "          10       0.28      0.30      0.29        37\n",
      "          11       0.63      0.85      0.72        34\n",
      "          12       0.91      0.88      0.89        33\n",
      "          13       0.58      0.60      0.59        35\n",
      "          14       0.69      0.55      0.61        49\n",
      "          15       0.91      0.65      0.76        46\n",
      "          16       0.80      0.67      0.73        54\n",
      "          17       0.55      1.00      0.71        33\n",
      "          18       0.71      0.87      0.78        47\n",
      "          19       0.76      0.88      0.81        32\n",
      "          20       0.74      0.57      0.65        40\n",
      "\n",
      "    accuracy                           0.49       840\n",
      "   macro avg       0.49      0.50      0.48       840\n",
      "weighted avg       0.50      0.49      0.48       840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_orig, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_34 (Conv2D)          (None, 39, 173, 16)       80        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 39, 173, 16)      64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPoolin  (None, 19, 86, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 19, 86, 16)        0         \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 18, 85, 32)        2080      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 18, 85, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPoolin  (None, 9, 42, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 9, 42, 32)         0         \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 8, 41, 64)         8256      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 41, 64)        256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_33 (MaxPoolin  (None, 4, 20, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 4, 20, 64)         0         \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 3, 19, 128)        32896     \n",
      "                                                                 \n",
      " max_pooling2d_34 (MaxPoolin  (None, 1, 9, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 1, 9, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_6   (None, 128)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 21)                2709      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,469\n",
      "Trainable params: 46,245\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_elu = Sequential()\n",
    "model_elu.add(Conv2D(filters=16, kernel_size=2, input_shape=input_shape, activation='elu'))\n",
    "model_elu.add(BatchNormalization())\n",
    "model_elu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_elu.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model_elu.add(Conv2D(filters=32, kernel_size=2, activation='elu'))\n",
    "model_elu.add(BatchNormalization())\n",
    "model_elu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_elu.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model_elu.add(Conv2D(filters=64, kernel_size=2, activation='elu'))\n",
    "model_elu.add(BatchNormalization())\n",
    "model_elu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_elu.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model_elu.add(Conv2D(filters=128, kernel_size=2, activation='elu'))\n",
    "model_elu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_elu.add(Dropout(0.2))\n",
    "model_elu.add(GlobalAveragePooling2D())\n",
    "model_elu.add(Flatten())\n",
    "model_elu.add(Dense(21, activation='softmax'))\n",
    "\n",
    "model_elu.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model_elu.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.8670 - accuracy: 0.1448\n",
      "Epoch 00001: val_loss improved from inf to 3.02174, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 12s 419ms/step - loss: 2.8670 - accuracy: 0.1448 - val_loss: 3.0217 - val_accuracy: 0.0548\n",
      "Epoch 2/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.4333 - accuracy: 0.2230\n",
      "Epoch 00002: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 385ms/step - loss: 2.4333 - accuracy: 0.2230 - val_loss: 3.0632 - val_accuracy: 0.0405\n",
      "Epoch 3/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.2859 - accuracy: 0.2710\n",
      "Epoch 00003: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 373ms/step - loss: 2.2859 - accuracy: 0.2710 - val_loss: 3.1531 - val_accuracy: 0.0405\n",
      "Epoch 4/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.1761 - accuracy: 0.2952\n",
      "Epoch 00004: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 2.1761 - accuracy: 0.2952 - val_loss: 3.2506 - val_accuracy: 0.0417\n",
      "Epoch 5/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.1218 - accuracy: 0.3171\n",
      "Epoch 00005: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 2.1218 - accuracy: 0.3171 - val_loss: 3.3658 - val_accuracy: 0.0417\n",
      "Epoch 6/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.0746 - accuracy: 0.3313\n",
      "Epoch 00006: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 378ms/step - loss: 2.0746 - accuracy: 0.3313 - val_loss: 3.5221 - val_accuracy: 0.0429\n",
      "Epoch 7/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.0133 - accuracy: 0.3575\n",
      "Epoch 00007: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 389ms/step - loss: 2.0133 - accuracy: 0.3575 - val_loss: 3.5516 - val_accuracy: 0.0464\n",
      "Epoch 8/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.9742 - accuracy: 0.3583\n",
      "Epoch 00008: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 15s 587ms/step - loss: 1.9742 - accuracy: 0.3583 - val_loss: 3.6857 - val_accuracy: 0.0524\n",
      "Epoch 9/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.9483 - accuracy: 0.3742\n",
      "Epoch 00009: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 14s 554ms/step - loss: 1.9483 - accuracy: 0.3742 - val_loss: 3.6336 - val_accuracy: 0.0583\n",
      "Epoch 10/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.9325 - accuracy: 0.3829\n",
      "Epoch 00010: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 11s 401ms/step - loss: 1.9325 - accuracy: 0.3829 - val_loss: 3.6129 - val_accuracy: 0.0679\n",
      "Epoch 11/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.8682 - accuracy: 0.4036\n",
      "Epoch 00011: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 374ms/step - loss: 1.8682 - accuracy: 0.4036 - val_loss: 3.6729 - val_accuracy: 0.0798\n",
      "Epoch 12/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.8551 - accuracy: 0.4052\n",
      "Epoch 00012: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 372ms/step - loss: 1.8551 - accuracy: 0.4052 - val_loss: 3.7387 - val_accuracy: 0.0833\n",
      "Epoch 13/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7946 - accuracy: 0.4218\n",
      "Epoch 00013: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 380ms/step - loss: 1.7946 - accuracy: 0.4218 - val_loss: 3.5242 - val_accuracy: 0.1071\n",
      "Epoch 14/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7785 - accuracy: 0.4262\n",
      "Epoch 00014: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 373ms/step - loss: 1.7785 - accuracy: 0.4262 - val_loss: 3.4367 - val_accuracy: 0.1262\n",
      "Epoch 15/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7543 - accuracy: 0.4393\n",
      "Epoch 00015: val_loss did not improve from 3.02174\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.7543 - accuracy: 0.4393 - val_loss: 3.1619 - val_accuracy: 0.1464\n",
      "Epoch 16/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7406 - accuracy: 0.4409\n",
      "Epoch 00016: val_loss improved from 3.02174 to 3.01428, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.7406 - accuracy: 0.4409 - val_loss: 3.0143 - val_accuracy: 0.1524\n",
      "Epoch 17/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6994 - accuracy: 0.4532\n",
      "Epoch 00017: val_loss did not improve from 3.01428\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 1.6994 - accuracy: 0.4532 - val_loss: 3.0865 - val_accuracy: 0.1643\n",
      "Epoch 18/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7049 - accuracy: 0.4440\n",
      "Epoch 00018: val_loss improved from 3.01428 to 2.73417, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 385ms/step - loss: 1.7049 - accuracy: 0.4440 - val_loss: 2.7342 - val_accuracy: 0.2083\n",
      "Epoch 19/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6746 - accuracy: 0.4599\n",
      "Epoch 00019: val_loss improved from 2.73417 to 2.61447, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 13s 487ms/step - loss: 1.6746 - accuracy: 0.4599 - val_loss: 2.6145 - val_accuracy: 0.2179\n",
      "Epoch 20/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6346 - accuracy: 0.4734\n",
      "Epoch 00020: val_loss improved from 2.61447 to 2.48363, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 376ms/step - loss: 1.6346 - accuracy: 0.4734 - val_loss: 2.4836 - val_accuracy: 0.2643\n",
      "Epoch 21/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.6205 - accuracy: 0.4865\n",
      "Epoch 00021: val_loss improved from 2.48363 to 2.43774, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 1.6205 - accuracy: 0.4865 - val_loss: 2.4377 - val_accuracy: 0.2750\n",
      "Epoch 22/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5924 - accuracy: 0.4921\n",
      "Epoch 00022: val_loss did not improve from 2.43774\n",
      "26/26 [==============================] - 10s 378ms/step - loss: 1.5924 - accuracy: 0.4921 - val_loss: 2.7089 - val_accuracy: 0.2405\n",
      "Epoch 23/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5686 - accuracy: 0.5040\n",
      "Epoch 00023: val_loss did not improve from 2.43774\n",
      "26/26 [==============================] - 12s 459ms/step - loss: 1.5686 - accuracy: 0.5040 - val_loss: 2.8264 - val_accuracy: 0.2762\n",
      "Epoch 24/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5483 - accuracy: 0.5107\n",
      "Epoch 00024: val_loss did not improve from 2.43774\n",
      "26/26 [==============================] - 12s 445ms/step - loss: 1.5483 - accuracy: 0.5107 - val_loss: 3.0928 - val_accuracy: 0.2429\n",
      "Epoch 25/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5352 - accuracy: 0.5190\n",
      "Epoch 00025: val_loss did not improve from 2.43774\n",
      "26/26 [==============================] - 10s 376ms/step - loss: 1.5352 - accuracy: 0.5190 - val_loss: 3.0966 - val_accuracy: 0.2643\n",
      "Epoch 26/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5464 - accuracy: 0.5087\n",
      "Epoch 00026: val_loss did not improve from 2.43774\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 1.5464 - accuracy: 0.5087 - val_loss: 3.2493 - val_accuracy: 0.2643\n",
      "Epoch 27/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.5265 - accuracy: 0.5091\n",
      "Epoch 00027: val_loss did not improve from 2.43774\n",
      "26/26 [==============================] - 10s 378ms/step - loss: 1.5265 - accuracy: 0.5091 - val_loss: 2.9502 - val_accuracy: 0.2929\n",
      "Epoch 28/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4938 - accuracy: 0.5278\n",
      "Epoch 00028: val_loss improved from 2.43774 to 2.43085, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 380ms/step - loss: 1.4938 - accuracy: 0.5278 - val_loss: 2.4308 - val_accuracy: 0.3298\n",
      "Epoch 29/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4712 - accuracy: 0.5345\n",
      "Epoch 00029: val_loss did not improve from 2.43085\n",
      "26/26 [==============================] - 10s 387ms/step - loss: 1.4712 - accuracy: 0.5345 - val_loss: 2.8296 - val_accuracy: 0.2810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4721 - accuracy: 0.5234\n",
      "Epoch 00030: val_loss did not improve from 2.43085\n",
      "26/26 [==============================] - 10s 378ms/step - loss: 1.4721 - accuracy: 0.5234 - val_loss: 2.7248 - val_accuracy: 0.3083\n",
      "Epoch 31/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4434 - accuracy: 0.5298\n",
      "Epoch 00031: val_loss improved from 2.43085 to 2.34051, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 12s 457ms/step - loss: 1.4434 - accuracy: 0.5298 - val_loss: 2.3405 - val_accuracy: 0.3524\n",
      "Epoch 32/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4417 - accuracy: 0.5325\n",
      "Epoch 00032: val_loss improved from 2.34051 to 1.99511, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.4417 - accuracy: 0.5325 - val_loss: 1.9951 - val_accuracy: 0.4060\n",
      "Epoch 33/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4363 - accuracy: 0.5345\n",
      "Epoch 00033: val_loss improved from 1.99511 to 1.98971, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 374ms/step - loss: 1.4363 - accuracy: 0.5345 - val_loss: 1.9897 - val_accuracy: 0.4048\n",
      "Epoch 34/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.4268 - accuracy: 0.5421\n",
      "Epoch 00034: val_loss improved from 1.98971 to 1.95259, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 1.4268 - accuracy: 0.5421 - val_loss: 1.9526 - val_accuracy: 0.4071\n",
      "Epoch 35/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3871 - accuracy: 0.5464\n",
      "Epoch 00035: val_loss did not improve from 1.95259\n",
      "26/26 [==============================] - 10s 376ms/step - loss: 1.3871 - accuracy: 0.5464 - val_loss: 1.9781 - val_accuracy: 0.4119\n",
      "Epoch 36/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3804 - accuracy: 0.5623\n",
      "Epoch 00036: val_loss did not improve from 1.95259\n",
      "26/26 [==============================] - 10s 371ms/step - loss: 1.3804 - accuracy: 0.5623 - val_loss: 1.9810 - val_accuracy: 0.4179\n",
      "Epoch 37/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3690 - accuracy: 0.5571\n",
      "Epoch 00037: val_loss did not improve from 1.95259\n",
      "26/26 [==============================] - 10s 371ms/step - loss: 1.3690 - accuracy: 0.5571 - val_loss: 3.2441 - val_accuracy: 0.2786\n",
      "Epoch 38/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3460 - accuracy: 0.5754\n",
      "Epoch 00038: val_loss did not improve from 1.95259\n",
      "26/26 [==============================] - 10s 371ms/step - loss: 1.3460 - accuracy: 0.5754 - val_loss: 3.3176 - val_accuracy: 0.2464\n",
      "Epoch 39/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3290 - accuracy: 0.5742\n",
      "Epoch 00039: val_loss did not improve from 1.95259\n",
      "26/26 [==============================] - 11s 409ms/step - loss: 1.3290 - accuracy: 0.5742 - val_loss: 2.5659 - val_accuracy: 0.3310\n",
      "Epoch 40/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.3511 - accuracy: 0.5583\n",
      "Epoch 00040: val_loss did not improve from 1.95259\n",
      "26/26 [==============================] - 10s 388ms/step - loss: 1.3511 - accuracy: 0.5583 - val_loss: 2.0069 - val_accuracy: 0.4190\n",
      "Epoch 41/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2965 - accuracy: 0.5841\n",
      "Epoch 00041: val_loss did not improve from 1.95259\n",
      "26/26 [==============================] - 10s 390ms/step - loss: 1.2965 - accuracy: 0.5841 - val_loss: 1.9746 - val_accuracy: 0.4238\n",
      "Epoch 42/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2904 - accuracy: 0.5869\n",
      "Epoch 00042: val_loss improved from 1.95259 to 1.92499, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 394ms/step - loss: 1.2904 - accuracy: 0.5869 - val_loss: 1.9250 - val_accuracy: 0.4440\n",
      "Epoch 43/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2836 - accuracy: 0.5925\n",
      "Epoch 00043: val_loss improved from 1.92499 to 1.90803, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 402ms/step - loss: 1.2836 - accuracy: 0.5925 - val_loss: 1.9080 - val_accuracy: 0.4452\n",
      "Epoch 44/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2767 - accuracy: 0.5869\n",
      "Epoch 00044: val_loss did not improve from 1.90803\n",
      "26/26 [==============================] - 11s 415ms/step - loss: 1.2767 - accuracy: 0.5869 - val_loss: 2.1012 - val_accuracy: 0.3940\n",
      "Epoch 45/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2637 - accuracy: 0.5901\n",
      "Epoch 00045: val_loss did not improve from 1.90803\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.2637 - accuracy: 0.5901 - val_loss: 2.4228 - val_accuracy: 0.3143\n",
      "Epoch 46/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2579 - accuracy: 0.5901\n",
      "Epoch 00046: val_loss did not improve from 1.90803\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 1.2579 - accuracy: 0.5901 - val_loss: 1.9846 - val_accuracy: 0.4143\n",
      "Epoch 47/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2750 - accuracy: 0.5877\n",
      "Epoch 00047: val_loss improved from 1.90803 to 1.87253, saving model to weights7.best.hdf5\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 1.2750 - accuracy: 0.5877 - val_loss: 1.8725 - val_accuracy: 0.4810\n",
      "Epoch 48/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2481 - accuracy: 0.6028\n",
      "Epoch 00048: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 1.2481 - accuracy: 0.6028 - val_loss: 2.3062 - val_accuracy: 0.3667\n",
      "Epoch 49/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2302 - accuracy: 0.6095\n",
      "Epoch 00049: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 1.2302 - accuracy: 0.6095 - val_loss: 2.4627 - val_accuracy: 0.3417\n",
      "Epoch 50/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2259 - accuracy: 0.5964\n",
      "Epoch 00050: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 389ms/step - loss: 1.2259 - accuracy: 0.5964 - val_loss: 2.1878 - val_accuracy: 0.3952\n",
      "Epoch 51/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2196 - accuracy: 0.6083\n",
      "Epoch 00051: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 378ms/step - loss: 1.2196 - accuracy: 0.6083 - val_loss: 2.2352 - val_accuracy: 0.3929\n",
      "Epoch 52/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2163 - accuracy: 0.6040\n",
      "Epoch 00052: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 11s 431ms/step - loss: 1.2163 - accuracy: 0.6040 - val_loss: 2.2042 - val_accuracy: 0.4048\n",
      "Epoch 53/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.2087 - accuracy: 0.6004\n",
      "Epoch 00053: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 391ms/step - loss: 1.2087 - accuracy: 0.6004 - val_loss: 2.1790 - val_accuracy: 0.4155\n",
      "Epoch 54/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1999 - accuracy: 0.6048\n",
      "Epoch 00054: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 380ms/step - loss: 1.1999 - accuracy: 0.6048 - val_loss: 2.1439 - val_accuracy: 0.4333\n",
      "Epoch 55/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1697 - accuracy: 0.6183\n",
      "Epoch 00055: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 379ms/step - loss: 1.1697 - accuracy: 0.6183 - val_loss: 2.0270 - val_accuracy: 0.4452\n",
      "Epoch 56/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1598 - accuracy: 0.6242\n",
      "Epoch 00056: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 1.1598 - accuracy: 0.6242 - val_loss: 2.0262 - val_accuracy: 0.4548\n",
      "Epoch 57/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1365 - accuracy: 0.6262\n",
      "Epoch 00057: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 1.1365 - accuracy: 0.6262 - val_loss: 2.2600 - val_accuracy: 0.4012\n",
      "Epoch 58/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1512 - accuracy: 0.6206\n",
      "Epoch 00058: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 376ms/step - loss: 1.1512 - accuracy: 0.6206 - val_loss: 2.0308 - val_accuracy: 0.4440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1413 - accuracy: 0.6238\n",
      "Epoch 00059: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 397ms/step - loss: 1.1413 - accuracy: 0.6238 - val_loss: 1.9754 - val_accuracy: 0.4524\n",
      "Epoch 60/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1384 - accuracy: 0.6306\n",
      "Epoch 00060: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 403ms/step - loss: 1.1384 - accuracy: 0.6306 - val_loss: 1.9545 - val_accuracy: 0.4762\n",
      "Epoch 61/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1282 - accuracy: 0.6286\n",
      "Epoch 00061: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 11s 409ms/step - loss: 1.1282 - accuracy: 0.6286 - val_loss: 2.0729 - val_accuracy: 0.4298\n",
      "Epoch 62/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1219 - accuracy: 0.6389\n",
      "Epoch 00062: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 384ms/step - loss: 1.1219 - accuracy: 0.6389 - val_loss: 2.0807 - val_accuracy: 0.4417\n",
      "Epoch 63/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1313 - accuracy: 0.6317\n",
      "Epoch 00063: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 390ms/step - loss: 1.1313 - accuracy: 0.6317 - val_loss: 2.0525 - val_accuracy: 0.4583\n",
      "Epoch 64/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1169 - accuracy: 0.6302\n",
      "Epoch 00064: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.1169 - accuracy: 0.6302 - val_loss: 2.1204 - val_accuracy: 0.4357\n",
      "Epoch 65/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0837 - accuracy: 0.6500\n",
      "Epoch 00065: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 387ms/step - loss: 1.0837 - accuracy: 0.6500 - val_loss: 2.3429 - val_accuracy: 0.4119\n",
      "Epoch 66/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0896 - accuracy: 0.6353\n",
      "Epoch 00066: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 376ms/step - loss: 1.0896 - accuracy: 0.6353 - val_loss: 2.2302 - val_accuracy: 0.4190\n",
      "Epoch 67/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.1238 - accuracy: 0.6369\n",
      "Epoch 00067: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 373ms/step - loss: 1.1238 - accuracy: 0.6369 - val_loss: 2.4691 - val_accuracy: 0.4012\n",
      "Epoch 68/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0850 - accuracy: 0.6492\n",
      "Epoch 00068: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.0850 - accuracy: 0.6492 - val_loss: 2.3400 - val_accuracy: 0.4048\n",
      "Epoch 69/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0673 - accuracy: 0.6484\n",
      "Epoch 00069: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.0673 - accuracy: 0.6484 - val_loss: 2.7431 - val_accuracy: 0.3810\n",
      "Epoch 70/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0510 - accuracy: 0.6587\n",
      "Epoch 00070: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 11s 423ms/step - loss: 1.0510 - accuracy: 0.6587 - val_loss: 2.5015 - val_accuracy: 0.3810\n",
      "Epoch 71/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0459 - accuracy: 0.6496\n",
      "Epoch 00071: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 384ms/step - loss: 1.0459 - accuracy: 0.6496 - val_loss: 2.1495 - val_accuracy: 0.4464\n",
      "Epoch 72/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0550 - accuracy: 0.6595\n",
      "Epoch 00072: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 389ms/step - loss: 1.0550 - accuracy: 0.6595 - val_loss: 1.9704 - val_accuracy: 0.4440\n",
      "Epoch 73/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0308 - accuracy: 0.6710\n",
      "Epoch 00073: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.0308 - accuracy: 0.6710 - val_loss: 2.1348 - val_accuracy: 0.4310\n",
      "Epoch 74/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.6603\n",
      "Epoch 00074: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.0345 - accuracy: 0.6603 - val_loss: 2.2766 - val_accuracy: 0.4310\n",
      "Epoch 75/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0326 - accuracy: 0.6671\n",
      "Epoch 00075: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.0326 - accuracy: 0.6671 - val_loss: 2.3433 - val_accuracy: 0.4190\n",
      "Epoch 76/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0195 - accuracy: 0.6647\n",
      "Epoch 00076: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 378ms/step - loss: 1.0195 - accuracy: 0.6647 - val_loss: 2.2044 - val_accuracy: 0.4345\n",
      "Epoch 77/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0341 - accuracy: 0.6627\n",
      "Epoch 00077: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 380ms/step - loss: 1.0341 - accuracy: 0.6627 - val_loss: 2.4296 - val_accuracy: 0.3917\n",
      "Epoch 78/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0084 - accuracy: 0.6750\n",
      "Epoch 00078: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 403ms/step - loss: 1.0084 - accuracy: 0.6750 - val_loss: 2.1643 - val_accuracy: 0.4560\n",
      "Epoch 79/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0227 - accuracy: 0.6702\n",
      "Epoch 00079: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 11s 403ms/step - loss: 1.0227 - accuracy: 0.6702 - val_loss: 2.1560 - val_accuracy: 0.4357\n",
      "Epoch 80/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0039 - accuracy: 0.6611\n",
      "Epoch 00080: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.0039 - accuracy: 0.6611 - val_loss: 2.6735 - val_accuracy: 0.3893\n",
      "Epoch 81/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.9845 - accuracy: 0.6754\n",
      "Epoch 00081: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 377ms/step - loss: 0.9845 - accuracy: 0.6754 - val_loss: 3.1388 - val_accuracy: 0.3679\n",
      "Epoch 82/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.9898 - accuracy: 0.6734\n",
      "Epoch 00082: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 378ms/step - loss: 0.9898 - accuracy: 0.6734 - val_loss: 3.2445 - val_accuracy: 0.3690\n",
      "Epoch 83/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0217 - accuracy: 0.6623\n",
      "Epoch 00083: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 378ms/step - loss: 1.0217 - accuracy: 0.6623 - val_loss: 2.4300 - val_accuracy: 0.4381\n",
      "Epoch 84/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.0040 - accuracy: 0.6683\n",
      "Epoch 00084: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 375ms/step - loss: 1.0040 - accuracy: 0.6683 - val_loss: 2.2563 - val_accuracy: 0.4381\n",
      "Epoch 85/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.9846 - accuracy: 0.6722\n",
      "Epoch 00085: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 378ms/step - loss: 0.9846 - accuracy: 0.6722 - val_loss: 2.4017 - val_accuracy: 0.4214\n",
      "Epoch 86/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.9840 - accuracy: 0.6825\n",
      "Epoch 00086: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 378ms/step - loss: 0.9840 - accuracy: 0.6825 - val_loss: 2.7240 - val_accuracy: 0.3976\n",
      "Epoch 87/300\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.9837 - accuracy: 0.6710\n",
      "Epoch 00087: val_loss did not improve from 1.87253\n",
      "26/26 [==============================] - 10s 376ms/step - loss: 0.9837 - accuracy: 0.6710 - val_loss: 2.2084 - val_accuracy: 0.4583\n",
      "Training completed in time:  0:14:46.997709\n"
     ]
    }
   ],
   "source": [
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=40, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights7.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "\n",
    "model_elu.fit(X_train2, y_train,\n",
    "          batch_size = 100, \n",
    "          epochs = 300,\n",
    "          validation_data = (X_val2, y_val), \n",
    "          callbacks=[checkpointer, es])\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training-Validation Accuracy')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#saving the model history\n",
    "loss = pd.DataFrame(model_elu.history.history)\n",
    "\n",
    "\n",
    "#plotting the loss and accuracy \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(loss[\"loss\"], label =\"Loss\")\n",
    "plt.plot(loss[\"val_loss\"], label = \"Validation_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(loss['accuracy'],label = \"Training Accuracy\")\n",
    "plt.plot(loss['val_accuracy'], label =\"Validation_ Accuracy \")\n",
    "plt.legend()\n",
    "plt.title(\"Training-Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_42 (Conv2D)          (None, 39, 173, 16)       80        \n",
      "                                                                 \n",
      " max_pooling2d_39 (MaxPoolin  (None, 19, 86, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 19, 86, 16)        0         \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 18, 85, 32)        2080      \n",
      "                                                                 \n",
      " max_pooling2d_40 (MaxPoolin  (None, 9, 42, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 9, 42, 32)         0         \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 8, 41, 64)         8256      \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPoolin  (None, 4, 20, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 4, 20, 64)         0         \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 3, 19, 128)        32896     \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPoolin  (None, 1, 9, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 1, 9, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_8   (None, 128)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 21)                2709      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,021\n",
      "Trainable params: 46,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=input_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(21, activation='softmax')) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 3.0042 - accuracy: 0.0607\n",
      "Epoch 00001: val_loss improved from inf to 2.97581, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 7s 360ms/step - loss: 3.0042 - accuracy: 0.0607 - val_loss: 2.9758 - val_accuracy: 0.0738\n",
      "Epoch 2/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.9012 - accuracy: 0.0845\n",
      "Epoch 00002: val_loss improved from 2.97581 to 2.86731, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 332ms/step - loss: 2.9012 - accuracy: 0.0845 - val_loss: 2.8673 - val_accuracy: 0.1119\n",
      "Epoch 3/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.7551 - accuracy: 0.1365\n",
      "Epoch 00003: val_loss improved from 2.86731 to 2.69363, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 335ms/step - loss: 2.7551 - accuracy: 0.1365 - val_loss: 2.6936 - val_accuracy: 0.1905\n",
      "Epoch 4/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.5604 - accuracy: 0.1782\n",
      "Epoch 00004: val_loss improved from 2.69363 to 2.52429, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 337ms/step - loss: 2.5604 - accuracy: 0.1782 - val_loss: 2.5243 - val_accuracy: 0.2155\n",
      "Epoch 5/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.4403 - accuracy: 0.1897\n",
      "Epoch 00005: val_loss improved from 2.52429 to 2.45308, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 338ms/step - loss: 2.4403 - accuracy: 0.1897 - val_loss: 2.4531 - val_accuracy: 0.1929\n",
      "Epoch 6/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.3927 - accuracy: 0.2242\n",
      "Epoch 00006: val_loss improved from 2.45308 to 2.43098, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 341ms/step - loss: 2.3927 - accuracy: 0.2242 - val_loss: 2.4310 - val_accuracy: 0.1940\n",
      "Epoch 7/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.3294 - accuracy: 0.2409\n",
      "Epoch 00007: val_loss improved from 2.43098 to 2.37997, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 343ms/step - loss: 2.3294 - accuracy: 0.2409 - val_loss: 2.3800 - val_accuracy: 0.2262\n",
      "Epoch 8/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.2921 - accuracy: 0.2663\n",
      "Epoch 00008: val_loss improved from 2.37997 to 2.34155, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 343ms/step - loss: 2.2921 - accuracy: 0.2663 - val_loss: 2.3416 - val_accuracy: 0.2595\n",
      "Epoch 9/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.2721 - accuracy: 0.2687\n",
      "Epoch 00009: val_loss improved from 2.34155 to 2.30242, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 355ms/step - loss: 2.2721 - accuracy: 0.2687 - val_loss: 2.3024 - val_accuracy: 0.2810\n",
      "Epoch 10/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.2463 - accuracy: 0.2591\n",
      "Epoch 00010: val_loss improved from 2.30242 to 2.29901, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 345ms/step - loss: 2.2463 - accuracy: 0.2591 - val_loss: 2.2990 - val_accuracy: 0.2726\n",
      "Epoch 11/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.2127 - accuracy: 0.2821\n",
      "Epoch 00011: val_loss improved from 2.29901 to 2.26288, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 349ms/step - loss: 2.2127 - accuracy: 0.2821 - val_loss: 2.2629 - val_accuracy: 0.2881\n",
      "Epoch 12/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.1863 - accuracy: 0.2901\n",
      "Epoch 00012: val_loss did not improve from 2.26288\n",
      "17/17 [==============================] - 6s 342ms/step - loss: 2.1863 - accuracy: 0.2901 - val_loss: 2.2683 - val_accuracy: 0.2655\n",
      "Epoch 13/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.1700 - accuracy: 0.3004\n",
      "Epoch 00013: val_loss improved from 2.26288 to 2.24371, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 343ms/step - loss: 2.1700 - accuracy: 0.3004 - val_loss: 2.2437 - val_accuracy: 0.2905\n",
      "Epoch 14/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.1328 - accuracy: 0.3067\n",
      "Epoch 00014: val_loss improved from 2.24371 to 2.20600, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 344ms/step - loss: 2.1328 - accuracy: 0.3067 - val_loss: 2.2060 - val_accuracy: 0.3024\n",
      "Epoch 15/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.1240 - accuracy: 0.3079\n",
      "Epoch 00015: val_loss improved from 2.20600 to 2.20590, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 344ms/step - loss: 2.1240 - accuracy: 0.3079 - val_loss: 2.2059 - val_accuracy: 0.2893\n",
      "Epoch 16/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.0903 - accuracy: 0.3317\n",
      "Epoch 00016: val_loss improved from 2.20590 to 2.17179, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 345ms/step - loss: 2.0903 - accuracy: 0.3317 - val_loss: 2.1718 - val_accuracy: 0.3095\n",
      "Epoch 17/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.0641 - accuracy: 0.3286\n",
      "Epoch 00017: val_loss improved from 2.17179 to 2.14800, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 8s 445ms/step - loss: 2.0641 - accuracy: 0.3286 - val_loss: 2.1480 - val_accuracy: 0.3286\n",
      "Epoch 18/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.0539 - accuracy: 0.3373\n",
      "Epoch 00018: val_loss did not improve from 2.14800\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 2.0539 - accuracy: 0.3373 - val_loss: 2.1525 - val_accuracy: 0.3405\n",
      "Epoch 19/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.0334 - accuracy: 0.3512\n",
      "Epoch 00019: val_loss improved from 2.14800 to 2.13538, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 346ms/step - loss: 2.0334 - accuracy: 0.3512 - val_loss: 2.1354 - val_accuracy: 0.3083\n",
      "Epoch 20/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.0031 - accuracy: 0.3528\n",
      "Epoch 00020: val_loss improved from 2.13538 to 2.11908, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 348ms/step - loss: 2.0031 - accuracy: 0.3528 - val_loss: 2.1191 - val_accuracy: 0.3298\n",
      "Epoch 21/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9936 - accuracy: 0.3472\n",
      "Epoch 00021: val_loss improved from 2.11908 to 2.09934, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 346ms/step - loss: 1.9936 - accuracy: 0.3472 - val_loss: 2.0993 - val_accuracy: 0.3464\n",
      "Epoch 22/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9708 - accuracy: 0.3651\n",
      "Epoch 00022: val_loss improved from 2.09934 to 2.09594, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 350ms/step - loss: 1.9708 - accuracy: 0.3651 - val_loss: 2.0959 - val_accuracy: 0.3560\n",
      "Epoch 23/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9640 - accuracy: 0.3667\n",
      "Epoch 00023: val_loss improved from 2.09594 to 2.08472, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 366ms/step - loss: 1.9640 - accuracy: 0.3667 - val_loss: 2.0847 - val_accuracy: 0.3452\n",
      "Epoch 24/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9578 - accuracy: 0.3738\n",
      "Epoch 00024: val_loss improved from 2.08472 to 2.07470, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 365ms/step - loss: 1.9578 - accuracy: 0.3738 - val_loss: 2.0747 - val_accuracy: 0.3500\n",
      "Epoch 25/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9455 - accuracy: 0.3655\n",
      "Epoch 00025: val_loss did not improve from 2.07470\n",
      "17/17 [==============================] - 6s 355ms/step - loss: 1.9455 - accuracy: 0.3655 - val_loss: 2.0821 - val_accuracy: 0.3500\n",
      "Epoch 26/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9158 - accuracy: 0.3750\n",
      "Epoch 00026: val_loss improved from 2.07470 to 2.05805, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 360ms/step - loss: 1.9158 - accuracy: 0.3750 - val_loss: 2.0581 - val_accuracy: 0.3548\n",
      "Epoch 27/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 1.9201 - accuracy: 0.3810\n",
      "Epoch 00027: val_loss improved from 2.05805 to 2.05781, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 362ms/step - loss: 1.9201 - accuracy: 0.3810 - val_loss: 2.0578 - val_accuracy: 0.3440\n",
      "Epoch 28/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8923 - accuracy: 0.3960\n",
      "Epoch 00028: val_loss improved from 2.05781 to 2.05298, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 355ms/step - loss: 1.8923 - accuracy: 0.3960 - val_loss: 2.0530 - val_accuracy: 0.3571\n",
      "Epoch 29/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8803 - accuracy: 0.3960\n",
      "Epoch 00029: val_loss improved from 2.05298 to 2.03834, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 357ms/step - loss: 1.8803 - accuracy: 0.3960 - val_loss: 2.0383 - val_accuracy: 0.3500\n",
      "Epoch 30/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8788 - accuracy: 0.3948\n",
      "Epoch 00030: val_loss improved from 2.03834 to 2.02273, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 350ms/step - loss: 1.8788 - accuracy: 0.3948 - val_loss: 2.0227 - val_accuracy: 0.3869\n",
      "Epoch 31/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8612 - accuracy: 0.4048\n",
      "Epoch 00031: val_loss did not improve from 2.02273\n",
      "17/17 [==============================] - 6s 354ms/step - loss: 1.8612 - accuracy: 0.4048 - val_loss: 2.0300 - val_accuracy: 0.3631\n",
      "Epoch 32/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8561 - accuracy: 0.3956\n",
      "Epoch 00032: val_loss improved from 2.02273 to 2.01060, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 350ms/step - loss: 1.8561 - accuracy: 0.3956 - val_loss: 2.0106 - val_accuracy: 0.3845\n",
      "Epoch 33/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8428 - accuracy: 0.4099\n",
      "Epoch 00033: val_loss did not improve from 2.01060\n",
      "17/17 [==============================] - 6s 349ms/step - loss: 1.8428 - accuracy: 0.4099 - val_loss: 2.0328 - val_accuracy: 0.3464\n",
      "Epoch 34/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8304 - accuracy: 0.4107\n",
      "Epoch 00034: val_loss improved from 2.01060 to 2.01051, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 348ms/step - loss: 1.8304 - accuracy: 0.4107 - val_loss: 2.0105 - val_accuracy: 0.3774\n",
      "Epoch 35/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8192 - accuracy: 0.4190\n",
      "Epoch 00035: val_loss improved from 2.01051 to 1.98895, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 349ms/step - loss: 1.8192 - accuracy: 0.4190 - val_loss: 1.9889 - val_accuracy: 0.3833\n",
      "Epoch 36/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.8005 - accuracy: 0.4242\n",
      "Epoch 00036: val_loss did not improve from 1.98895\n",
      "17/17 [==============================] - 6s 351ms/step - loss: 1.8005 - accuracy: 0.4242 - val_loss: 1.9955 - val_accuracy: 0.3917\n",
      "Epoch 37/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7985 - accuracy: 0.4270\n",
      "Epoch 00037: val_loss improved from 1.98895 to 1.97978, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 357ms/step - loss: 1.7985 - accuracy: 0.4270 - val_loss: 1.9798 - val_accuracy: 0.3810\n",
      "Epoch 38/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7722 - accuracy: 0.4329\n",
      "Epoch 00038: val_loss did not improve from 1.97978\n",
      "17/17 [==============================] - 6s 347ms/step - loss: 1.7722 - accuracy: 0.4329 - val_loss: 1.9801 - val_accuracy: 0.3964\n",
      "Epoch 39/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7867 - accuracy: 0.4250\n",
      "Epoch 00039: val_loss did not improve from 1.97978\n",
      "17/17 [==============================] - 6s 355ms/step - loss: 1.7867 - accuracy: 0.4250 - val_loss: 1.9992 - val_accuracy: 0.3643\n",
      "Epoch 40/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7753 - accuracy: 0.4325\n",
      "Epoch 00040: val_loss improved from 1.97978 to 1.95774, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 355ms/step - loss: 1.7753 - accuracy: 0.4325 - val_loss: 1.9577 - val_accuracy: 0.3798\n",
      "Epoch 41/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7493 - accuracy: 0.4417\n",
      "Epoch 00041: val_loss improved from 1.95774 to 1.94825, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 358ms/step - loss: 1.7493 - accuracy: 0.4417 - val_loss: 1.9483 - val_accuracy: 0.4000\n",
      "Epoch 42/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7367 - accuracy: 0.4599\n",
      "Epoch 00042: val_loss did not improve from 1.94825\n",
      "17/17 [==============================] - 6s 362ms/step - loss: 1.7367 - accuracy: 0.4599 - val_loss: 1.9553 - val_accuracy: 0.3952\n",
      "Epoch 43/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7374 - accuracy: 0.4421\n",
      "Epoch 00043: val_loss did not improve from 1.94825\n",
      "17/17 [==============================] - 6s 356ms/step - loss: 1.7374 - accuracy: 0.4421 - val_loss: 1.9526 - val_accuracy: 0.3988\n",
      "Epoch 44/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7188 - accuracy: 0.4476\n",
      "Epoch 00044: val_loss improved from 1.94825 to 1.94244, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 354ms/step - loss: 1.7188 - accuracy: 0.4476 - val_loss: 1.9424 - val_accuracy: 0.3952\n",
      "Epoch 45/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.7189 - accuracy: 0.4563\n",
      "Epoch 00045: val_loss improved from 1.94244 to 1.92731, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 359ms/step - loss: 1.7189 - accuracy: 0.4563 - val_loss: 1.9273 - val_accuracy: 0.4119\n",
      "Epoch 46/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6987 - accuracy: 0.4583\n",
      "Epoch 00046: val_loss did not improve from 1.92731\n",
      "17/17 [==============================] - 6s 360ms/step - loss: 1.6987 - accuracy: 0.4583 - val_loss: 1.9409 - val_accuracy: 0.3952\n",
      "Epoch 47/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6855 - accuracy: 0.4591\n",
      "Epoch 00047: val_loss did not improve from 1.92731\n",
      "17/17 [==============================] - 6s 360ms/step - loss: 1.6855 - accuracy: 0.4591 - val_loss: 1.9452 - val_accuracy: 0.3869\n",
      "Epoch 48/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6909 - accuracy: 0.4611\n",
      "Epoch 00048: val_loss did not improve from 1.92731\n",
      "17/17 [==============================] - 6s 351ms/step - loss: 1.6909 - accuracy: 0.4611 - val_loss: 1.9343 - val_accuracy: 0.4036\n",
      "Epoch 49/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6858 - accuracy: 0.4575\n",
      "Epoch 00049: val_loss did not improve from 1.92731\n",
      "17/17 [==============================] - 7s 433ms/step - loss: 1.6858 - accuracy: 0.4575 - val_loss: 1.9346 - val_accuracy: 0.4000\n",
      "Epoch 50/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6709 - accuracy: 0.4599\n",
      "Epoch 00050: val_loss improved from 1.92731 to 1.92648, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 9s 549ms/step - loss: 1.6709 - accuracy: 0.4599 - val_loss: 1.9265 - val_accuracy: 0.4000\n",
      "Epoch 51/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6590 - accuracy: 0.4714\n",
      "Epoch 00051: val_loss did not improve from 1.92648\n",
      "17/17 [==============================] - 8s 468ms/step - loss: 1.6590 - accuracy: 0.4714 - val_loss: 1.9325 - val_accuracy: 0.4083\n",
      "Epoch 52/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6595 - accuracy: 0.4623\n",
      "Epoch 00052: val_loss improved from 1.92648 to 1.90443, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 363ms/step - loss: 1.6595 - accuracy: 0.4623 - val_loss: 1.9044 - val_accuracy: 0.4179\n",
      "Epoch 53/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6549 - accuracy: 0.4635\n",
      "Epoch 00053: val_loss did not improve from 1.90443\n",
      "17/17 [==============================] - 6s 346ms/step - loss: 1.6549 - accuracy: 0.4635 - val_loss: 1.9395 - val_accuracy: 0.3929\n",
      "Epoch 54/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6456 - accuracy: 0.4710\n",
      "Epoch 00054: val_loss did not improve from 1.90443\n",
      "17/17 [==============================] - 6s 344ms/step - loss: 1.6456 - accuracy: 0.4710 - val_loss: 1.9263 - val_accuracy: 0.4048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6218 - accuracy: 0.4849\n",
      "Epoch 00055: val_loss did not improve from 1.90443\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 1.6218 - accuracy: 0.4849 - val_loss: 1.9096 - val_accuracy: 0.4048\n",
      "Epoch 56/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6207 - accuracy: 0.4790\n",
      "Epoch 00056: val_loss improved from 1.90443 to 1.89592, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 354ms/step - loss: 1.6207 - accuracy: 0.4790 - val_loss: 1.8959 - val_accuracy: 0.4060\n",
      "Epoch 57/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.6006 - accuracy: 0.4790\n",
      "Epoch 00057: val_loss improved from 1.89592 to 1.88487, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 357ms/step - loss: 1.6006 - accuracy: 0.4790 - val_loss: 1.8849 - val_accuracy: 0.4143\n",
      "Epoch 58/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5926 - accuracy: 0.4913\n",
      "Epoch 00058: val_loss did not improve from 1.88487\n",
      "17/17 [==============================] - 6s 358ms/step - loss: 1.5926 - accuracy: 0.4913 - val_loss: 1.9240 - val_accuracy: 0.4095\n",
      "Epoch 59/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5926 - accuracy: 0.4984\n",
      "Epoch 00059: val_loss did not improve from 1.88487\n",
      "17/17 [==============================] - 6s 383ms/step - loss: 1.5926 - accuracy: 0.4984 - val_loss: 1.9142 - val_accuracy: 0.4060\n",
      "Epoch 60/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5837 - accuracy: 0.4861\n",
      "Epoch 00060: val_loss did not improve from 1.88487\n",
      "17/17 [==============================] - 6s 364ms/step - loss: 1.5837 - accuracy: 0.4861 - val_loss: 1.9057 - val_accuracy: 0.4095\n",
      "Epoch 61/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5825 - accuracy: 0.4897\n",
      "Epoch 00061: val_loss improved from 1.88487 to 1.88342, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 368ms/step - loss: 1.5825 - accuracy: 0.4897 - val_loss: 1.8834 - val_accuracy: 0.4143\n",
      "Epoch 62/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5923 - accuracy: 0.4988\n",
      "Epoch 00062: val_loss did not improve from 1.88342\n",
      "17/17 [==============================] - 6s 365ms/step - loss: 1.5923 - accuracy: 0.4988 - val_loss: 1.9020 - val_accuracy: 0.4071\n",
      "Epoch 63/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5549 - accuracy: 0.5075\n",
      "Epoch 00063: val_loss improved from 1.88342 to 1.88231, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 1.5549 - accuracy: 0.5075 - val_loss: 1.8823 - val_accuracy: 0.4298\n",
      "Epoch 64/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5413 - accuracy: 0.5008\n",
      "Epoch 00064: val_loss did not improve from 1.88231\n",
      "17/17 [==============================] - 6s 361ms/step - loss: 1.5413 - accuracy: 0.5008 - val_loss: 1.8992 - val_accuracy: 0.4202\n",
      "Epoch 65/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5487 - accuracy: 0.5056\n",
      "Epoch 00065: val_loss did not improve from 1.88231\n",
      "17/17 [==============================] - 6s 358ms/step - loss: 1.5487 - accuracy: 0.5056 - val_loss: 1.8990 - val_accuracy: 0.4250\n",
      "Epoch 66/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5256 - accuracy: 0.5131\n",
      "Epoch 00066: val_loss improved from 1.88231 to 1.87973, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 358ms/step - loss: 1.5256 - accuracy: 0.5131 - val_loss: 1.8797 - val_accuracy: 0.4274\n",
      "Epoch 67/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5402 - accuracy: 0.5028\n",
      "Epoch 00067: val_loss did not improve from 1.87973\n",
      "17/17 [==============================] - 6s 363ms/step - loss: 1.5402 - accuracy: 0.5028 - val_loss: 1.8852 - val_accuracy: 0.4250\n",
      "Epoch 68/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5198 - accuracy: 0.5155\n",
      "Epoch 00068: val_loss improved from 1.87973 to 1.87123, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 362ms/step - loss: 1.5198 - accuracy: 0.5155 - val_loss: 1.8712 - val_accuracy: 0.4429\n",
      "Epoch 69/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5279 - accuracy: 0.5103\n",
      "Epoch 00069: val_loss improved from 1.87123 to 1.85436, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 365ms/step - loss: 1.5279 - accuracy: 0.5103 - val_loss: 1.8544 - val_accuracy: 0.4369\n",
      "Epoch 70/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5155 - accuracy: 0.5107\n",
      "Epoch 00070: val_loss did not improve from 1.85436\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 1.5155 - accuracy: 0.5107 - val_loss: 1.8792 - val_accuracy: 0.4357\n",
      "Epoch 71/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5057 - accuracy: 0.5052\n",
      "Epoch 00071: val_loss did not improve from 1.85436\n",
      "17/17 [==============================] - 6s 360ms/step - loss: 1.5057 - accuracy: 0.5052 - val_loss: 1.8960 - val_accuracy: 0.4274\n",
      "Epoch 72/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4994 - accuracy: 0.5155\n",
      "Epoch 00072: val_loss did not improve from 1.85436\n",
      "17/17 [==============================] - 6s 361ms/step - loss: 1.4994 - accuracy: 0.5155 - val_loss: 1.8874 - val_accuracy: 0.4440\n",
      "Epoch 73/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4939 - accuracy: 0.5175\n",
      "Epoch 00073: val_loss did not improve from 1.85436\n",
      "17/17 [==============================] - 6s 355ms/step - loss: 1.4939 - accuracy: 0.5175 - val_loss: 1.8641 - val_accuracy: 0.4440\n",
      "Epoch 74/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4843 - accuracy: 0.5163\n",
      "Epoch 00074: val_loss did not improve from 1.85436\n",
      "17/17 [==============================] - 6s 364ms/step - loss: 1.4843 - accuracy: 0.5163 - val_loss: 1.8750 - val_accuracy: 0.4357\n",
      "Epoch 75/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4707 - accuracy: 0.5206\n",
      "Epoch 00075: val_loss did not improve from 1.85436\n",
      "17/17 [==============================] - 7s 397ms/step - loss: 1.4707 - accuracy: 0.5206 - val_loss: 1.8745 - val_accuracy: 0.4298\n",
      "Epoch 76/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4798 - accuracy: 0.5206\n",
      "Epoch 00076: val_loss did not improve from 1.85436\n",
      "17/17 [==============================] - 6s 360ms/step - loss: 1.4798 - accuracy: 0.5206 - val_loss: 1.8753 - val_accuracy: 0.4226\n",
      "Epoch 77/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4462 - accuracy: 0.5429\n",
      "Epoch 00077: val_loss improved from 1.85436 to 1.84676, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 359ms/step - loss: 1.4462 - accuracy: 0.5429 - val_loss: 1.8468 - val_accuracy: 0.4393\n",
      "Epoch 78/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4468 - accuracy: 0.5298\n",
      "Epoch 00078: val_loss did not improve from 1.84676\n",
      "17/17 [==============================] - 6s 346ms/step - loss: 1.4468 - accuracy: 0.5298 - val_loss: 1.8833 - val_accuracy: 0.4226\n",
      "Epoch 79/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4450 - accuracy: 0.5389\n",
      "Epoch 00079: val_loss did not improve from 1.84676\n",
      "17/17 [==============================] - 6s 343ms/step - loss: 1.4450 - accuracy: 0.5389 - val_loss: 1.8632 - val_accuracy: 0.4357\n",
      "Epoch 80/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4385 - accuracy: 0.5381\n",
      "Epoch 00080: val_loss did not improve from 1.84676\n",
      "17/17 [==============================] - 6s 346ms/step - loss: 1.4385 - accuracy: 0.5381 - val_loss: 1.8642 - val_accuracy: 0.4333\n",
      "Epoch 81/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4384 - accuracy: 0.5429\n",
      "Epoch 00081: val_loss did not improve from 1.84676\n",
      "17/17 [==============================] - 6s 344ms/step - loss: 1.4384 - accuracy: 0.5429 - val_loss: 1.8480 - val_accuracy: 0.4393\n",
      "Epoch 82/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4227 - accuracy: 0.5429\n",
      "Epoch 00082: val_loss did not improve from 1.84676\n",
      "17/17 [==============================] - 6s 348ms/step - loss: 1.4227 - accuracy: 0.5429 - val_loss: 1.8531 - val_accuracy: 0.4560\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 1.4146 - accuracy: 0.5361\n",
      "Epoch 00083: val_loss did not improve from 1.84676\n",
      "17/17 [==============================] - 6s 352ms/step - loss: 1.4146 - accuracy: 0.5361 - val_loss: 1.8719 - val_accuracy: 0.4464\n",
      "Epoch 84/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4121 - accuracy: 0.5421\n",
      "Epoch 00084: val_loss improved from 1.84676 to 1.84091, saving model to weights8.best.hdf5\n",
      "17/17 [==============================] - 6s 360ms/step - loss: 1.4121 - accuracy: 0.5421 - val_loss: 1.8409 - val_accuracy: 0.4583\n",
      "Epoch 85/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4075 - accuracy: 0.5484\n",
      "Epoch 00085: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 354ms/step - loss: 1.4075 - accuracy: 0.5484 - val_loss: 1.8577 - val_accuracy: 0.4548\n",
      "Epoch 86/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4147 - accuracy: 0.5365\n",
      "Epoch 00086: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 356ms/step - loss: 1.4147 - accuracy: 0.5365 - val_loss: 1.8817 - val_accuracy: 0.4548\n",
      "Epoch 87/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3896 - accuracy: 0.5456\n",
      "Epoch 00087: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 363ms/step - loss: 1.3896 - accuracy: 0.5456 - val_loss: 1.8650 - val_accuracy: 0.4536\n",
      "Epoch 88/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3945 - accuracy: 0.5500\n",
      "Epoch 00088: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 355ms/step - loss: 1.3945 - accuracy: 0.5500 - val_loss: 1.8822 - val_accuracy: 0.4405\n",
      "Epoch 89/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3880 - accuracy: 0.5528\n",
      "Epoch 00089: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 359ms/step - loss: 1.3880 - accuracy: 0.5528 - val_loss: 1.8759 - val_accuracy: 0.4583\n",
      "Epoch 90/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3816 - accuracy: 0.5583\n",
      "Epoch 00090: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 359ms/step - loss: 1.3816 - accuracy: 0.5583 - val_loss: 1.8619 - val_accuracy: 0.4393\n",
      "Epoch 91/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3760 - accuracy: 0.5536\n",
      "Epoch 00091: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 357ms/step - loss: 1.3760 - accuracy: 0.5536 - val_loss: 1.8597 - val_accuracy: 0.4524\n",
      "Epoch 92/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3759 - accuracy: 0.5563\n",
      "Epoch 00092: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 1.3759 - accuracy: 0.5563 - val_loss: 1.8806 - val_accuracy: 0.4548\n",
      "Epoch 93/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3597 - accuracy: 0.5575\n",
      "Epoch 00093: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 359ms/step - loss: 1.3597 - accuracy: 0.5575 - val_loss: 1.8763 - val_accuracy: 0.4393\n",
      "Epoch 94/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3434 - accuracy: 0.5651\n",
      "Epoch 00094: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 360ms/step - loss: 1.3434 - accuracy: 0.5651 - val_loss: 1.8514 - val_accuracy: 0.4655\n",
      "Epoch 95/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3385 - accuracy: 0.5631\n",
      "Epoch 00095: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 360ms/step - loss: 1.3385 - accuracy: 0.5631 - val_loss: 1.8584 - val_accuracy: 0.4524\n",
      "Epoch 96/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3338 - accuracy: 0.5647\n",
      "Epoch 00096: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 361ms/step - loss: 1.3338 - accuracy: 0.5647 - val_loss: 1.8665 - val_accuracy: 0.4619\n",
      "Epoch 97/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3108 - accuracy: 0.5802\n",
      "Epoch 00097: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 362ms/step - loss: 1.3108 - accuracy: 0.5802 - val_loss: 1.8611 - val_accuracy: 0.4560\n",
      "Epoch 98/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3282 - accuracy: 0.5730\n",
      "Epoch 00098: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 1.3282 - accuracy: 0.5730 - val_loss: 1.8569 - val_accuracy: 0.4643\n",
      "Epoch 99/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3268 - accuracy: 0.5786\n",
      "Epoch 00099: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 359ms/step - loss: 1.3268 - accuracy: 0.5786 - val_loss: 1.8639 - val_accuracy: 0.4702\n",
      "Epoch 100/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3229 - accuracy: 0.5671\n",
      "Epoch 00100: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 363ms/step - loss: 1.3229 - accuracy: 0.5671 - val_loss: 1.8524 - val_accuracy: 0.4655\n",
      "Epoch 101/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3187 - accuracy: 0.5813\n",
      "Epoch 00101: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 1.3187 - accuracy: 0.5813 - val_loss: 1.8605 - val_accuracy: 0.4655\n",
      "Epoch 102/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3251 - accuracy: 0.5659\n",
      "Epoch 00102: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 359ms/step - loss: 1.3251 - accuracy: 0.5659 - val_loss: 1.8493 - val_accuracy: 0.4643\n",
      "Epoch 103/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3074 - accuracy: 0.5718\n",
      "Epoch 00103: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 362ms/step - loss: 1.3074 - accuracy: 0.5718 - val_loss: 1.8652 - val_accuracy: 0.4536\n",
      "Epoch 104/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3027 - accuracy: 0.5833\n",
      "Epoch 00104: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 355ms/step - loss: 1.3027 - accuracy: 0.5833 - val_loss: 1.8434 - val_accuracy: 0.4595\n",
      "Epoch 105/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3110 - accuracy: 0.5782\n",
      "Epoch 00105: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 360ms/step - loss: 1.3110 - accuracy: 0.5782 - val_loss: 1.8475 - val_accuracy: 0.4595\n",
      "Epoch 106/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3014 - accuracy: 0.5734\n",
      "Epoch 00106: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 358ms/step - loss: 1.3014 - accuracy: 0.5734 - val_loss: 1.8533 - val_accuracy: 0.4714\n",
      "Epoch 107/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2883 - accuracy: 0.5786\n",
      "Epoch 00107: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 363ms/step - loss: 1.2883 - accuracy: 0.5786 - val_loss: 1.8518 - val_accuracy: 0.4738\n",
      "Epoch 108/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2726 - accuracy: 0.5921\n",
      "Epoch 00108: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 352ms/step - loss: 1.2726 - accuracy: 0.5921 - val_loss: 1.8487 - val_accuracy: 0.4702\n",
      "Epoch 109/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2899 - accuracy: 0.5829\n",
      "Epoch 00109: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 372ms/step - loss: 1.2899 - accuracy: 0.5829 - val_loss: 1.8503 - val_accuracy: 0.4750\n",
      "Epoch 110/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2770 - accuracy: 0.5925\n",
      "Epoch 00110: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 7s 403ms/step - loss: 1.2770 - accuracy: 0.5925 - val_loss: 1.8470 - val_accuracy: 0.4702\n",
      "Epoch 111/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2759 - accuracy: 0.5873\n",
      "Epoch 00111: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 343ms/step - loss: 1.2759 - accuracy: 0.5873 - val_loss: 1.8455 - val_accuracy: 0.4631\n",
      "Epoch 112/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 1.2714 - accuracy: 0.5730\n",
      "Epoch 00112: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 353ms/step - loss: 1.2714 - accuracy: 0.5730 - val_loss: 1.8525 - val_accuracy: 0.4679\n",
      "Epoch 113/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2520 - accuracy: 0.5968\n",
      "Epoch 00113: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 356ms/step - loss: 1.2520 - accuracy: 0.5968 - val_loss: 1.8544 - val_accuracy: 0.4750\n",
      "Epoch 114/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2471 - accuracy: 0.5889\n",
      "Epoch 00114: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 366ms/step - loss: 1.2471 - accuracy: 0.5889 - val_loss: 1.8522 - val_accuracy: 0.4786\n",
      "Epoch 115/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2409 - accuracy: 0.5925\n",
      "Epoch 00115: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 7s 417ms/step - loss: 1.2409 - accuracy: 0.5925 - val_loss: 1.8798 - val_accuracy: 0.4667\n",
      "Epoch 116/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2502 - accuracy: 0.6040\n",
      "Epoch 00116: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 362ms/step - loss: 1.2502 - accuracy: 0.6040 - val_loss: 1.8684 - val_accuracy: 0.4679\n",
      "Epoch 117/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2241 - accuracy: 0.5940\n",
      "Epoch 00117: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 338ms/step - loss: 1.2241 - accuracy: 0.5940 - val_loss: 1.8890 - val_accuracy: 0.4738\n",
      "Epoch 118/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2291 - accuracy: 0.5968\n",
      "Epoch 00118: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 341ms/step - loss: 1.2291 - accuracy: 0.5968 - val_loss: 1.9107 - val_accuracy: 0.4738\n",
      "Epoch 119/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2297 - accuracy: 0.5984\n",
      "Epoch 00119: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 338ms/step - loss: 1.2297 - accuracy: 0.5984 - val_loss: 1.8942 - val_accuracy: 0.4738\n",
      "Epoch 120/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2276 - accuracy: 0.6020\n",
      "Epoch 00120: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 338ms/step - loss: 1.2276 - accuracy: 0.6020 - val_loss: 1.9188 - val_accuracy: 0.4679\n",
      "Epoch 121/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2420 - accuracy: 0.5929\n",
      "Epoch 00121: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 339ms/step - loss: 1.2420 - accuracy: 0.5929 - val_loss: 1.9125 - val_accuracy: 0.4679\n",
      "Epoch 122/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2564 - accuracy: 0.5813\n",
      "Epoch 00122: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 338ms/step - loss: 1.2564 - accuracy: 0.5813 - val_loss: 1.8554 - val_accuracy: 0.4762\n",
      "Epoch 123/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2047 - accuracy: 0.6107\n",
      "Epoch 00123: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 339ms/step - loss: 1.2047 - accuracy: 0.6107 - val_loss: 1.8994 - val_accuracy: 0.4869\n",
      "Epoch 124/300\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2393 - accuracy: 0.5921\n",
      "Epoch 00124: val_loss did not improve from 1.84091\n",
      "17/17 [==============================] - 6s 342ms/step - loss: 1.2393 - accuracy: 0.5921 - val_loss: 1.8699 - val_accuracy: 0.4869\n",
      "Training completed in time:  0:12:36.345926\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=40, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights8.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "model.fit(X_train2, y_train,\n",
    "          batch_size = 150, \n",
    "          epochs = 300,\n",
    "          validation_data = (X_val2, y_val), \n",
    "          callbacks=[checkpointer, es],verbose=1)\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_66 (Conv2D)          (None, 39, 173, 16)       80        \n",
      "                                                                 \n",
      " max_pooling2d_63 (MaxPoolin  (None, 19, 86, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_69 (Dropout)        (None, 19, 86, 16)        0         \n",
      "                                                                 \n",
      " conv2d_67 (Conv2D)          (None, 18, 85, 32)        2080      \n",
      "                                                                 \n",
      " max_pooling2d_64 (MaxPoolin  (None, 9, 42, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_70 (Dropout)        (None, 9, 42, 32)         0         \n",
      "                                                                 \n",
      " conv2d_68 (Conv2D)          (None, 8, 41, 64)         8256      \n",
      "                                                                 \n",
      " max_pooling2d_65 (MaxPoolin  (None, 4, 20, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 4, 20, 64)         0         \n",
      "                                                                 \n",
      " conv2d_69 (Conv2D)          (None, 3, 19, 128)        32896     \n",
      "                                                                 \n",
      " max_pooling2d_66 (MaxPoolin  (None, 1, 9, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 1, 9, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_14  (None, 128)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 18)                2322      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,634\n",
      "Trainable params: 45,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=input_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(18, activation='softmax')) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.8160 - accuracy: 0.0643\n",
      "Epoch 00001: val_loss improved from inf to 2.74821, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 5s 103ms/step - loss: 2.8165 - accuracy: 0.0640 - val_loss: 2.7482 - val_accuracy: 0.0856\n",
      "Epoch 2/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.5587 - accuracy: 0.1558\n",
      "Epoch 00002: val_loss improved from 2.74821 to 2.43741, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 2.5572 - accuracy: 0.1559 - val_loss: 2.4374 - val_accuracy: 0.1759\n",
      "Epoch 3/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.3216 - accuracy: 0.2047\n",
      "Epoch 00003: val_loss improved from 2.43741 to 2.32249, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 96ms/step - loss: 2.3215 - accuracy: 0.2045 - val_loss: 2.3225 - val_accuracy: 0.1921\n",
      "Epoch 4/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.2702 - accuracy: 0.2295\n",
      "Epoch 00004: val_loss improved from 2.32249 to 2.24548, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 93ms/step - loss: 2.2687 - accuracy: 0.2299 - val_loss: 2.2455 - val_accuracy: 0.2407\n",
      "Epoch 5/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.1810 - accuracy: 0.2426\n",
      "Epoch 00005: val_loss improved from 2.24548 to 2.20778, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 2.1788 - accuracy: 0.2431 - val_loss: 2.2078 - val_accuracy: 0.2384\n",
      "Epoch 6/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.1453 - accuracy: 0.2566\n",
      "Epoch 00006: val_loss improved from 2.20778 to 2.20506, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 92ms/step - loss: 2.1468 - accuracy: 0.2562 - val_loss: 2.2051 - val_accuracy: 0.2315\n",
      "Epoch 7/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.0941 - accuracy: 0.2915\n",
      "Epoch 00007: val_loss improved from 2.20506 to 2.14199, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 87ms/step - loss: 2.0935 - accuracy: 0.2932 - val_loss: 2.1420 - val_accuracy: 0.2824\n",
      "Epoch 8/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 2.0461 - accuracy: 0.3225\n",
      "Epoch 00008: val_loss did not improve from 2.14199\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 2.0446 - accuracy: 0.3225 - val_loss: 2.1481 - val_accuracy: 0.2384\n",
      "Epoch 9/300\n",
      "44/44 [==============================] - ETA: 0s - loss: 2.0128 - accuracy: 0.3179\n",
      "Epoch 00009: val_loss improved from 2.14199 to 2.10500, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 94ms/step - loss: 2.0128 - accuracy: 0.3179 - val_loss: 2.1050 - val_accuracy: 0.2708\n",
      "Epoch 10/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.9716 - accuracy: 0.3434\n",
      "Epoch 00010: val_loss improved from 2.10500 to 2.04683, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 92ms/step - loss: 1.9704 - accuracy: 0.3434 - val_loss: 2.0468 - val_accuracy: 0.3171\n",
      "Epoch 11/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.9353 - accuracy: 0.3550\n",
      "Epoch 00011: val_loss did not improve from 2.04683\n",
      "44/44 [==============================] - 4s 87ms/step - loss: 1.9338 - accuracy: 0.3549 - val_loss: 2.1062 - val_accuracy: 0.2940\n",
      "Epoch 12/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.9115 - accuracy: 0.3760\n",
      "Epoch 00012: val_loss improved from 2.04683 to 2.00118, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 89ms/step - loss: 1.9117 - accuracy: 0.3758 - val_loss: 2.0012 - val_accuracy: 0.3287\n",
      "Epoch 13/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.8608 - accuracy: 0.3860\n",
      "Epoch 00013: val_loss did not improve from 2.00118\n",
      "44/44 [==============================] - 4s 89ms/step - loss: 1.8580 - accuracy: 0.3873 - val_loss: 2.0101 - val_accuracy: 0.3056\n",
      "Epoch 14/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.8348 - accuracy: 0.3837\n",
      "Epoch 00014: val_loss did not improve from 2.00118\n",
      "44/44 [==============================] - 4s 87ms/step - loss: 1.8346 - accuracy: 0.3827 - val_loss: 2.0018 - val_accuracy: 0.3310\n",
      "Epoch 15/300\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.8006 - accuracy: 0.4066\n",
      "Epoch 00015: val_loss improved from 2.00118 to 1.94776, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 99ms/step - loss: 1.8006 - accuracy: 0.4066 - val_loss: 1.9478 - val_accuracy: 0.3519\n",
      "Epoch 16/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.7948 - accuracy: 0.3953\n",
      "Epoch 00016: val_loss improved from 1.94776 to 1.94459, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 91ms/step - loss: 1.7951 - accuracy: 0.3951 - val_loss: 1.9446 - val_accuracy: 0.3819\n",
      "Epoch 17/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.7672 - accuracy: 0.4062\n",
      "Epoch 00017: val_loss improved from 1.94459 to 1.91978, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 1.7656 - accuracy: 0.4066 - val_loss: 1.9198 - val_accuracy: 0.3958\n",
      "Epoch 18/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.7531 - accuracy: 0.4132\n",
      "Epoch 00018: val_loss did not improve from 1.91978\n",
      "44/44 [==============================] - 4s 88ms/step - loss: 1.7524 - accuracy: 0.4144 - val_loss: 1.9571 - val_accuracy: 0.3542\n",
      "Epoch 19/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.7278 - accuracy: 0.4209\n",
      "Epoch 00019: val_loss improved from 1.91978 to 1.90471, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 1.7290 - accuracy: 0.4190 - val_loss: 1.9047 - val_accuracy: 0.3889\n",
      "Epoch 20/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.7168 - accuracy: 0.4248\n",
      "Epoch 00020: val_loss did not improve from 1.90471\n",
      "44/44 [==============================] - 4s 88ms/step - loss: 1.7182 - accuracy: 0.4244 - val_loss: 1.9494 - val_accuracy: 0.3611\n",
      "Epoch 21/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.6942 - accuracy: 0.4341\n",
      "Epoch 00021: val_loss improved from 1.90471 to 1.89494, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 1.6931 - accuracy: 0.4344 - val_loss: 1.8949 - val_accuracy: 0.3819\n",
      "Epoch 22/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.6437 - accuracy: 0.4488\n",
      "Epoch 00022: val_loss did not improve from 1.89494\n",
      "44/44 [==============================] - 4s 97ms/step - loss: 1.6443 - accuracy: 0.4491 - val_loss: 1.9019 - val_accuracy: 0.3935\n",
      "Epoch 23/300\n",
      "44/44 [==============================] - ETA: 0s - loss: 1.6515 - accuracy: 0.4460\n",
      "Epoch 00023: val_loss improved from 1.89494 to 1.85465, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 5s 125ms/step - loss: 1.6515 - accuracy: 0.4460 - val_loss: 1.8546 - val_accuracy: 0.4005\n",
      "Epoch 24/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.6045 - accuracy: 0.4860\n",
      "Epoch 00024: val_loss did not improve from 1.85465\n",
      "44/44 [==============================] - 4s 98ms/step - loss: 1.6012 - accuracy: 0.4877 - val_loss: 1.8740 - val_accuracy: 0.3912\n",
      "Epoch 25/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.5970 - accuracy: 0.4822\n",
      "Epoch 00025: val_loss improved from 1.85465 to 1.80773, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 94ms/step - loss: 1.5938 - accuracy: 0.4838 - val_loss: 1.8077 - val_accuracy: 0.3981\n",
      "Epoch 26/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.5617 - accuracy: 0.4868\n",
      "Epoch 00026: val_loss did not improve from 1.80773\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 1.5635 - accuracy: 0.4861 - val_loss: 1.8448 - val_accuracy: 0.3796\n",
      "Epoch 27/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.5401 - accuracy: 0.4977\n",
      "Epoch 00027: val_loss did not improve from 1.80773\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 1.5394 - accuracy: 0.4977 - val_loss: 1.8482 - val_accuracy: 0.4074\n",
      "Epoch 28/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 1.5233 - accuracy: 0.4915\n",
      "Epoch 00028: val_loss did not improve from 1.80773\n",
      "44/44 [==============================] - 4s 85ms/step - loss: 1.5238 - accuracy: 0.4915 - val_loss: 1.8466 - val_accuracy: 0.4167\n",
      "Epoch 29/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.5004 - accuracy: 0.4969\n",
      "Epoch 00029: val_loss did not improve from 1.80773\n",
      "44/44 [==============================] - 4s 84ms/step - loss: 1.5005 - accuracy: 0.4961 - val_loss: 1.8105 - val_accuracy: 0.4398\n",
      "Epoch 30/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.4981 - accuracy: 0.5016 ETA\n",
      "Epoch 00030: val_loss improved from 1.80773 to 1.78528, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 99ms/step - loss: 1.4983 - accuracy: 0.5008 - val_loss: 1.7853 - val_accuracy: 0.4259\n",
      "Epoch 31/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.4578 - accuracy: 0.5147\n",
      "Epoch 00031: val_loss did not improve from 1.78528\n",
      "44/44 [==============================] - 5s 110ms/step - loss: 1.4596 - accuracy: 0.5139 - val_loss: 1.8005 - val_accuracy: 0.4190\n",
      "Epoch 32/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.4726 - accuracy: 0.5093\n",
      "Epoch 00032: val_loss improved from 1.78528 to 1.72558, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 95ms/step - loss: 1.4701 - accuracy: 0.5100 - val_loss: 1.7256 - val_accuracy: 0.4352\n",
      "Epoch 33/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.4235 - accuracy: 0.5225\n",
      "Epoch 00033: val_loss did not improve from 1.72558\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 1.4232 - accuracy: 0.5224 - val_loss: 1.7805 - val_accuracy: 0.4329\n",
      "Epoch 34/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.3795 - accuracy: 0.5450\n",
      "Epoch 00034: val_loss did not improve from 1.72558\n",
      "44/44 [==============================] - 4s 87ms/step - loss: 1.3807 - accuracy: 0.5448 - val_loss: 1.7583 - val_accuracy: 0.4583\n",
      "Epoch 35/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.3810 - accuracy: 0.5450\n",
      "Epoch 00035: val_loss did not improve from 1.72558\n",
      "44/44 [==============================] - 4s 85ms/step - loss: 1.3807 - accuracy: 0.5448 - val_loss: 1.8140 - val_accuracy: 0.4352\n",
      "Epoch 36/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.3791 - accuracy: 0.5434\n",
      "Epoch 00036: val_loss did not improve from 1.72558\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 1.3788 - accuracy: 0.5424 - val_loss: 1.8132 - val_accuracy: 0.4352\n",
      "Epoch 37/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.3378 - accuracy: 0.5550\n",
      "Epoch 00037: val_loss did not improve from 1.72558\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 1.3379 - accuracy: 0.5548 - val_loss: 1.7558 - val_accuracy: 0.4537\n",
      "Epoch 38/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.3278 - accuracy: 0.5512\n",
      "Epoch 00038: val_loss did not improve from 1.72558\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 1.3274 - accuracy: 0.5517 - val_loss: 1.7677 - val_accuracy: 0.4468\n",
      "Epoch 39/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.3400 - accuracy: 0.5426\n",
      "Epoch 00039: val_loss did not improve from 1.72558\n",
      "44/44 [==============================] - 4s 86ms/step - loss: 1.3396 - accuracy: 0.5417 - val_loss: 1.7371 - val_accuracy: 0.4606\n",
      "Epoch 40/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.3154 - accuracy: 0.5628\n",
      "Epoch 00040: val_loss did not improve from 1.72558\n",
      "44/44 [==============================] - 4s 88ms/step - loss: 1.3180 - accuracy: 0.5625 - val_loss: 1.7402 - val_accuracy: 0.4491\n",
      "Epoch 41/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.2805 - accuracy: 0.5775\n",
      "Epoch 00041: val_loss did not improve from 1.72558\n",
      "44/44 [==============================] - 4s 88ms/step - loss: 1.2832 - accuracy: 0.5764 - val_loss: 1.7513 - val_accuracy: 0.4583\n",
      "Epoch 42/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.2706 - accuracy: 0.5806\n",
      "Epoch 00042: val_loss did not improve from 1.72558\n",
      "44/44 [==============================] - 4s 88ms/step - loss: 1.2743 - accuracy: 0.5802 - val_loss: 1.7702 - val_accuracy: 0.4514\n",
      "Epoch 43/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.2717 - accuracy: 0.5868\n",
      "Epoch 00043: val_loss improved from 1.72558 to 1.69835, saving model to weights12.best.hdf5\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 1.2728 - accuracy: 0.5864 - val_loss: 1.6983 - val_accuracy: 0.4630\n",
      "Epoch 44/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.2568 - accuracy: 0.5767\n",
      "Epoch 00044: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 89ms/step - loss: 1.2597 - accuracy: 0.5764 - val_loss: 1.7632 - val_accuracy: 0.4722\n",
      "Epoch 45/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.2408 - accuracy: 0.5868\n",
      "Epoch 00045: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 89ms/step - loss: 1.2422 - accuracy: 0.5864 - val_loss: 1.7814 - val_accuracy: 0.4560\n",
      "Epoch 46/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.2271 - accuracy: 0.5829\n",
      "Epoch 00046: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 1.2262 - accuracy: 0.5841 - val_loss: 1.7649 - val_accuracy: 0.4861\n",
      "Epoch 47/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.2150 - accuracy: 0.5922\n",
      "Epoch 00047: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 5s 103ms/step - loss: 1.2169 - accuracy: 0.5926 - val_loss: 1.7226 - val_accuracy: 0.4838\n",
      "Epoch 48/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1876 - accuracy: 0.6039\n",
      "Epoch 00048: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 5s 114ms/step - loss: 1.1860 - accuracy: 0.6042 - val_loss: 1.7365 - val_accuracy: 0.4653\n",
      "Epoch 49/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1467 - accuracy: 0.6233\n",
      "Epoch 00049: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 5s 111ms/step - loss: 1.1504 - accuracy: 0.6227 - val_loss: 1.7311 - val_accuracy: 0.4931\n",
      "Epoch 50/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1590 - accuracy: 0.6202\n",
      "Epoch 00050: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 92ms/step - loss: 1.1597 - accuracy: 0.6196 - val_loss: 1.7589 - val_accuracy: 0.4769\n",
      "Epoch 51/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1531 - accuracy: 0.6085\n",
      "Epoch 00051: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 92ms/step - loss: 1.1527 - accuracy: 0.6096 - val_loss: 1.7065 - val_accuracy: 0.5000\n",
      "Epoch 52/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1284 - accuracy: 0.6202\n",
      "Epoch 00052: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 92ms/step - loss: 1.1287 - accuracy: 0.6196 - val_loss: 1.7492 - val_accuracy: 0.4815\n",
      "Epoch 53/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1224 - accuracy: 0.6372\n",
      "Epoch 00053: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 93ms/step - loss: 1.1224 - accuracy: 0.6373 - val_loss: 1.7508 - val_accuracy: 0.5023\n",
      "Epoch 54/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1201 - accuracy: 0.6302\n",
      "Epoch 00054: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 1.1226 - accuracy: 0.6289 - val_loss: 1.7392 - val_accuracy: 0.4722\n",
      "Epoch 55/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.1180 - accuracy: 0.6240\n",
      "Epoch 00055: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 94ms/step - loss: 1.1202 - accuracy: 0.6242 - val_loss: 1.7405 - val_accuracy: 0.4815\n",
      "Epoch 56/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0808 - accuracy: 0.6302\n",
      "Epoch 00056: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 92ms/step - loss: 1.0799 - accuracy: 0.6312 - val_loss: 1.7549 - val_accuracy: 0.4815\n",
      "Epoch 57/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/44 [============================>.] - ETA: 0s - loss: 1.0577 - accuracy: 0.6581\n",
      "Epoch 00057: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 85ms/step - loss: 1.0591 - accuracy: 0.6582 - val_loss: 1.7201 - val_accuracy: 0.5324\n",
      "Epoch 58/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0504 - accuracy: 0.6519\n",
      "Epoch 00058: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 88ms/step - loss: 1.0489 - accuracy: 0.6528 - val_loss: 1.7442 - val_accuracy: 0.5301\n",
      "Epoch 59/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0819 - accuracy: 0.6395\n",
      "Epoch 00059: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 5s 104ms/step - loss: 1.0817 - accuracy: 0.6397 - val_loss: 1.7493 - val_accuracy: 0.4954\n",
      "Epoch 60/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0704 - accuracy: 0.6395\n",
      "Epoch 00060: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 96ms/step - loss: 1.0669 - accuracy: 0.6412 - val_loss: 1.8183 - val_accuracy: 0.4861\n",
      "Epoch 61/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0150 - accuracy: 0.6682\n",
      "Epoch 00061: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 88ms/step - loss: 1.0107 - accuracy: 0.6698 - val_loss: 1.7915 - val_accuracy: 0.4815\n",
      "Epoch 62/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0307 - accuracy: 0.6457\n",
      "Epoch 00062: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 87ms/step - loss: 1.0292 - accuracy: 0.6466 - val_loss: 1.7218 - val_accuracy: 0.5208\n",
      "Epoch 63/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 1.0169 - accuracy: 0.6589\n",
      "Epoch 00063: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 89ms/step - loss: 1.0176 - accuracy: 0.6582 - val_loss: 1.8733 - val_accuracy: 0.5093\n",
      "Epoch 64/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.9800 - accuracy: 0.6690\n",
      "Epoch 00064: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 0.9845 - accuracy: 0.6674 - val_loss: 1.7626 - val_accuracy: 0.5185\n",
      "Epoch 65/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.9542 - accuracy: 0.6705\n",
      "Epoch 00065: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 88ms/step - loss: 0.9546 - accuracy: 0.6705 - val_loss: 1.8220 - val_accuracy: 0.5093\n",
      "Epoch 66/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.9672 - accuracy: 0.6868\n",
      "Epoch 00066: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 89ms/step - loss: 0.9684 - accuracy: 0.6867 - val_loss: 1.8277 - val_accuracy: 0.5162\n",
      "Epoch 67/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.9894 - accuracy: 0.6574\n",
      "Epoch 00067: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 89ms/step - loss: 0.9920 - accuracy: 0.6566 - val_loss: 1.8173 - val_accuracy: 0.5093\n",
      "Epoch 68/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.9682 - accuracy: 0.6651\n",
      "Epoch 00068: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 89ms/step - loss: 0.9647 - accuracy: 0.6667 - val_loss: 1.8037 - val_accuracy: 0.5162\n",
      "Epoch 69/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.9162 - accuracy: 0.6992\n",
      "Epoch 00069: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 5s 123ms/step - loss: 0.9154 - accuracy: 0.6998 - val_loss: 1.8353 - val_accuracy: 0.5162\n",
      "Epoch 70/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.9497 - accuracy: 0.6798\n",
      "Epoch 00070: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 5s 107ms/step - loss: 0.9500 - accuracy: 0.6806 - val_loss: 1.8104 - val_accuracy: 0.5255\n",
      "Epoch 71/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.9892 - accuracy: 0.6605\n",
      "Epoch 00071: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.9897 - accuracy: 0.6605 - val_loss: 1.8332 - val_accuracy: 0.4954\n",
      "Epoch 72/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.8933 - accuracy: 0.6992\n",
      "Epoch 00072: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 100ms/step - loss: 0.8947 - accuracy: 0.6991 - val_loss: 1.7956 - val_accuracy: 0.5301\n",
      "Epoch 73/300\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.8929 - accuracy: 0.6961\n",
      "Epoch 00073: val_loss did not improve from 1.69835\n",
      "44/44 [==============================] - 4s 90ms/step - loss: 0.8950 - accuracy: 0.6944 - val_loss: 1.7487 - val_accuracy: 0.5532\n",
      "Training completed in time:  0:05:01.079252\n"
     ]
    }
   ],
   "source": [
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights12.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "\n",
    "model.fit(X_train2, y_train,\n",
    "          batch_size = 30, \n",
    "          epochs = 300,\n",
    "          validation_data = (X_val2, y_val), \n",
    "          callbacks=[checkpointer, es],verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_74 (Conv2D)          (None, 39, 173, 16)       80        \n",
      "                                                                 \n",
      " max_pooling2d_71 (MaxPoolin  (None, 19, 86, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_77 (Dropout)        (None, 19, 86, 16)        0         \n",
      "                                                                 \n",
      " conv2d_75 (Conv2D)          (None, 18, 85, 128)       8320      \n",
      "                                                                 \n",
      " max_pooling2d_72 (MaxPoolin  (None, 9, 42, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_78 (Dropout)        (None, 9, 42, 128)        0         \n",
      "                                                                 \n",
      " conv2d_76 (Conv2D)          (None, 8, 41, 256)        131328    \n",
      "                                                                 \n",
      " max_pooling2d_73 (MaxPoolin  (None, 4, 20, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_79 (Dropout)        (None, 4, 20, 256)        0         \n",
      "                                                                 \n",
      " conv2d_77 (Conv2D)          (None, 3, 19, 256)        262400    \n",
      "                                                                 \n",
      " max_pooling2d_74 (MaxPoolin  (None, 1, 9, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_80 (Dropout)        (None, 1, 9, 256)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_16  (None, 256)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 18)                4626      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 406,754\n",
      "Trainable params: 406,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Conv2D(filters=16, kernel_size=2, input_shape=input_shape, activation='relu'))\n",
    "model_relu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_relu.add(Dropout(0.2))\n",
    "\n",
    "model_relu.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model_relu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_relu.add(Dropout(0.2))\n",
    "\n",
    "model_relu.add(Conv2D(filters=256, kernel_size=2, activation='relu'))\n",
    "model_relu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_relu.add(Dropout(0.2))\n",
    "\n",
    "model_relu.add(Conv2D(filters=256, kernel_size=2, activation='relu'))\n",
    "model_relu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_relu.add(Dropout(0.25))\n",
    "model_relu.add(GlobalAveragePooling2D())\n",
    "model_relu.add(Flatten())\n",
    "model_relu.add(Dense(18, activation='softmax'))\n",
    "\n",
    "\n",
    "model_relu.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 2.3097 - accuracy: 0.2383\n",
      "Epoch 00001: val_loss improved from inf to 2.04981, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 46s 698ms/step - loss: 2.3097 - accuracy: 0.2383 - val_loss: 2.0498 - val_accuracy: 0.3521\n",
      "Epoch 2/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.8674 - accuracy: 0.3754\n",
      "Epoch 00002: val_loss improved from 2.04981 to 1.80407, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 44s 691ms/step - loss: 1.8674 - accuracy: 0.3754 - val_loss: 1.8041 - val_accuracy: 0.4045\n",
      "Epoch 3/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.6901 - accuracy: 0.4507\n",
      "Epoch 00003: val_loss improved from 1.80407 to 1.65900, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 698ms/step - loss: 1.6901 - accuracy: 0.4507 - val_loss: 1.6590 - val_accuracy: 0.4759\n",
      "Epoch 4/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.5656 - accuracy: 0.4962\n",
      "Epoch 00004: val_loss improved from 1.65900 to 1.50194, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 703ms/step - loss: 1.5656 - accuracy: 0.4962 - val_loss: 1.5019 - val_accuracy: 0.5213\n",
      "Epoch 5/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.4742 - accuracy: 0.5246\n",
      "Epoch 00005: val_loss improved from 1.50194 to 1.42641, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 709ms/step - loss: 1.4742 - accuracy: 0.5246 - val_loss: 1.4264 - val_accuracy: 0.5378\n",
      "Epoch 6/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.3904 - accuracy: 0.5603\n",
      "Epoch 00006: val_loss improved from 1.42641 to 1.37534, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 704ms/step - loss: 1.3904 - accuracy: 0.5603 - val_loss: 1.3753 - val_accuracy: 0.5629\n",
      "Epoch 7/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.3057 - accuracy: 0.5788\n",
      "Epoch 00007: val_loss improved from 1.37534 to 1.30368, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 46s 719ms/step - loss: 1.3057 - accuracy: 0.5788 - val_loss: 1.3037 - val_accuracy: 0.5813\n",
      "Epoch 8/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.2365 - accuracy: 0.6098\n",
      "Epoch 00008: val_loss improved from 1.30368 to 1.29082, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 47s 730ms/step - loss: 1.2365 - accuracy: 0.6098 - val_loss: 1.2908 - val_accuracy: 0.5988\n",
      "Epoch 9/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.1870 - accuracy: 0.6175\n",
      "Epoch 00009: val_loss improved from 1.29082 to 1.21392, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 46s 722ms/step - loss: 1.1870 - accuracy: 0.6175 - val_loss: 1.2139 - val_accuracy: 0.6011\n",
      "Epoch 10/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.1240 - accuracy: 0.6405\n",
      "Epoch 00010: val_loss improved from 1.21392 to 1.20759, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 701ms/step - loss: 1.1240 - accuracy: 0.6405 - val_loss: 1.2076 - val_accuracy: 0.6026\n",
      "Epoch 11/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.0759 - accuracy: 0.6566\n",
      "Epoch 00011: val_loss improved from 1.20759 to 1.13258, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 46s 710ms/step - loss: 1.0759 - accuracy: 0.6566 - val_loss: 1.1326 - val_accuracy: 0.6309\n",
      "Epoch 12/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 1.0247 - accuracy: 0.6712\n",
      "Epoch 00012: val_loss improved from 1.13258 to 1.07954, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 47s 729ms/step - loss: 1.0247 - accuracy: 0.6712 - val_loss: 1.0795 - val_accuracy: 0.6493\n",
      "Epoch 13/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.9641 - accuracy: 0.6842\n",
      "Epoch 00013: val_loss improved from 1.07954 to 1.03133, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 46s 712ms/step - loss: 0.9641 - accuracy: 0.6842 - val_loss: 1.0313 - val_accuracy: 0.6645\n",
      "Epoch 14/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.9553 - accuracy: 0.6985\n",
      "Epoch 00014: val_loss did not improve from 1.03133\n",
      "64/64 [==============================] - 46s 712ms/step - loss: 0.9553 - accuracy: 0.6985 - val_loss: 1.0394 - val_accuracy: 0.6673\n",
      "Epoch 15/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.9212 - accuracy: 0.6968\n",
      "Epoch 00015: val_loss improved from 1.03133 to 1.02314, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 46s 721ms/step - loss: 0.9212 - accuracy: 0.6968 - val_loss: 1.0231 - val_accuracy: 0.6786\n",
      "Epoch 16/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.8658 - accuracy: 0.7161\n",
      "Epoch 00016: val_loss improved from 1.02314 to 0.98040, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 709ms/step - loss: 0.8658 - accuracy: 0.7161 - val_loss: 0.9804 - val_accuracy: 0.6957\n",
      "Epoch 17/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.8473 - accuracy: 0.7213\n",
      "Epoch 00017: val_loss improved from 0.98040 to 0.93200, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 46s 713ms/step - loss: 0.8473 - accuracy: 0.7213 - val_loss: 0.9320 - val_accuracy: 0.7084\n",
      "Epoch 18/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.8324 - accuracy: 0.7261\n",
      "Epoch 00018: val_loss improved from 0.93200 to 0.91403, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 700ms/step - loss: 0.8324 - accuracy: 0.7261 - val_loss: 0.9140 - val_accuracy: 0.7046\n",
      "Epoch 19/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.8019 - accuracy: 0.7286\n",
      "Epoch 00019: val_loss improved from 0.91403 to 0.90070, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 704ms/step - loss: 0.8019 - accuracy: 0.7286 - val_loss: 0.9007 - val_accuracy: 0.7108\n",
      "Epoch 20/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.7697 - accuracy: 0.7487\n",
      "Epoch 00020: val_loss did not improve from 0.90070\n",
      "64/64 [==============================] - 44s 694ms/step - loss: 0.7697 - accuracy: 0.7487 - val_loss: 0.9155 - val_accuracy: 0.7051\n",
      "Epoch 21/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.7297 - accuracy: 0.7552\n",
      "Epoch 00021: val_loss improved from 0.90070 to 0.86912, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 696ms/step - loss: 0.7297 - accuracy: 0.7552 - val_loss: 0.8691 - val_accuracy: 0.7169\n",
      "Epoch 22/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.7329 - accuracy: 0.7566\n",
      "Epoch 00022: val_loss improved from 0.86912 to 0.84189, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 698ms/step - loss: 0.7329 - accuracy: 0.7566 - val_loss: 0.8419 - val_accuracy: 0.7335\n",
      "Epoch 23/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.6880 - accuracy: 0.7676\n",
      "Epoch 00023: val_loss did not improve from 0.84189\n",
      "64/64 [==============================] - 47s 729ms/step - loss: 0.6880 - accuracy: 0.7676 - val_loss: 0.8804 - val_accuracy: 0.7344\n",
      "Epoch 24/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.6771 - accuracy: 0.7739\n",
      "Epoch 00024: val_loss improved from 0.84189 to 0.83303, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 710ms/step - loss: 0.6771 - accuracy: 0.7739 - val_loss: 0.8330 - val_accuracy: 0.7377\n",
      "Epoch 25/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.6740 - accuracy: 0.7700\n",
      "Epoch 00025: val_loss did not improve from 0.83303\n",
      "64/64 [==============================] - 46s 711ms/step - loss: 0.6740 - accuracy: 0.7700 - val_loss: 0.9104 - val_accuracy: 0.7141\n",
      "Epoch 26/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.6655 - accuracy: 0.7714\n",
      "Epoch 00026: val_loss improved from 0.83303 to 0.82773, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 710ms/step - loss: 0.6655 - accuracy: 0.7714 - val_loss: 0.8277 - val_accuracy: 0.7391\n",
      "Epoch 27/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 0.6184 - accuracy: 0.7928\n",
      "Epoch 00027: val_loss improved from 0.82773 to 0.81747, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 47s 735ms/step - loss: 0.6184 - accuracy: 0.7928 - val_loss: 0.8175 - val_accuracy: 0.7339\n",
      "Epoch 28/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.6199 - accuracy: 0.7892\n",
      "Epoch 00028: val_loss improved from 0.81747 to 0.79226, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 45s 709ms/step - loss: 0.6199 - accuracy: 0.7892 - val_loss: 0.7923 - val_accuracy: 0.7528\n",
      "Epoch 29/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5991 - accuracy: 0.7951\n",
      "Epoch 00029: val_loss did not improve from 0.79226\n",
      "64/64 [==============================] - 45s 710ms/step - loss: 0.5991 - accuracy: 0.7951 - val_loss: 0.8066 - val_accuracy: 0.7547\n",
      "Epoch 30/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.8036\n",
      "Epoch 00030: val_loss did not improve from 0.79226\n",
      "64/64 [==============================] - 46s 726ms/step - loss: 0.5862 - accuracy: 0.8036 - val_loss: 0.8792 - val_accuracy: 0.7344\n",
      "Epoch 31/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5944 - accuracy: 0.7966\n",
      "Epoch 00031: val_loss did not improve from 0.79226\n",
      "64/64 [==============================] - 44s 689ms/step - loss: 0.5944 - accuracy: 0.7966 - val_loss: 0.8230 - val_accuracy: 0.7453\n",
      "Epoch 32/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5546 - accuracy: 0.8061\n",
      "Epoch 00032: val_loss improved from 0.79226 to 0.76566, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 49s 763ms/step - loss: 0.5546 - accuracy: 0.8061 - val_loss: 0.7657 - val_accuracy: 0.7604\n",
      "Epoch 33/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5369 - accuracy: 0.8149\n",
      "Epoch 00033: val_loss did not improve from 0.76566\n",
      "64/64 [==============================] - 48s 746ms/step - loss: 0.5369 - accuracy: 0.8149 - val_loss: 0.7763 - val_accuracy: 0.7557\n",
      "Epoch 34/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5412 - accuracy: 0.8143\n",
      "Epoch 00034: val_loss did not improve from 0.76566\n",
      "64/64 [==============================] - 52s 820ms/step - loss: 0.5412 - accuracy: 0.8143 - val_loss: 0.7808 - val_accuracy: 0.7637\n",
      "Epoch 35/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5321 - accuracy: 0.8151\n",
      "Epoch 00035: val_loss did not improve from 0.76566\n",
      "64/64 [==============================] - 51s 800ms/step - loss: 0.5321 - accuracy: 0.8151 - val_loss: 0.8109 - val_accuracy: 0.7595\n",
      "Epoch 36/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5128 - accuracy: 0.8214\n",
      "Epoch 00036: val_loss did not improve from 0.76566\n",
      "64/64 [==============================] - 51s 801ms/step - loss: 0.5128 - accuracy: 0.8214 - val_loss: 0.7936 - val_accuracy: 0.7595\n",
      "Epoch 37/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5090 - accuracy: 0.8266\n",
      "Epoch 00037: val_loss improved from 0.76566 to 0.75165, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 49s 764ms/step - loss: 0.5090 - accuracy: 0.8266 - val_loss: 0.7517 - val_accuracy: 0.7613\n",
      "Epoch 38/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.8258\n",
      "Epoch 00038: val_loss did not improve from 0.75165\n",
      "64/64 [==============================] - 46s 724ms/step - loss: 0.5023 - accuracy: 0.8258 - val_loss: 0.7542 - val_accuracy: 0.7651\n",
      "Epoch 39/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4886 - accuracy: 0.8346\n",
      "Epoch 00039: val_loss did not improve from 0.75165\n",
      "64/64 [==============================] - 51s 792ms/step - loss: 0.4886 - accuracy: 0.8346 - val_loss: 0.7755 - val_accuracy: 0.7632\n",
      "Epoch 40/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4734 - accuracy: 0.8348\n",
      "Epoch 00040: val_loss did not improve from 0.75165\n",
      "64/64 [==============================] - 48s 753ms/step - loss: 0.4734 - accuracy: 0.8348 - val_loss: 0.7598 - val_accuracy: 0.7736\n",
      "Epoch 41/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.8357\n",
      "Epoch 00041: val_loss did not improve from 0.75165\n",
      "64/64 [==============================] - 46s 724ms/step - loss: 0.4635 - accuracy: 0.8357 - val_loss: 0.7908 - val_accuracy: 0.7746\n",
      "Epoch 42/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4400 - accuracy: 0.8496\n",
      "Epoch 00042: val_loss did not improve from 0.75165\n",
      "64/64 [==============================] - 45s 705ms/step - loss: 0.4400 - accuracy: 0.8496 - val_loss: 0.7985 - val_accuracy: 0.7713\n",
      "Epoch 43/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4600 - accuracy: 0.8423\n",
      "Epoch 00043: val_loss did not improve from 0.75165\n",
      "64/64 [==============================] - 47s 725ms/step - loss: 0.4600 - accuracy: 0.8423 - val_loss: 0.7744 - val_accuracy: 0.7727\n",
      "Epoch 44/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.8434\n",
      "Epoch 00044: val_loss improved from 0.75165 to 0.72685, saving model to weights16.best.hdf5\n",
      "64/64 [==============================] - 53s 824ms/step - loss: 0.4474 - accuracy: 0.8434 - val_loss: 0.7269 - val_accuracy: 0.7845\n",
      "Epoch 45/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.8532\n",
      "Epoch 00045: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 47s 726ms/step - loss: 0.4245 - accuracy: 0.8532 - val_loss: 0.7538 - val_accuracy: 0.7845\n",
      "Epoch 46/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.8510\n",
      "Epoch 00046: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 49s 763ms/step - loss: 0.4361 - accuracy: 0.8510 - val_loss: 0.7867 - val_accuracy: 0.7774\n",
      "Epoch 47/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.8529\n",
      "Epoch 00047: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 56s 884ms/step - loss: 0.4296 - accuracy: 0.8529 - val_loss: 0.7702 - val_accuracy: 0.7836\n",
      "Epoch 48/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4186 - accuracy: 0.8541\n",
      "Epoch 00048: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 726ms/step - loss: 0.4186 - accuracy: 0.8541 - val_loss: 0.7483 - val_accuracy: 0.7826\n",
      "Epoch 49/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.8478\n",
      "Epoch 00049: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 715ms/step - loss: 0.4183 - accuracy: 0.8478 - val_loss: 0.7741 - val_accuracy: 0.7708\n",
      "Epoch 50/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4131 - accuracy: 0.8570\n",
      "Epoch 00050: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 715ms/step - loss: 0.4131 - accuracy: 0.8570 - val_loss: 0.7377 - val_accuracy: 0.7840\n",
      "Epoch 51/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.8642\n",
      "Epoch 00051: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 711ms/step - loss: 0.3959 - accuracy: 0.8642 - val_loss: 0.7534 - val_accuracy: 0.7888\n",
      "Epoch 52/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3890 - accuracy: 0.8628\n",
      "Epoch 00052: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 45s 699ms/step - loss: 0.3890 - accuracy: 0.8628 - val_loss: 0.7874 - val_accuracy: 0.7769\n",
      "Epoch 53/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3902 - accuracy: 0.8587\n",
      "Epoch 00053: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 717ms/step - loss: 0.3902 - accuracy: 0.8587 - val_loss: 0.7918 - val_accuracy: 0.7817\n",
      "Epoch 54/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3672 - accuracy: 0.8719\n",
      "Epoch 00054: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 49s 770ms/step - loss: 0.3672 - accuracy: 0.8719 - val_loss: 0.7714 - val_accuracy: 0.7802\n",
      "Epoch 55/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3784 - accuracy: 0.8670\n",
      "Epoch 00055: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 50s 777ms/step - loss: 0.3784 - accuracy: 0.8670 - val_loss: 0.7807 - val_accuracy: 0.7793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3712 - accuracy: 0.8688\n",
      "Epoch 00056: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 53s 826ms/step - loss: 0.3712 - accuracy: 0.8688 - val_loss: 0.8192 - val_accuracy: 0.7755\n",
      "Epoch 57/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3627 - accuracy: 0.8715\n",
      "Epoch 00057: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 51s 799ms/step - loss: 0.3627 - accuracy: 0.8715 - val_loss: 0.8397 - val_accuracy: 0.7798\n",
      "Epoch 58/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3663 - accuracy: 0.8710\n",
      "Epoch 00058: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 52s 808ms/step - loss: 0.3663 - accuracy: 0.8710 - val_loss: 0.7738 - val_accuracy: 0.7793\n",
      "Epoch 59/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8661\n",
      "Epoch 00059: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 714ms/step - loss: 0.3696 - accuracy: 0.8661 - val_loss: 0.7657 - val_accuracy: 0.7840\n",
      "Epoch 60/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3609 - accuracy: 0.8785\n",
      "Epoch 00060: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 47s 728ms/step - loss: 0.3609 - accuracy: 0.8785 - val_loss: 0.8190 - val_accuracy: 0.7765\n",
      "Epoch 61/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.8781\n",
      "Epoch 00061: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 45s 705ms/step - loss: 0.3493 - accuracy: 0.8781 - val_loss: 0.7650 - val_accuracy: 0.7921\n",
      "Epoch 62/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3442 - accuracy: 0.8800\n",
      "Epoch 00062: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 47s 732ms/step - loss: 0.3442 - accuracy: 0.8800 - val_loss: 0.8051 - val_accuracy: 0.7812\n",
      "Epoch 63/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3473 - accuracy: 0.8763\n",
      "Epoch 00063: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 45s 702ms/step - loss: 0.3473 - accuracy: 0.8763 - val_loss: 0.8000 - val_accuracy: 0.7817\n",
      "Epoch 64/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3630 - accuracy: 0.8715\n",
      "Epoch 00064: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 716ms/step - loss: 0.3630 - accuracy: 0.8715 - val_loss: 0.7863 - val_accuracy: 0.7888\n",
      "Epoch 65/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.8891\n",
      "Epoch 00065: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 49s 766ms/step - loss: 0.3271 - accuracy: 0.8891 - val_loss: 0.8079 - val_accuracy: 0.7802\n",
      "Epoch 66/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3403 - accuracy: 0.8782\n",
      "Epoch 00066: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 49s 762ms/step - loss: 0.3403 - accuracy: 0.8782 - val_loss: 0.8107 - val_accuracy: 0.7802\n",
      "Epoch 67/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.8820\n",
      "Epoch 00067: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 47s 729ms/step - loss: 0.3338 - accuracy: 0.8820 - val_loss: 0.8488 - val_accuracy: 0.7878\n",
      "Epoch 68/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3361 - accuracy: 0.8784\n",
      "Epoch 00068: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 712ms/step - loss: 0.3361 - accuracy: 0.8784 - val_loss: 0.8047 - val_accuracy: 0.7840\n",
      "Epoch 69/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3280 - accuracy: 0.8850\n",
      "Epoch 00069: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 45s 698ms/step - loss: 0.3280 - accuracy: 0.8850 - val_loss: 0.8152 - val_accuracy: 0.7812\n",
      "Epoch 70/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3153 - accuracy: 0.8907\n",
      "Epoch 00070: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 47s 738ms/step - loss: 0.3153 - accuracy: 0.8907 - val_loss: 0.8193 - val_accuracy: 0.7878\n",
      "Epoch 71/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3277 - accuracy: 0.8848\n",
      "Epoch 00071: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 48s 751ms/step - loss: 0.3277 - accuracy: 0.8848 - val_loss: 0.8142 - val_accuracy: 0.7836\n",
      "Epoch 72/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.8863\n",
      "Epoch 00072: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 45s 700ms/step - loss: 0.3123 - accuracy: 0.8863 - val_loss: 0.8323 - val_accuracy: 0.7854\n",
      "Epoch 73/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.8904\n",
      "Epoch 00073: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 711ms/step - loss: 0.3098 - accuracy: 0.8904 - val_loss: 0.8105 - val_accuracy: 0.7897\n",
      "Epoch 74/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.8839\n",
      "Epoch 00074: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 47s 739ms/step - loss: 0.3175 - accuracy: 0.8839 - val_loss: 0.8485 - val_accuracy: 0.7798\n",
      "Epoch 75/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.8893\n",
      "Epoch 00075: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 49s 763ms/step - loss: 0.3090 - accuracy: 0.8893 - val_loss: 0.8716 - val_accuracy: 0.7798\n",
      "Epoch 76/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3335 - accuracy: 0.8864\n",
      "Epoch 00076: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 45s 702ms/step - loss: 0.3335 - accuracy: 0.8864 - val_loss: 0.8673 - val_accuracy: 0.7845\n",
      "Epoch 77/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.8859\n",
      "Epoch 00077: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 45s 697ms/step - loss: 0.3065 - accuracy: 0.8859 - val_loss: 0.8218 - val_accuracy: 0.7925\n",
      "Epoch 78/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3091 - accuracy: 0.8859\n",
      "Epoch 00078: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 715ms/step - loss: 0.3091 - accuracy: 0.8859 - val_loss: 0.8075 - val_accuracy: 0.7850\n",
      "Epoch 79/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3173 - accuracy: 0.8875\n",
      "Epoch 00079: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 45s 702ms/step - loss: 0.3173 - accuracy: 0.8875 - val_loss: 0.8296 - val_accuracy: 0.7793\n",
      "Epoch 80/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3173 - accuracy: 0.8836\n",
      "Epoch 00080: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 721ms/step - loss: 0.3173 - accuracy: 0.8836 - val_loss: 0.8386 - val_accuracy: 0.7750\n",
      "Epoch 81/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.8948\n",
      "Epoch 00081: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 47s 735ms/step - loss: 0.2897 - accuracy: 0.8948 - val_loss: 0.8126 - val_accuracy: 0.7949\n",
      "Epoch 82/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.8970\n",
      "Epoch 00082: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 46s 717ms/step - loss: 0.2912 - accuracy: 0.8970 - val_loss: 0.8771 - val_accuracy: 0.7802\n",
      "Epoch 83/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.8899\n",
      "Epoch 00083: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 47s 727ms/step - loss: 0.3036 - accuracy: 0.8899 - val_loss: 0.8614 - val_accuracy: 0.7817\n",
      "Epoch 84/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.8902\n",
      "Epoch 00084: val_loss did not improve from 0.72685\n",
      "64/64 [==============================] - 45s 706ms/step - loss: 0.3012 - accuracy: 0.8902 - val_loss: 0.8887 - val_accuracy: 0.7854\n",
      "Training completed in time:  1:05:32.050109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=40, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights16.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "\n",
    "model_relu.fit(X_train2, y_train,\n",
    "          batch_size = 100, \n",
    "          epochs = 300,\n",
    "          validation_data = (X_val2, y_val), \n",
    "          callbacks=[checkpointer, es])\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training-Validation Accuracy')"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#saving the model history\n",
    "loss = pd.DataFrame(model_relu.history.history)\n",
    "\n",
    "\n",
    "#plotting the loss and accuracy \n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(loss[\"loss\"], label =\"Loss\")\n",
    "plt.plot(loss[\"val_loss\"], label = \"Validation_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(loss['accuracy'],label = \"Training Accuracy\")\n",
    "plt.plot(loss['val_accuracy'], label =\"Validation_ Accuracy \")\n",
    "plt.legend()\n",
    "plt.title(\"Training-Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13  0 11 ... 16 10 12]\n"
     ]
    }
   ],
   "source": [
    "prediction = model_relu.predict(X_val2)\n",
    "\n",
    "# finding class with larget predicted probability using argmax of numpy \n",
    "y_pred = np.argmax(prediction, axis = 1)  # prediction using model \n",
    "y_val_orig = np.argmax(y_val, axis = 1) # original y_val\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.30      0.26        54\n",
      "           1       0.10      0.05      0.06        41\n",
      "           2       0.21      0.11      0.14        66\n",
      "           3       0.27      0.37      0.31        27\n",
      "           4       0.34      0.36      0.35        58\n",
      "           5       0.17      0.06      0.09        35\n",
      "           6       0.29      0.33      0.31        54\n",
      "           7       0.07      0.16      0.10        19\n",
      "           8       0.95      0.97      0.96       211\n",
      "           9       0.92      0.94      0.93        95\n",
      "          10       0.85      0.79      0.82       197\n",
      "          11       0.92      0.83      0.87       211\n",
      "          12       0.91      0.89      0.90       199\n",
      "          13       0.93      0.95      0.94       208\n",
      "          14       0.75      0.95      0.84        64\n",
      "          15       0.89      0.95      0.92       210\n",
      "          16       0.95      0.92      0.94       198\n",
      "          17       0.75      0.83      0.79       169\n",
      "\n",
      "    accuracy                           0.79      2116\n",
      "   macro avg       0.58      0.60      0.58      2116\n",
      "weighted avg       0.78      0.79      0.78      2116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val_orig, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_78 (Conv2D)          (None, 39, 173, 16)       80        \n",
      "                                                                 \n",
      " max_pooling2d_75 (MaxPoolin  (None, 19, 86, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_81 (Dropout)        (None, 19, 86, 16)        0         \n",
      "                                                                 \n",
      " conv2d_79 (Conv2D)          (None, 18, 85, 32)        2080      \n",
      "                                                                 \n",
      " max_pooling2d_76 (MaxPoolin  (None, 9, 42, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_82 (Dropout)        (None, 9, 42, 32)         0         \n",
      "                                                                 \n",
      " conv2d_80 (Conv2D)          (None, 8, 41, 64)         8256      \n",
      "                                                                 \n",
      " max_pooling2d_77 (MaxPoolin  (None, 4, 20, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_83 (Dropout)        (None, 4, 20, 64)         0         \n",
      "                                                                 \n",
      " conv2d_81 (Conv2D)          (None, 3, 19, 128)        32896     \n",
      "                                                                 \n",
      " max_pooling2d_78 (MaxPoolin  (None, 1, 9, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_84 (Dropout)        (None, 1, 9, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_17  (None, 128)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 18)                2322      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,634\n",
      "Trainable params: 45,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=input_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(18, activation='softmax')) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.6896 - accuracy: 0.1041\n",
      "Epoch 00001: val_loss improved from inf to 2.52728, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 19s 668ms/step - loss: 2.6896 - accuracy: 0.1041 - val_loss: 2.5273 - val_accuracy: 0.1857\n",
      "Epoch 2/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.4292 - accuracy: 0.1827\n",
      "Epoch 00002: val_loss improved from 2.52728 to 2.31135, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 660ms/step - loss: 2.4292 - accuracy: 0.1827 - val_loss: 2.3114 - val_accuracy: 0.2335\n",
      "Epoch 3/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.2479 - accuracy: 0.2557\n",
      "Epoch 00003: val_loss improved from 2.31135 to 2.16790, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 672ms/step - loss: 2.2479 - accuracy: 0.2557 - val_loss: 2.1679 - val_accuracy: 0.2987\n",
      "Epoch 4/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.1457 - accuracy: 0.2889\n",
      "Epoch 00004: val_loss improved from 2.16790 to 2.07365, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 14s 655ms/step - loss: 2.1457 - accuracy: 0.2889 - val_loss: 2.0736 - val_accuracy: 0.3284\n",
      "Epoch 5/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 2.0628 - accuracy: 0.3198\n",
      "Epoch 00005: val_loss improved from 2.07365 to 2.01282, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 656ms/step - loss: 2.0628 - accuracy: 0.3198 - val_loss: 2.0128 - val_accuracy: 0.3445\n",
      "Epoch 6/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.9911 - accuracy: 0.3371\n",
      "Epoch 00006: val_loss improved from 2.01282 to 1.94946, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 675ms/step - loss: 1.9911 - accuracy: 0.3371 - val_loss: 1.9495 - val_accuracy: 0.3823\n",
      "Epoch 7/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.9157 - accuracy: 0.3626\n",
      "Epoch 00007: val_loss improved from 1.94946 to 1.88338, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 702ms/step - loss: 1.9157 - accuracy: 0.3626 - val_loss: 1.8834 - val_accuracy: 0.3951\n",
      "Epoch 8/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.8502 - accuracy: 0.3864\n",
      "Epoch 00008: val_loss improved from 1.88338 to 1.82813, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 694ms/step - loss: 1.8502 - accuracy: 0.3864 - val_loss: 1.8281 - val_accuracy: 0.4182\n",
      "Epoch 9/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.8086 - accuracy: 0.4009\n",
      "Epoch 00009: val_loss improved from 1.82813 to 1.82319, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 14s 655ms/step - loss: 1.8086 - accuracy: 0.4009 - val_loss: 1.8232 - val_accuracy: 0.4201\n",
      "Epoch 10/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.7572 - accuracy: 0.4165\n",
      "Epoch 00010: val_loss improved from 1.82319 to 1.72175, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 658ms/step - loss: 1.7572 - accuracy: 0.4165 - val_loss: 1.7218 - val_accuracy: 0.4447\n",
      "Epoch 11/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.7185 - accuracy: 0.4422\n",
      "Epoch 00011: val_loss improved from 1.72175 to 1.70445, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 664ms/step - loss: 1.7185 - accuracy: 0.4422 - val_loss: 1.7045 - val_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.6733 - accuracy: 0.4545\n",
      "Epoch 00012: val_loss improved from 1.70445 to 1.67762, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 673ms/step - loss: 1.6733 - accuracy: 0.4545 - val_loss: 1.6776 - val_accuracy: 0.4721\n",
      "Epoch 13/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.6445 - accuracy: 0.4624\n",
      "Epoch 00013: val_loss improved from 1.67762 to 1.64254, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 663ms/step - loss: 1.6445 - accuracy: 0.4624 - val_loss: 1.6425 - val_accuracy: 0.4844\n",
      "Epoch 14/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.6137 - accuracy: 0.4795\n",
      "Epoch 00014: val_loss improved from 1.64254 to 1.60180, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 658ms/step - loss: 1.6137 - accuracy: 0.4795 - val_loss: 1.6018 - val_accuracy: 0.4948\n",
      "Epoch 15/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.5842 - accuracy: 0.4899\n",
      "Epoch 00015: val_loss improved from 1.60180 to 1.58704, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 657ms/step - loss: 1.5842 - accuracy: 0.4899 - val_loss: 1.5870 - val_accuracy: 0.4943\n",
      "Epoch 16/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.5668 - accuracy: 0.4940\n",
      "Epoch 00016: val_loss improved from 1.58704 to 1.55899, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 688ms/step - loss: 1.5668 - accuracy: 0.4940 - val_loss: 1.5590 - val_accuracy: 0.5066\n",
      "Epoch 17/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.5399 - accuracy: 0.4983\n",
      "Epoch 00017: val_loss did not improve from 1.55899\n",
      "22/22 [==============================] - 15s 666ms/step - loss: 1.5399 - accuracy: 0.4983 - val_loss: 1.5615 - val_accuracy: 0.5118\n",
      "Epoch 18/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.5241 - accuracy: 0.5142\n",
      "Epoch 00018: val_loss improved from 1.55899 to 1.54920, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 14s 656ms/step - loss: 1.5241 - accuracy: 0.5142 - val_loss: 1.5492 - val_accuracy: 0.5090\n",
      "Epoch 19/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.5102 - accuracy: 0.5128\n",
      "Epoch 00019: val_loss improved from 1.54920 to 1.52868, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 14s 655ms/step - loss: 1.5102 - accuracy: 0.5128 - val_loss: 1.5287 - val_accuracy: 0.5071\n",
      "Epoch 20/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.4881 - accuracy: 0.5225\n",
      "Epoch 00020: val_loss improved from 1.52868 to 1.48493, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 746ms/step - loss: 1.4881 - accuracy: 0.5225 - val_loss: 1.4849 - val_accuracy: 0.5279\n",
      "Epoch 21/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.4717 - accuracy: 0.5255\n",
      "Epoch 00021: val_loss did not improve from 1.48493\n",
      "22/22 [==============================] - 15s 665ms/step - loss: 1.4717 - accuracy: 0.5255 - val_loss: 1.4895 - val_accuracy: 0.5208\n",
      "Epoch 22/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.4479 - accuracy: 0.5329\n",
      "Epoch 00022: val_loss improved from 1.48493 to 1.45308, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 658ms/step - loss: 1.4479 - accuracy: 0.5329 - val_loss: 1.4531 - val_accuracy: 0.5373\n",
      "Epoch 23/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.4309 - accuracy: 0.5425\n",
      "Epoch 00023: val_loss improved from 1.45308 to 1.43512, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 682ms/step - loss: 1.4309 - accuracy: 0.5425 - val_loss: 1.4351 - val_accuracy: 0.5331\n",
      "Epoch 24/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.4219 - accuracy: 0.5411\n",
      "Epoch 00024: val_loss did not improve from 1.43512\n",
      "22/22 [==============================] - 15s 694ms/step - loss: 1.4219 - accuracy: 0.5411 - val_loss: 1.4355 - val_accuracy: 0.5354\n",
      "Epoch 25/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.4086 - accuracy: 0.5381\n",
      "Epoch 00025: val_loss improved from 1.43512 to 1.42569, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 689ms/step - loss: 1.4086 - accuracy: 0.5381 - val_loss: 1.4257 - val_accuracy: 0.5501\n",
      "Epoch 26/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.3766 - accuracy: 0.5566\n",
      "Epoch 00026: val_loss improved from 1.42569 to 1.41864, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 701ms/step - loss: 1.3766 - accuracy: 0.5566 - val_loss: 1.4186 - val_accuracy: 0.5520\n",
      "Epoch 27/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - ETA: 0s - loss: 1.3707 - accuracy: 0.5561\n",
      "Epoch 00027: val_loss improved from 1.41864 to 1.39586, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 675ms/step - loss: 1.3707 - accuracy: 0.5561 - val_loss: 1.3959 - val_accuracy: 0.5562\n",
      "Epoch 28/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.3656 - accuracy: 0.5624\n",
      "Epoch 00028: val_loss did not improve from 1.39586\n",
      "22/22 [==============================] - 15s 683ms/step - loss: 1.3656 - accuracy: 0.5624 - val_loss: 1.4033 - val_accuracy: 0.5572\n",
      "Epoch 29/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.3571 - accuracy: 0.5534\n",
      "Epoch 00029: val_loss did not improve from 1.39586\n",
      "22/22 [==============================] - 15s 686ms/step - loss: 1.3571 - accuracy: 0.5534 - val_loss: 1.4035 - val_accuracy: 0.5681\n",
      "Epoch 30/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.3420 - accuracy: 0.5671\n",
      "Epoch 00030: val_loss improved from 1.39586 to 1.35983, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 691ms/step - loss: 1.3420 - accuracy: 0.5671 - val_loss: 1.3598 - val_accuracy: 0.5699\n",
      "Epoch 31/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.3216 - accuracy: 0.5676\n",
      "Epoch 00031: val_loss improved from 1.35983 to 1.32889, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 17s 769ms/step - loss: 1.3216 - accuracy: 0.5676 - val_loss: 1.3289 - val_accuracy: 0.5789\n",
      "Epoch 32/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.3125 - accuracy: 0.5825\n",
      "Epoch 00032: val_loss did not improve from 1.32889\n",
      "22/22 [==============================] - 15s 687ms/step - loss: 1.3125 - accuracy: 0.5825 - val_loss: 1.3494 - val_accuracy: 0.5770\n",
      "Epoch 33/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.2929 - accuracy: 0.5794\n",
      "Epoch 00033: val_loss did not improve from 1.32889\n",
      "22/22 [==============================] - 15s 667ms/step - loss: 1.2929 - accuracy: 0.5794 - val_loss: 1.3513 - val_accuracy: 0.5742\n",
      "Epoch 34/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.2839 - accuracy: 0.5884\n",
      "Epoch 00034: val_loss did not improve from 1.32889\n",
      "22/22 [==============================] - 15s 668ms/step - loss: 1.2839 - accuracy: 0.5884 - val_loss: 1.3348 - val_accuracy: 0.5709\n",
      "Epoch 35/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.2772 - accuracy: 0.5912\n",
      "Epoch 00035: val_loss improved from 1.32889 to 1.29933, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 675ms/step - loss: 1.2772 - accuracy: 0.5912 - val_loss: 1.2993 - val_accuracy: 0.5917\n",
      "Epoch 36/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.2606 - accuracy: 0.5923\n",
      "Epoch 00036: val_loss improved from 1.29933 to 1.27809, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 677ms/step - loss: 1.2606 - accuracy: 0.5923 - val_loss: 1.2781 - val_accuracy: 0.5955\n",
      "Epoch 37/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.2488 - accuracy: 0.5980\n",
      "Epoch 00037: val_loss improved from 1.27809 to 1.26032, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 677ms/step - loss: 1.2488 - accuracy: 0.5980 - val_loss: 1.2603 - val_accuracy: 0.5922\n",
      "Epoch 38/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.2279 - accuracy: 0.6049\n",
      "Epoch 00038: val_loss improved from 1.26032 to 1.25901, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 676ms/step - loss: 1.2279 - accuracy: 0.6049 - val_loss: 1.2590 - val_accuracy: 0.6002\n",
      "Epoch 39/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.2318 - accuracy: 0.6051\n",
      "Epoch 00039: val_loss improved from 1.25901 to 1.23969, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 678ms/step - loss: 1.2318 - accuracy: 0.6051 - val_loss: 1.2397 - val_accuracy: 0.6002\n",
      "Epoch 40/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.2209 - accuracy: 0.6071\n",
      "Epoch 00040: val_loss improved from 1.23969 to 1.22415, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 735ms/step - loss: 1.2209 - accuracy: 0.6071 - val_loss: 1.2242 - val_accuracy: 0.6068\n",
      "Epoch 41/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1979 - accuracy: 0.6078\n",
      "Epoch 00041: val_loss improved from 1.22415 to 1.20534, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 672ms/step - loss: 1.1979 - accuracy: 0.6078 - val_loss: 1.2053 - val_accuracy: 0.6181\n",
      "Epoch 42/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.2121 - accuracy: 0.6093\n",
      "Epoch 00042: val_loss did not improve from 1.20534\n",
      "22/22 [==============================] - 15s 670ms/step - loss: 1.2121 - accuracy: 0.6093 - val_loss: 1.2227 - val_accuracy: 0.6129\n",
      "Epoch 43/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1860 - accuracy: 0.6185\n",
      "Epoch 00043: val_loss improved from 1.20534 to 1.20093, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 671ms/step - loss: 1.1860 - accuracy: 0.6185 - val_loss: 1.2009 - val_accuracy: 0.6210\n",
      "Epoch 44/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1830 - accuracy: 0.6164\n",
      "Epoch 00044: val_loss improved from 1.20093 to 1.17170, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 670ms/step - loss: 1.1830 - accuracy: 0.6164 - val_loss: 1.1717 - val_accuracy: 0.6304\n",
      "Epoch 45/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1775 - accuracy: 0.6222\n",
      "Epoch 00045: val_loss did not improve from 1.17170\n",
      "22/22 [==============================] - 15s 668ms/step - loss: 1.1775 - accuracy: 0.6222 - val_loss: 1.1802 - val_accuracy: 0.6300\n",
      "Epoch 46/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1589 - accuracy: 0.6259\n",
      "Epoch 00046: val_loss did not improve from 1.17170\n",
      "22/22 [==============================] - 15s 676ms/step - loss: 1.1589 - accuracy: 0.6259 - val_loss: 1.2045 - val_accuracy: 0.6106\n",
      "Epoch 47/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1570 - accuracy: 0.6292\n",
      "Epoch 00047: val_loss did not improve from 1.17170\n",
      "22/22 [==============================] - 15s 665ms/step - loss: 1.1570 - accuracy: 0.6292 - val_loss: 1.1909 - val_accuracy: 0.6120\n",
      "Epoch 48/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1437 - accuracy: 0.6309\n",
      "Epoch 00048: val_loss did not improve from 1.17170\n",
      "22/22 [==============================] - 15s 697ms/step - loss: 1.1437 - accuracy: 0.6309 - val_loss: 1.1775 - val_accuracy: 0.6262\n",
      "Epoch 49/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1348 - accuracy: 0.6319\n",
      "Epoch 00049: val_loss improved from 1.17170 to 1.15374, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 667ms/step - loss: 1.1348 - accuracy: 0.6319 - val_loss: 1.1537 - val_accuracy: 0.6309\n",
      "Epoch 50/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1191 - accuracy: 0.6334\n",
      "Epoch 00050: val_loss did not improve from 1.15374\n",
      "22/22 [==============================] - 15s 662ms/step - loss: 1.1191 - accuracy: 0.6334 - val_loss: 1.1716 - val_accuracy: 0.6243\n",
      "Epoch 51/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1101 - accuracy: 0.6364\n",
      "Epoch 00051: val_loss improved from 1.15374 to 1.13628, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 696ms/step - loss: 1.1101 - accuracy: 0.6364 - val_loss: 1.1363 - val_accuracy: 0.6333\n",
      "Epoch 52/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1087 - accuracy: 0.6405\n",
      "Epoch 00052: val_loss did not improve from 1.13628\n",
      "22/22 [==============================] - 15s 669ms/step - loss: 1.1087 - accuracy: 0.6405 - val_loss: 1.1499 - val_accuracy: 0.6304\n",
      "Epoch 53/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.1063 - accuracy: 0.6437\n",
      "Epoch 00053: val_loss improved from 1.13628 to 1.12596, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 683ms/step - loss: 1.1063 - accuracy: 0.6437 - val_loss: 1.1260 - val_accuracy: 0.6352\n",
      "Epoch 54/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - ETA: 0s - loss: 1.1137 - accuracy: 0.6399\n",
      "Epoch 00054: val_loss improved from 1.12596 to 1.10542, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 695ms/step - loss: 1.1137 - accuracy: 0.6399 - val_loss: 1.1054 - val_accuracy: 0.6531\n",
      "Epoch 55/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0789 - accuracy: 0.6514\n",
      "Epoch 00055: val_loss improved from 1.10542 to 1.10397, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 695ms/step - loss: 1.0789 - accuracy: 0.6514 - val_loss: 1.1040 - val_accuracy: 0.6474\n",
      "Epoch 56/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0867 - accuracy: 0.6449\n",
      "Epoch 00056: val_loss improved from 1.10397 to 1.10173, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 723ms/step - loss: 1.0867 - accuracy: 0.6449 - val_loss: 1.1017 - val_accuracy: 0.6498\n",
      "Epoch 57/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0812 - accuracy: 0.6512\n",
      "Epoch 00057: val_loss did not improve from 1.10173\n",
      "22/22 [==============================] - 15s 681ms/step - loss: 1.0812 - accuracy: 0.6512 - val_loss: 1.1029 - val_accuracy: 0.6446\n",
      "Epoch 58/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0568 - accuracy: 0.6547\n",
      "Epoch 00058: val_loss improved from 1.10173 to 1.08325, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 716ms/step - loss: 1.0568 - accuracy: 0.6547 - val_loss: 1.0833 - val_accuracy: 0.6456\n",
      "Epoch 59/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0606 - accuracy: 0.6531\n",
      "Epoch 00059: val_loss did not improve from 1.08325\n",
      "22/22 [==============================] - 16s 704ms/step - loss: 1.0606 - accuracy: 0.6531 - val_loss: 1.1038 - val_accuracy: 0.6413\n",
      "Epoch 60/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0519 - accuracy: 0.6561\n",
      "Epoch 00060: val_loss did not improve from 1.08325\n",
      "22/22 [==============================] - 15s 695ms/step - loss: 1.0519 - accuracy: 0.6561 - val_loss: 1.0986 - val_accuracy: 0.6493\n",
      "Epoch 61/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0361 - accuracy: 0.6629\n",
      "Epoch 00061: val_loss improved from 1.08325 to 1.07398, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 683ms/step - loss: 1.0361 - accuracy: 0.6629 - val_loss: 1.0740 - val_accuracy: 0.6526\n",
      "Epoch 62/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0405 - accuracy: 0.6641\n",
      "Epoch 00062: val_loss improved from 1.07398 to 1.05202, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 690ms/step - loss: 1.0405 - accuracy: 0.6641 - val_loss: 1.0520 - val_accuracy: 0.6673\n",
      "Epoch 63/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0376 - accuracy: 0.6635\n",
      "Epoch 00063: val_loss improved from 1.05202 to 1.05042, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 687ms/step - loss: 1.0376 - accuracy: 0.6635 - val_loss: 1.0504 - val_accuracy: 0.6621\n",
      "Epoch 64/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0274 - accuracy: 0.6619\n",
      "Epoch 00064: val_loss did not improve from 1.05042\n",
      "22/22 [==============================] - 15s 695ms/step - loss: 1.0274 - accuracy: 0.6619 - val_loss: 1.0818 - val_accuracy: 0.6522\n",
      "Epoch 65/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0173 - accuracy: 0.6616\n",
      "Epoch 00065: val_loss did not improve from 1.05042\n",
      "22/22 [==============================] - 15s 685ms/step - loss: 1.0173 - accuracy: 0.6616 - val_loss: 1.0538 - val_accuracy: 0.6564\n",
      "Epoch 66/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0081 - accuracy: 0.6679\n",
      "Epoch 00066: val_loss improved from 1.05042 to 1.03833, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 688ms/step - loss: 1.0081 - accuracy: 0.6679 - val_loss: 1.0383 - val_accuracy: 0.6716\n",
      "Epoch 67/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0185 - accuracy: 0.6640\n",
      "Epoch 00067: val_loss improved from 1.03833 to 1.03248, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 706ms/step - loss: 1.0185 - accuracy: 0.6640 - val_loss: 1.0325 - val_accuracy: 0.6701\n",
      "Epoch 68/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.0037 - accuracy: 0.6704\n",
      "Epoch 00068: val_loss did not improve from 1.03248\n",
      "22/22 [==============================] - 15s 693ms/step - loss: 1.0037 - accuracy: 0.6704 - val_loss: 1.0578 - val_accuracy: 0.6597\n",
      "Epoch 69/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9956 - accuracy: 0.6730\n",
      "Epoch 00069: val_loss improved from 1.03248 to 1.01597, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 20s 941ms/step - loss: 0.9956 - accuracy: 0.6730 - val_loss: 1.0160 - val_accuracy: 0.6758\n",
      "Epoch 70/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9892 - accuracy: 0.6749\n",
      "Epoch 00070: val_loss did not improve from 1.01597\n",
      "22/22 [==============================] - 17s 767ms/step - loss: 0.9892 - accuracy: 0.6749 - val_loss: 1.0418 - val_accuracy: 0.6716\n",
      "Epoch 71/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9975 - accuracy: 0.6703\n",
      "Epoch 00071: val_loss did not improve from 1.01597\n",
      "22/22 [==============================] - 16s 746ms/step - loss: 0.9975 - accuracy: 0.6703 - val_loss: 1.0283 - val_accuracy: 0.6716\n",
      "Epoch 72/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9889 - accuracy: 0.6749\n",
      "Epoch 00072: val_loss did not improve from 1.01597\n",
      "22/22 [==============================] - 15s 677ms/step - loss: 0.9889 - accuracy: 0.6749 - val_loss: 1.0172 - val_accuracy: 0.6782\n",
      "Epoch 73/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9802 - accuracy: 0.6758\n",
      "Epoch 00073: val_loss improved from 1.01597 to 1.00869, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 663ms/step - loss: 0.9802 - accuracy: 0.6758 - val_loss: 1.0087 - val_accuracy: 0.6716\n",
      "Epoch 74/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9654 - accuracy: 0.6848\n",
      "Epoch 00074: val_loss improved from 1.00869 to 1.00229, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 661ms/step - loss: 0.9654 - accuracy: 0.6848 - val_loss: 1.0023 - val_accuracy: 0.6805\n",
      "Epoch 75/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9561 - accuracy: 0.6892\n",
      "Epoch 00075: val_loss did not improve from 1.00229\n",
      "22/22 [==============================] - 15s 662ms/step - loss: 0.9561 - accuracy: 0.6892 - val_loss: 1.0067 - val_accuracy: 0.6782\n",
      "Epoch 76/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9631 - accuracy: 0.6838\n",
      "Epoch 00076: val_loss improved from 1.00229 to 0.99716, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 663ms/step - loss: 0.9631 - accuracy: 0.6838 - val_loss: 0.9972 - val_accuracy: 0.6829\n",
      "Epoch 77/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9562 - accuracy: 0.6835\n",
      "Epoch 00077: val_loss did not improve from 0.99716\n",
      "22/22 [==============================] - 15s 664ms/step - loss: 0.9562 - accuracy: 0.6835 - val_loss: 1.0198 - val_accuracy: 0.6772\n",
      "Epoch 78/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9509 - accuracy: 0.6890\n",
      "Epoch 00078: val_loss did not improve from 0.99716\n",
      "22/22 [==============================] - 15s 658ms/step - loss: 0.9509 - accuracy: 0.6890 - val_loss: 0.9974 - val_accuracy: 0.6801\n",
      "Epoch 79/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9435 - accuracy: 0.6933\n",
      "Epoch 00079: val_loss improved from 0.99716 to 0.99633, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 670ms/step - loss: 0.9435 - accuracy: 0.6933 - val_loss: 0.9963 - val_accuracy: 0.6782\n",
      "Epoch 80/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9382 - accuracy: 0.6894\n",
      "Epoch 00080: val_loss improved from 0.99633 to 0.98292, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 666ms/step - loss: 0.9382 - accuracy: 0.6894 - val_loss: 0.9829 - val_accuracy: 0.6819\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - ETA: 0s - loss: 0.9252 - accuracy: 0.6947\n",
      "Epoch 00081: val_loss did not improve from 0.98292\n",
      "22/22 [==============================] - 15s 681ms/step - loss: 0.9252 - accuracy: 0.6947 - val_loss: 0.9849 - val_accuracy: 0.6786\n",
      "Epoch 82/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9499 - accuracy: 0.6881\n",
      "Epoch 00082: val_loss did not improve from 0.98292\n",
      "22/22 [==============================] - 15s 677ms/step - loss: 0.9499 - accuracy: 0.6881 - val_loss: 0.9839 - val_accuracy: 0.6848\n",
      "Epoch 83/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9112 - accuracy: 0.7012\n",
      "Epoch 00083: val_loss improved from 0.98292 to 0.97482, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 684ms/step - loss: 0.9112 - accuracy: 0.7012 - val_loss: 0.9748 - val_accuracy: 0.6900\n",
      "Epoch 84/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9190 - accuracy: 0.6931\n",
      "Epoch 00084: val_loss did not improve from 0.97482\n",
      "22/22 [==============================] - 15s 681ms/step - loss: 0.9190 - accuracy: 0.6931 - val_loss: 0.9931 - val_accuracy: 0.6824\n",
      "Epoch 85/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9124 - accuracy: 0.7007\n",
      "Epoch 00085: val_loss did not improve from 0.97482\n",
      "22/22 [==============================] - 15s 674ms/step - loss: 0.9124 - accuracy: 0.7007 - val_loss: 0.9752 - val_accuracy: 0.6853\n",
      "Epoch 86/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9150 - accuracy: 0.6952\n",
      "Epoch 00086: val_loss improved from 0.97482 to 0.97255, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 685ms/step - loss: 0.9150 - accuracy: 0.6952 - val_loss: 0.9726 - val_accuracy: 0.6928\n",
      "Epoch 87/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8979 - accuracy: 0.7045\n",
      "Epoch 00087: val_loss improved from 0.97255 to 0.94745, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 700ms/step - loss: 0.8979 - accuracy: 0.7045 - val_loss: 0.9475 - val_accuracy: 0.7018\n",
      "Epoch 88/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9019 - accuracy: 0.7004\n",
      "Epoch 00088: val_loss improved from 0.94745 to 0.93939, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 691ms/step - loss: 0.9019 - accuracy: 0.7004 - val_loss: 0.9394 - val_accuracy: 0.6985\n",
      "Epoch 89/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.9182 - accuracy: 0.6982\n",
      "Epoch 00089: val_loss did not improve from 0.93939\n",
      "22/22 [==============================] - 15s 684ms/step - loss: 0.9182 - accuracy: 0.6982 - val_loss: 0.9496 - val_accuracy: 0.7013\n",
      "Epoch 90/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8950 - accuracy: 0.7053\n",
      "Epoch 00090: val_loss improved from 0.93939 to 0.93904, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 689ms/step - loss: 0.8950 - accuracy: 0.7053 - val_loss: 0.9390 - val_accuracy: 0.7013\n",
      "Epoch 91/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8966 - accuracy: 0.6999\n",
      "Epoch 00091: val_loss did not improve from 0.93904\n",
      "22/22 [==============================] - 15s 670ms/step - loss: 0.8966 - accuracy: 0.6999 - val_loss: 0.9517 - val_accuracy: 0.6975\n",
      "Epoch 92/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8951 - accuracy: 0.7024\n",
      "Epoch 00092: val_loss improved from 0.93904 to 0.93582, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 681ms/step - loss: 0.8951 - accuracy: 0.7024 - val_loss: 0.9358 - val_accuracy: 0.7004\n",
      "Epoch 93/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8897 - accuracy: 0.7086\n",
      "Epoch 00093: val_loss did not improve from 0.93582\n",
      "22/22 [==============================] - 15s 682ms/step - loss: 0.8897 - accuracy: 0.7086 - val_loss: 0.9531 - val_accuracy: 0.6942\n",
      "Epoch 94/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8847 - accuracy: 0.7046\n",
      "Epoch 00094: val_loss improved from 0.93582 to 0.92418, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 684ms/step - loss: 0.8847 - accuracy: 0.7046 - val_loss: 0.9242 - val_accuracy: 0.7027\n",
      "Epoch 95/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8792 - accuracy: 0.7076\n",
      "Epoch 00095: val_loss did not improve from 0.92418\n",
      "22/22 [==============================] - 15s 688ms/step - loss: 0.8792 - accuracy: 0.7076 - val_loss: 0.9419 - val_accuracy: 0.6971\n",
      "Epoch 96/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8793 - accuracy: 0.7105\n",
      "Epoch 00096: val_loss did not improve from 0.92418\n",
      "22/22 [==============================] - 15s 683ms/step - loss: 0.8793 - accuracy: 0.7105 - val_loss: 0.9577 - val_accuracy: 0.6947\n",
      "Epoch 97/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8743 - accuracy: 0.7097\n",
      "Epoch 00097: val_loss did not improve from 0.92418\n",
      "22/22 [==============================] - 15s 673ms/step - loss: 0.8743 - accuracy: 0.7097 - val_loss: 0.9334 - val_accuracy: 0.7046\n",
      "Epoch 98/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8743 - accuracy: 0.7095\n",
      "Epoch 00098: val_loss improved from 0.92418 to 0.91708, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 683ms/step - loss: 0.8743 - accuracy: 0.7095 - val_loss: 0.9171 - val_accuracy: 0.7065\n",
      "Epoch 99/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8545 - accuracy: 0.7128\n",
      "Epoch 00099: val_loss did not improve from 0.91708\n",
      "22/22 [==============================] - 15s 673ms/step - loss: 0.8545 - accuracy: 0.7128 - val_loss: 0.9225 - val_accuracy: 0.7013\n",
      "Epoch 100/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8624 - accuracy: 0.7133\n",
      "Epoch 00100: val_loss improved from 0.91708 to 0.91358, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 17s 783ms/step - loss: 0.8624 - accuracy: 0.7133 - val_loss: 0.9136 - val_accuracy: 0.7164\n",
      "Epoch 101/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8626 - accuracy: 0.7075\n",
      "Epoch 00101: val_loss improved from 0.91358 to 0.90384, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 670ms/step - loss: 0.8626 - accuracy: 0.7075 - val_loss: 0.9038 - val_accuracy: 0.7112\n",
      "Epoch 102/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8555 - accuracy: 0.7220\n",
      "Epoch 00102: val_loss did not improve from 0.90384\n",
      "22/22 [==============================] - 14s 645ms/step - loss: 0.8555 - accuracy: 0.7220 - val_loss: 0.9052 - val_accuracy: 0.7098\n",
      "Epoch 103/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8564 - accuracy: 0.7111\n",
      "Epoch 00103: val_loss improved from 0.90384 to 0.90190, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 17s 795ms/step - loss: 0.8564 - accuracy: 0.7111 - val_loss: 0.9019 - val_accuracy: 0.7117\n",
      "Epoch 104/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8479 - accuracy: 0.7092\n",
      "Epoch 00104: val_loss did not improve from 0.90190\n",
      "22/22 [==============================] - 14s 658ms/step - loss: 0.8479 - accuracy: 0.7092 - val_loss: 0.9119 - val_accuracy: 0.7094\n",
      "Epoch 105/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8432 - accuracy: 0.7190\n",
      "Epoch 00105: val_loss did not improve from 0.90190\n",
      "22/22 [==============================] - 16s 752ms/step - loss: 0.8432 - accuracy: 0.7190 - val_loss: 0.9110 - val_accuracy: 0.7089\n",
      "Epoch 106/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8365 - accuracy: 0.7161\n",
      "Epoch 00106: val_loss did not improve from 0.90190\n",
      "22/22 [==============================] - 15s 677ms/step - loss: 0.8365 - accuracy: 0.7161 - val_loss: 0.9125 - val_accuracy: 0.7103\n",
      "Epoch 107/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8360 - accuracy: 0.7177\n",
      "Epoch 00107: val_loss improved from 0.90190 to 0.89587, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 682ms/step - loss: 0.8360 - accuracy: 0.7177 - val_loss: 0.8959 - val_accuracy: 0.7131\n",
      "Epoch 108/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8331 - accuracy: 0.7311\n",
      "Epoch 00108: val_loss improved from 0.89587 to 0.89142, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 748ms/step - loss: 0.8331 - accuracy: 0.7311 - val_loss: 0.8914 - val_accuracy: 0.7183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8340 - accuracy: 0.7194\n",
      "Epoch 00109: val_loss improved from 0.89142 to 0.88746, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 675ms/step - loss: 0.8340 - accuracy: 0.7194 - val_loss: 0.8875 - val_accuracy: 0.7160\n",
      "Epoch 110/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8145 - accuracy: 0.7227\n",
      "Epoch 00110: val_loss did not improve from 0.88746\n",
      "22/22 [==============================] - 15s 673ms/step - loss: 0.8145 - accuracy: 0.7227 - val_loss: 0.8883 - val_accuracy: 0.7174\n",
      "Epoch 111/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8204 - accuracy: 0.7264\n",
      "Epoch 00111: val_loss improved from 0.88746 to 0.87050, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 688ms/step - loss: 0.8204 - accuracy: 0.7264 - val_loss: 0.8705 - val_accuracy: 0.7245\n",
      "Epoch 112/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8134 - accuracy: 0.7276\n",
      "Epoch 00112: val_loss did not improve from 0.87050\n",
      "22/22 [==============================] - 15s 691ms/step - loss: 0.8134 - accuracy: 0.7276 - val_loss: 0.8867 - val_accuracy: 0.7174\n",
      "Epoch 113/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8172 - accuracy: 0.7240\n",
      "Epoch 00113: val_loss did not improve from 0.87050\n",
      "22/22 [==============================] - 15s 663ms/step - loss: 0.8172 - accuracy: 0.7240 - val_loss: 0.8886 - val_accuracy: 0.7155\n",
      "Epoch 114/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8236 - accuracy: 0.7298\n",
      "Epoch 00114: val_loss did not improve from 0.87050\n",
      "22/22 [==============================] - 15s 677ms/step - loss: 0.8236 - accuracy: 0.7298 - val_loss: 0.8872 - val_accuracy: 0.7193\n",
      "Epoch 115/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7957 - accuracy: 0.7346\n",
      "Epoch 00115: val_loss did not improve from 0.87050\n",
      "22/22 [==============================] - 14s 648ms/step - loss: 0.7957 - accuracy: 0.7346 - val_loss: 0.8809 - val_accuracy: 0.7193\n",
      "Epoch 116/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8167 - accuracy: 0.7237\n",
      "Epoch 00116: val_loss improved from 0.87050 to 0.86972, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 14s 650ms/step - loss: 0.8167 - accuracy: 0.7237 - val_loss: 0.8697 - val_accuracy: 0.7287\n",
      "Epoch 117/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8004 - accuracy: 0.7352\n",
      "Epoch 00117: val_loss did not improve from 0.86972\n",
      "22/22 [==============================] - 15s 686ms/step - loss: 0.8004 - accuracy: 0.7352 - val_loss: 0.8913 - val_accuracy: 0.7188\n",
      "Epoch 118/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8171 - accuracy: 0.7292\n",
      "Epoch 00118: val_loss improved from 0.86972 to 0.86574, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 14s 650ms/step - loss: 0.8171 - accuracy: 0.7292 - val_loss: 0.8657 - val_accuracy: 0.7302\n",
      "Epoch 119/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8150 - accuracy: 0.7281\n",
      "Epoch 00119: val_loss did not improve from 0.86574\n",
      "22/22 [==============================] - 14s 657ms/step - loss: 0.8150 - accuracy: 0.7281 - val_loss: 0.8838 - val_accuracy: 0.7193\n",
      "Epoch 120/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.8014 - accuracy: 0.7287\n",
      "Epoch 00120: val_loss did not improve from 0.86574\n",
      "22/22 [==============================] - 15s 701ms/step - loss: 0.8014 - accuracy: 0.7287 - val_loss: 0.8723 - val_accuracy: 0.7245\n",
      "Epoch 121/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7962 - accuracy: 0.7355\n",
      "Epoch 00121: val_loss did not improve from 0.86574\n",
      "22/22 [==============================] - 15s 667ms/step - loss: 0.7962 - accuracy: 0.7355 - val_loss: 0.8720 - val_accuracy: 0.7216\n",
      "Epoch 122/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7838 - accuracy: 0.7365\n",
      "Epoch 00122: val_loss did not improve from 0.86574\n",
      "22/22 [==============================] - 16s 728ms/step - loss: 0.7838 - accuracy: 0.7365 - val_loss: 0.8668 - val_accuracy: 0.7207\n",
      "Epoch 123/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7852 - accuracy: 0.7358\n",
      "Epoch 00123: val_loss did not improve from 0.86574\n",
      "22/22 [==============================] - 14s 654ms/step - loss: 0.7852 - accuracy: 0.7358 - val_loss: 0.8659 - val_accuracy: 0.7278\n",
      "Epoch 124/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7934 - accuracy: 0.7382\n",
      "Epoch 00124: val_loss improved from 0.86574 to 0.85717, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 14s 653ms/step - loss: 0.7934 - accuracy: 0.7382 - val_loss: 0.8572 - val_accuracy: 0.7287\n",
      "Epoch 125/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7814 - accuracy: 0.7369\n",
      "Epoch 00125: val_loss did not improve from 0.85717\n",
      "22/22 [==============================] - 15s 684ms/step - loss: 0.7814 - accuracy: 0.7369 - val_loss: 0.8704 - val_accuracy: 0.7245\n",
      "Epoch 126/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7927 - accuracy: 0.7355\n",
      "Epoch 00126: val_loss improved from 0.85717 to 0.85402, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 688ms/step - loss: 0.7927 - accuracy: 0.7355 - val_loss: 0.8540 - val_accuracy: 0.7297\n",
      "Epoch 127/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7797 - accuracy: 0.7369\n",
      "Epoch 00127: val_loss did not improve from 0.85402\n",
      "22/22 [==============================] - 15s 667ms/step - loss: 0.7797 - accuracy: 0.7369 - val_loss: 0.8688 - val_accuracy: 0.7287\n",
      "Epoch 128/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7873 - accuracy: 0.7369\n",
      "Epoch 00128: val_loss did not improve from 0.85402\n",
      "22/22 [==============================] - 15s 658ms/step - loss: 0.7873 - accuracy: 0.7369 - val_loss: 0.8548 - val_accuracy: 0.7292\n",
      "Epoch 129/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7930 - accuracy: 0.7369\n",
      "Epoch 00129: val_loss improved from 0.85402 to 0.84802, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 660ms/step - loss: 0.7930 - accuracy: 0.7369 - val_loss: 0.8480 - val_accuracy: 0.7353\n",
      "Epoch 130/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7783 - accuracy: 0.7357\n",
      "Epoch 00130: val_loss did not improve from 0.84802\n",
      "22/22 [==============================] - 14s 657ms/step - loss: 0.7783 - accuracy: 0.7357 - val_loss: 0.8599 - val_accuracy: 0.7292\n",
      "Epoch 131/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7724 - accuracy: 0.7415\n",
      "Epoch 00131: val_loss improved from 0.84802 to 0.83215, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 661ms/step - loss: 0.7724 - accuracy: 0.7415 - val_loss: 0.8322 - val_accuracy: 0.7387\n",
      "Epoch 132/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7697 - accuracy: 0.7393\n",
      "Epoch 00132: val_loss improved from 0.83215 to 0.83122, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 663ms/step - loss: 0.7697 - accuracy: 0.7393 - val_loss: 0.8312 - val_accuracy: 0.7434\n",
      "Epoch 133/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7703 - accuracy: 0.7376\n",
      "Epoch 00133: val_loss did not improve from 0.83122\n",
      "22/22 [==============================] - 15s 659ms/step - loss: 0.7703 - accuracy: 0.7376 - val_loss: 0.8480 - val_accuracy: 0.7325\n",
      "Epoch 134/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7720 - accuracy: 0.7371\n",
      "Epoch 00134: val_loss did not improve from 0.83122\n",
      "22/22 [==============================] - 15s 662ms/step - loss: 0.7720 - accuracy: 0.7371 - val_loss: 0.8467 - val_accuracy: 0.7316\n",
      "Epoch 135/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7586 - accuracy: 0.7434\n",
      "Epoch 00135: val_loss did not improve from 0.83122\n",
      "22/22 [==============================] - 15s 669ms/step - loss: 0.7586 - accuracy: 0.7434 - val_loss: 0.8439 - val_accuracy: 0.7325\n",
      "Epoch 136/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7602 - accuracy: 0.7451\n",
      "Epoch 00136: val_loss did not improve from 0.83122\n",
      "22/22 [==============================] - 15s 660ms/step - loss: 0.7602 - accuracy: 0.7451 - val_loss: 0.8326 - val_accuracy: 0.7405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7671 - accuracy: 0.7431\n",
      "Epoch 00137: val_loss improved from 0.83122 to 0.83087, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 675ms/step - loss: 0.7671 - accuracy: 0.7431 - val_loss: 0.8309 - val_accuracy: 0.7410\n",
      "Epoch 138/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7519 - accuracy: 0.7429\n",
      "Epoch 00138: val_loss did not improve from 0.83087\n",
      "22/22 [==============================] - 15s 672ms/step - loss: 0.7519 - accuracy: 0.7429 - val_loss: 0.8417 - val_accuracy: 0.7372\n",
      "Epoch 139/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7589 - accuracy: 0.7398\n",
      "Epoch 00139: val_loss did not improve from 0.83087\n",
      "22/22 [==============================] - 15s 681ms/step - loss: 0.7589 - accuracy: 0.7398 - val_loss: 0.8382 - val_accuracy: 0.7391\n",
      "Epoch 140/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7328 - accuracy: 0.7481\n",
      "Epoch 00140: val_loss improved from 0.83087 to 0.82226, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 680ms/step - loss: 0.7328 - accuracy: 0.7481 - val_loss: 0.8223 - val_accuracy: 0.7415\n",
      "Epoch 141/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7335 - accuracy: 0.7500\n",
      "Epoch 00141: val_loss did not improve from 0.82226\n",
      "22/22 [==============================] - 15s 685ms/step - loss: 0.7335 - accuracy: 0.7500 - val_loss: 0.8273 - val_accuracy: 0.7434\n",
      "Epoch 142/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7496 - accuracy: 0.7454\n",
      "Epoch 00142: val_loss did not improve from 0.82226\n",
      "22/22 [==============================] - 15s 676ms/step - loss: 0.7496 - accuracy: 0.7454 - val_loss: 0.8338 - val_accuracy: 0.7353\n",
      "Epoch 143/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7472 - accuracy: 0.7476\n",
      "Epoch 00143: val_loss did not improve from 0.82226\n",
      "22/22 [==============================] - 15s 686ms/step - loss: 0.7472 - accuracy: 0.7476 - val_loss: 0.8394 - val_accuracy: 0.7320\n",
      "Epoch 144/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7571 - accuracy: 0.7498\n",
      "Epoch 00144: val_loss did not improve from 0.82226\n",
      "22/22 [==============================] - 15s 679ms/step - loss: 0.7571 - accuracy: 0.7498 - val_loss: 0.8331 - val_accuracy: 0.7453\n",
      "Epoch 145/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7383 - accuracy: 0.7494\n",
      "Epoch 00145: val_loss did not improve from 0.82226\n",
      "22/22 [==============================] - 18s 811ms/step - loss: 0.7383 - accuracy: 0.7494 - val_loss: 0.8304 - val_accuracy: 0.7377\n",
      "Epoch 146/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7336 - accuracy: 0.7525\n",
      "Epoch 00146: val_loss improved from 0.82226 to 0.82134, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 666ms/step - loss: 0.7336 - accuracy: 0.7525 - val_loss: 0.8213 - val_accuracy: 0.7415\n",
      "Epoch 147/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7387 - accuracy: 0.7500\n",
      "Epoch 00147: val_loss did not improve from 0.82134\n",
      "22/22 [==============================] - 14s 648ms/step - loss: 0.7387 - accuracy: 0.7500 - val_loss: 0.8292 - val_accuracy: 0.7377\n",
      "Epoch 148/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7361 - accuracy: 0.7476\n",
      "Epoch 00148: val_loss did not improve from 0.82134\n",
      "22/22 [==============================] - 16s 711ms/step - loss: 0.7361 - accuracy: 0.7476 - val_loss: 0.8316 - val_accuracy: 0.7410\n",
      "Epoch 149/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7301 - accuracy: 0.7495\n",
      "Epoch 00149: val_loss improved from 0.82134 to 0.81459, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 672ms/step - loss: 0.7301 - accuracy: 0.7495 - val_loss: 0.8146 - val_accuracy: 0.7448\n",
      "Epoch 150/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7225 - accuracy: 0.7520\n",
      "Epoch 00150: val_loss did not improve from 0.81459\n",
      "22/22 [==============================] - 14s 655ms/step - loss: 0.7225 - accuracy: 0.7520 - val_loss: 0.8212 - val_accuracy: 0.7387\n",
      "Epoch 151/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7251 - accuracy: 0.7547\n",
      "Epoch 00151: val_loss improved from 0.81459 to 0.80411, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 667ms/step - loss: 0.7251 - accuracy: 0.7547 - val_loss: 0.8041 - val_accuracy: 0.7415\n",
      "Epoch 152/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7124 - accuracy: 0.7582\n",
      "Epoch 00152: val_loss did not improve from 0.80411\n",
      "22/22 [==============================] - 14s 653ms/step - loss: 0.7124 - accuracy: 0.7582 - val_loss: 0.8258 - val_accuracy: 0.7410\n",
      "Epoch 153/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7311 - accuracy: 0.7528\n",
      "Epoch 00153: val_loss did not improve from 0.80411\n",
      "22/22 [==============================] - 15s 661ms/step - loss: 0.7311 - accuracy: 0.7528 - val_loss: 0.8292 - val_accuracy: 0.7420\n",
      "Epoch 154/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7420 - accuracy: 0.7472\n",
      "Epoch 00154: val_loss did not improve from 0.80411\n",
      "22/22 [==============================] - 15s 657ms/step - loss: 0.7420 - accuracy: 0.7472 - val_loss: 0.8225 - val_accuracy: 0.7396\n",
      "Epoch 155/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7123 - accuracy: 0.7552\n",
      "Epoch 00155: val_loss did not improve from 0.80411\n",
      "22/22 [==============================] - 15s 660ms/step - loss: 0.7123 - accuracy: 0.7552 - val_loss: 0.8122 - val_accuracy: 0.7486\n",
      "Epoch 156/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7132 - accuracy: 0.7609\n",
      "Epoch 00156: val_loss did not improve from 0.80411\n",
      "22/22 [==============================] - 15s 660ms/step - loss: 0.7132 - accuracy: 0.7609 - val_loss: 0.8215 - val_accuracy: 0.7396\n",
      "Epoch 157/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7093 - accuracy: 0.7610\n",
      "Epoch 00157: val_loss did not improve from 0.80411\n",
      "22/22 [==============================] - 15s 662ms/step - loss: 0.7093 - accuracy: 0.7610 - val_loss: 0.8043 - val_accuracy: 0.7448\n",
      "Epoch 158/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7218 - accuracy: 0.7588\n",
      "Epoch 00158: val_loss did not improve from 0.80411\n",
      "22/22 [==============================] - 14s 656ms/step - loss: 0.7218 - accuracy: 0.7588 - val_loss: 0.8189 - val_accuracy: 0.7410\n",
      "Epoch 159/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7191 - accuracy: 0.7571\n",
      "Epoch 00159: val_loss did not improve from 0.80411\n",
      "22/22 [==============================] - 15s 668ms/step - loss: 0.7191 - accuracy: 0.7571 - val_loss: 0.8228 - val_accuracy: 0.7424\n",
      "Epoch 160/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7035 - accuracy: 0.7615\n",
      "Epoch 00160: val_loss did not improve from 0.80411\n",
      "22/22 [==============================] - 15s 658ms/step - loss: 0.7035 - accuracy: 0.7615 - val_loss: 0.8042 - val_accuracy: 0.7462\n",
      "Epoch 161/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6954 - accuracy: 0.7665\n",
      "Epoch 00161: val_loss did not improve from 0.80411\n",
      "22/22 [==============================] - 16s 713ms/step - loss: 0.6954 - accuracy: 0.7665 - val_loss: 0.8046 - val_accuracy: 0.7424\n",
      "Epoch 162/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7019 - accuracy: 0.7628\n",
      "Epoch 00162: val_loss improved from 0.80411 to 0.79531, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 664ms/step - loss: 0.7019 - accuracy: 0.7628 - val_loss: 0.7953 - val_accuracy: 0.7528\n",
      "Epoch 163/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7111 - accuracy: 0.7547\n",
      "Epoch 00163: val_loss did not improve from 0.79531\n",
      "22/22 [==============================] - 15s 666ms/step - loss: 0.7111 - accuracy: 0.7547 - val_loss: 0.8068 - val_accuracy: 0.7457\n",
      "Epoch 164/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7039 - accuracy: 0.7610\n",
      "Epoch 00164: val_loss did not improve from 0.79531\n",
      "22/22 [==============================] - 15s 661ms/step - loss: 0.7039 - accuracy: 0.7610 - val_loss: 0.7976 - val_accuracy: 0.7495\n",
      "Epoch 165/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6875 - accuracy: 0.7672\n",
      "Epoch 00165: val_loss did not improve from 0.79531\n",
      "22/22 [==============================] - 15s 664ms/step - loss: 0.6875 - accuracy: 0.7672 - val_loss: 0.8096 - val_accuracy: 0.7491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7086 - accuracy: 0.7583\n",
      "Epoch 00166: val_loss did not improve from 0.79531\n",
      "22/22 [==============================] - 15s 680ms/step - loss: 0.7086 - accuracy: 0.7583 - val_loss: 0.8113 - val_accuracy: 0.7462\n",
      "Epoch 167/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.7050 - accuracy: 0.7631\n",
      "Epoch 00167: val_loss did not improve from 0.79531\n",
      "22/22 [==============================] - 15s 683ms/step - loss: 0.7050 - accuracy: 0.7631 - val_loss: 0.8035 - val_accuracy: 0.7448\n",
      "Epoch 168/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.7650\n",
      "Epoch 00168: val_loss did not improve from 0.79531\n",
      "22/22 [==============================] - 15s 679ms/step - loss: 0.6888 - accuracy: 0.7650 - val_loss: 0.8072 - val_accuracy: 0.7462\n",
      "Epoch 169/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6880 - accuracy: 0.7706\n",
      "Epoch 00169: val_loss improved from 0.79531 to 0.79283, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 679ms/step - loss: 0.6880 - accuracy: 0.7706 - val_loss: 0.7928 - val_accuracy: 0.7528\n",
      "Epoch 170/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.7659\n",
      "Epoch 00170: val_loss did not improve from 0.79283\n",
      "22/22 [==============================] - 15s 666ms/step - loss: 0.6892 - accuracy: 0.7659 - val_loss: 0.8023 - val_accuracy: 0.7500\n",
      "Epoch 171/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6731 - accuracy: 0.7691\n",
      "Epoch 00171: val_loss did not improve from 0.79283\n",
      "22/22 [==============================] - 15s 687ms/step - loss: 0.6731 - accuracy: 0.7691 - val_loss: 0.8076 - val_accuracy: 0.7424\n",
      "Epoch 172/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6840 - accuracy: 0.7672\n",
      "Epoch 00172: val_loss did not improve from 0.79283\n",
      "22/22 [==============================] - 15s 676ms/step - loss: 0.6840 - accuracy: 0.7672 - val_loss: 0.8150 - val_accuracy: 0.7448\n",
      "Epoch 173/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.7672\n",
      "Epoch 00173: val_loss did not improve from 0.79283\n",
      "22/22 [==============================] - 15s 678ms/step - loss: 0.6877 - accuracy: 0.7672 - val_loss: 0.8140 - val_accuracy: 0.7429\n",
      "Epoch 174/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.7651\n",
      "Epoch 00174: val_loss did not improve from 0.79283\n",
      "22/22 [==============================] - 15s 688ms/step - loss: 0.6924 - accuracy: 0.7651 - val_loss: 0.7971 - val_accuracy: 0.7524\n",
      "Epoch 175/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6803 - accuracy: 0.7656\n",
      "Epoch 00175: val_loss improved from 0.79283 to 0.78699, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 689ms/step - loss: 0.6803 - accuracy: 0.7656 - val_loss: 0.7870 - val_accuracy: 0.7528\n",
      "Epoch 176/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6718 - accuracy: 0.7725\n",
      "Epoch 00176: val_loss did not improve from 0.78699\n",
      "22/22 [==============================] - 15s 684ms/step - loss: 0.6718 - accuracy: 0.7725 - val_loss: 0.7928 - val_accuracy: 0.7543\n",
      "Epoch 177/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6839 - accuracy: 0.7670\n",
      "Epoch 00177: val_loss did not improve from 0.78699\n",
      "22/22 [==============================] - 15s 682ms/step - loss: 0.6839 - accuracy: 0.7670 - val_loss: 0.7886 - val_accuracy: 0.7566\n",
      "Epoch 178/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6759 - accuracy: 0.7697\n",
      "Epoch 00178: val_loss improved from 0.78699 to 0.78146, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 681ms/step - loss: 0.6759 - accuracy: 0.7697 - val_loss: 0.7815 - val_accuracy: 0.7500\n",
      "Epoch 179/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6851 - accuracy: 0.7710\n",
      "Epoch 00179: val_loss improved from 0.78146 to 0.78030, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 695ms/step - loss: 0.6851 - accuracy: 0.7710 - val_loss: 0.7803 - val_accuracy: 0.7557\n",
      "Epoch 180/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6639 - accuracy: 0.7698\n",
      "Epoch 00180: val_loss did not improve from 0.78030\n",
      "22/22 [==============================] - 15s 694ms/step - loss: 0.6639 - accuracy: 0.7698 - val_loss: 0.7866 - val_accuracy: 0.7519\n",
      "Epoch 181/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6810 - accuracy: 0.7670\n",
      "Epoch 00181: val_loss did not improve from 0.78030\n",
      "22/22 [==============================] - 15s 693ms/step - loss: 0.6810 - accuracy: 0.7670 - val_loss: 0.8031 - val_accuracy: 0.7481\n",
      "Epoch 182/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6633 - accuracy: 0.7719\n",
      "Epoch 00182: val_loss did not improve from 0.78030\n",
      "22/22 [==============================] - 15s 688ms/step - loss: 0.6633 - accuracy: 0.7719 - val_loss: 0.7816 - val_accuracy: 0.7524\n",
      "Epoch 183/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6702 - accuracy: 0.7719\n",
      "Epoch 00183: val_loss did not improve from 0.78030\n",
      "22/22 [==============================] - 15s 702ms/step - loss: 0.6702 - accuracy: 0.7719 - val_loss: 0.7823 - val_accuracy: 0.7524\n",
      "Epoch 184/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6793 - accuracy: 0.7658\n",
      "Epoch 00184: val_loss did not improve from 0.78030\n",
      "22/22 [==============================] - 19s 866ms/step - loss: 0.6793 - accuracy: 0.7658 - val_loss: 0.7814 - val_accuracy: 0.7509\n",
      "Epoch 185/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6808 - accuracy: 0.7694\n",
      "Epoch 00185: val_loss did not improve from 0.78030\n",
      "22/22 [==============================] - 15s 701ms/step - loss: 0.6808 - accuracy: 0.7694 - val_loss: 0.7904 - val_accuracy: 0.7476\n",
      "Epoch 186/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6692 - accuracy: 0.7738\n",
      "Epoch 00186: val_loss improved from 0.78030 to 0.77128, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 695ms/step - loss: 0.6692 - accuracy: 0.7738 - val_loss: 0.7713 - val_accuracy: 0.7561\n",
      "Epoch 187/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6577 - accuracy: 0.7785\n",
      "Epoch 00187: val_loss did not improve from 0.77128\n",
      "22/22 [==============================] - 18s 845ms/step - loss: 0.6577 - accuracy: 0.7785 - val_loss: 0.7928 - val_accuracy: 0.7495\n",
      "Epoch 188/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6556 - accuracy: 0.7763\n",
      "Epoch 00188: val_loss did not improve from 0.77128\n",
      "22/22 [==============================] - 18s 791ms/step - loss: 0.6556 - accuracy: 0.7763 - val_loss: 0.7846 - val_accuracy: 0.7495\n",
      "Epoch 189/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6684 - accuracy: 0.7744\n",
      "Epoch 00189: val_loss did not improve from 0.77128\n",
      "22/22 [==============================] - 15s 661ms/step - loss: 0.6684 - accuracy: 0.7744 - val_loss: 0.7866 - val_accuracy: 0.7514\n",
      "Epoch 190/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.7697\n",
      "Epoch 00190: val_loss improved from 0.77128 to 0.76722, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 664ms/step - loss: 0.6559 - accuracy: 0.7697 - val_loss: 0.7672 - val_accuracy: 0.7571\n",
      "Epoch 191/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6583 - accuracy: 0.7761\n",
      "Epoch 00191: val_loss improved from 0.76722 to 0.76708, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 675ms/step - loss: 0.6583 - accuracy: 0.7761 - val_loss: 0.7671 - val_accuracy: 0.7580\n",
      "Epoch 192/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.7760\n",
      "Epoch 00192: val_loss did not improve from 0.76708\n",
      "22/22 [==============================] - 15s 664ms/step - loss: 0.6561 - accuracy: 0.7760 - val_loss: 0.8096 - val_accuracy: 0.7472\n",
      "Epoch 193/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6484 - accuracy: 0.7760\n",
      "Epoch 00193: val_loss did not improve from 0.76708\n",
      "22/22 [==============================] - 15s 667ms/step - loss: 0.6484 - accuracy: 0.7760 - val_loss: 0.7714 - val_accuracy: 0.7618\n",
      "Epoch 194/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - ETA: 0s - loss: 0.6567 - accuracy: 0.7744\n",
      "Epoch 00194: val_loss improved from 0.76708 to 0.76277, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 696ms/step - loss: 0.6567 - accuracy: 0.7744 - val_loss: 0.7628 - val_accuracy: 0.7595\n",
      "Epoch 195/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6513 - accuracy: 0.7710\n",
      "Epoch 00195: val_loss did not improve from 0.76277\n",
      "22/22 [==============================] - 15s 700ms/step - loss: 0.6513 - accuracy: 0.7710 - val_loss: 0.7867 - val_accuracy: 0.7552\n",
      "Epoch 196/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.7791\n",
      "Epoch 00196: val_loss did not improve from 0.76277\n",
      "22/22 [==============================] - 15s 697ms/step - loss: 0.6430 - accuracy: 0.7791 - val_loss: 0.7748 - val_accuracy: 0.7590\n",
      "Epoch 197/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6480 - accuracy: 0.7776\n",
      "Epoch 00197: val_loss did not improve from 0.76277\n",
      "22/22 [==============================] - 15s 700ms/step - loss: 0.6480 - accuracy: 0.7776 - val_loss: 0.7741 - val_accuracy: 0.7618\n",
      "Epoch 198/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6548 - accuracy: 0.7736\n",
      "Epoch 00198: val_loss did not improve from 0.76277\n",
      "22/22 [==============================] - 16s 729ms/step - loss: 0.6548 - accuracy: 0.7736 - val_loss: 0.7667 - val_accuracy: 0.7590\n",
      "Epoch 199/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6570 - accuracy: 0.7785\n",
      "Epoch 00199: val_loss did not improve from 0.76277\n",
      "22/22 [==============================] - 17s 751ms/step - loss: 0.6570 - accuracy: 0.7785 - val_loss: 0.7755 - val_accuracy: 0.7491\n",
      "Epoch 200/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6456 - accuracy: 0.7876\n",
      "Epoch 00200: val_loss improved from 0.76277 to 0.76177, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 668ms/step - loss: 0.6456 - accuracy: 0.7876 - val_loss: 0.7618 - val_accuracy: 0.7580\n",
      "Epoch 201/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6505 - accuracy: 0.7784\n",
      "Epoch 00201: val_loss did not improve from 0.76177\n",
      "22/22 [==============================] - 15s 670ms/step - loss: 0.6505 - accuracy: 0.7784 - val_loss: 0.7690 - val_accuracy: 0.7566\n",
      "Epoch 202/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6535 - accuracy: 0.7769\n",
      "Epoch 00202: val_loss did not improve from 0.76177\n",
      "22/22 [==============================] - 15s 670ms/step - loss: 0.6535 - accuracy: 0.7769 - val_loss: 0.7620 - val_accuracy: 0.7642\n",
      "Epoch 203/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6419 - accuracy: 0.7774\n",
      "Epoch 00203: val_loss did not improve from 0.76177\n",
      "22/22 [==============================] - 15s 672ms/step - loss: 0.6419 - accuracy: 0.7774 - val_loss: 0.7839 - val_accuracy: 0.7552\n",
      "Epoch 204/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6351 - accuracy: 0.7788\n",
      "Epoch 00204: val_loss did not improve from 0.76177\n",
      "22/22 [==============================] - 15s 666ms/step - loss: 0.6351 - accuracy: 0.7788 - val_loss: 0.7791 - val_accuracy: 0.7632\n",
      "Epoch 205/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6563 - accuracy: 0.7774\n",
      "Epoch 00205: val_loss improved from 0.76177 to 0.76104, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 683ms/step - loss: 0.6563 - accuracy: 0.7774 - val_loss: 0.7610 - val_accuracy: 0.7590\n",
      "Epoch 206/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6301 - accuracy: 0.7867\n",
      "Epoch 00206: val_loss did not improve from 0.76104\n",
      "22/22 [==============================] - 15s 699ms/step - loss: 0.6301 - accuracy: 0.7867 - val_loss: 0.7689 - val_accuracy: 0.7580\n",
      "Epoch 207/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6393 - accuracy: 0.7836\n",
      "Epoch 00207: val_loss did not improve from 0.76104\n",
      "22/22 [==============================] - 19s 860ms/step - loss: 0.6393 - accuracy: 0.7836 - val_loss: 0.7623 - val_accuracy: 0.7623\n",
      "Epoch 208/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6362 - accuracy: 0.7796\n",
      "Epoch 00208: val_loss did not improve from 0.76104\n",
      "22/22 [==============================] - 16s 707ms/step - loss: 0.6362 - accuracy: 0.7796 - val_loss: 0.7748 - val_accuracy: 0.7566\n",
      "Epoch 209/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6409 - accuracy: 0.7763\n",
      "Epoch 00209: val_loss did not improve from 0.76104\n",
      "22/22 [==============================] - 15s 679ms/step - loss: 0.6409 - accuracy: 0.7763 - val_loss: 0.7665 - val_accuracy: 0.7580\n",
      "Epoch 210/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6335 - accuracy: 0.7859\n",
      "Epoch 00210: val_loss did not improve from 0.76104\n",
      "22/22 [==============================] - 15s 684ms/step - loss: 0.6335 - accuracy: 0.7859 - val_loss: 0.7753 - val_accuracy: 0.7604\n",
      "Epoch 211/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6377 - accuracy: 0.7765\n",
      "Epoch 00211: val_loss improved from 0.76104 to 0.75578, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 675ms/step - loss: 0.6377 - accuracy: 0.7765 - val_loss: 0.7558 - val_accuracy: 0.7623\n",
      "Epoch 212/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6290 - accuracy: 0.7791\n",
      "Epoch 00212: val_loss improved from 0.75578 to 0.74984, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 680ms/step - loss: 0.6290 - accuracy: 0.7791 - val_loss: 0.7498 - val_accuracy: 0.7599\n",
      "Epoch 213/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6453 - accuracy: 0.7796\n",
      "Epoch 00213: val_loss did not improve from 0.74984\n",
      "22/22 [==============================] - 15s 677ms/step - loss: 0.6453 - accuracy: 0.7796 - val_loss: 0.7702 - val_accuracy: 0.7552\n",
      "Epoch 214/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6393 - accuracy: 0.7801\n",
      "Epoch 00214: val_loss did not improve from 0.74984\n",
      "22/22 [==============================] - 15s 684ms/step - loss: 0.6393 - accuracy: 0.7801 - val_loss: 0.7545 - val_accuracy: 0.7642\n",
      "Epoch 215/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6356 - accuracy: 0.7839\n",
      "Epoch 00215: val_loss did not improve from 0.74984\n",
      "22/22 [==============================] - 16s 714ms/step - loss: 0.6356 - accuracy: 0.7839 - val_loss: 0.7675 - val_accuracy: 0.7623\n",
      "Epoch 216/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6263 - accuracy: 0.7828\n",
      "Epoch 00216: val_loss did not improve from 0.74984\n",
      "22/22 [==============================] - 15s 689ms/step - loss: 0.6263 - accuracy: 0.7828 - val_loss: 0.7677 - val_accuracy: 0.7599\n",
      "Epoch 217/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6311 - accuracy: 0.7807\n",
      "Epoch 00217: val_loss improved from 0.74984 to 0.74391, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 729ms/step - loss: 0.6311 - accuracy: 0.7807 - val_loss: 0.7439 - val_accuracy: 0.7698\n",
      "Epoch 218/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6233 - accuracy: 0.7876\n",
      "Epoch 00218: val_loss did not improve from 0.74391\n",
      "22/22 [==============================] - 15s 694ms/step - loss: 0.6233 - accuracy: 0.7876 - val_loss: 0.7518 - val_accuracy: 0.7628\n",
      "Epoch 219/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6232 - accuracy: 0.7859\n",
      "Epoch 00219: val_loss did not improve from 0.74391\n",
      "22/22 [==============================] - 15s 670ms/step - loss: 0.6232 - accuracy: 0.7859 - val_loss: 0.7518 - val_accuracy: 0.7623\n",
      "Epoch 220/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6224 - accuracy: 0.7847\n",
      "Epoch 00220: val_loss did not improve from 0.74391\n",
      "22/22 [==============================] - 16s 725ms/step - loss: 0.6224 - accuracy: 0.7847 - val_loss: 0.7528 - val_accuracy: 0.7675\n",
      "Epoch 221/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6284 - accuracy: 0.7910\n",
      "Epoch 00221: val_loss did not improve from 0.74391\n",
      "22/22 [==============================] - 17s 780ms/step - loss: 0.6284 - accuracy: 0.7910 - val_loss: 0.7602 - val_accuracy: 0.7623\n",
      "Epoch 222/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6292 - accuracy: 0.7832\n",
      "Epoch 00222: val_loss did not improve from 0.74391\n",
      "22/22 [==============================] - 17s 743ms/step - loss: 0.6292 - accuracy: 0.7832 - val_loss: 0.7543 - val_accuracy: 0.7661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.7845\n",
      "Epoch 00223: val_loss did not improve from 0.74391\n",
      "22/22 [==============================] - 16s 713ms/step - loss: 0.6135 - accuracy: 0.7845 - val_loss: 0.7593 - val_accuracy: 0.7590\n",
      "Epoch 224/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6184 - accuracy: 0.7840\n",
      "Epoch 00224: val_loss improved from 0.74391 to 0.74144, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 712ms/step - loss: 0.6184 - accuracy: 0.7840 - val_loss: 0.7414 - val_accuracy: 0.7713\n",
      "Epoch 225/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6170 - accuracy: 0.7894\n",
      "Epoch 00225: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 16s 711ms/step - loss: 0.6170 - accuracy: 0.7894 - val_loss: 0.7478 - val_accuracy: 0.7670\n",
      "Epoch 226/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6155 - accuracy: 0.7895\n",
      "Epoch 00226: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 16s 707ms/step - loss: 0.6155 - accuracy: 0.7895 - val_loss: 0.7612 - val_accuracy: 0.7656\n",
      "Epoch 227/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6210 - accuracy: 0.7878\n",
      "Epoch 00227: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 20s 940ms/step - loss: 0.6210 - accuracy: 0.7878 - val_loss: 0.7676 - val_accuracy: 0.7651\n",
      "Epoch 228/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6271 - accuracy: 0.7842\n",
      "Epoch 00228: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 17s 762ms/step - loss: 0.6271 - accuracy: 0.7842 - val_loss: 0.7463 - val_accuracy: 0.7698\n",
      "Epoch 229/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.7902\n",
      "Epoch 00229: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 15s 705ms/step - loss: 0.6074 - accuracy: 0.7902 - val_loss: 0.7691 - val_accuracy: 0.7604\n",
      "Epoch 230/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6210 - accuracy: 0.7876\n",
      "Epoch 00230: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 15s 662ms/step - loss: 0.6210 - accuracy: 0.7876 - val_loss: 0.7495 - val_accuracy: 0.7651\n",
      "Epoch 231/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7914\n",
      "Epoch 00231: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 15s 700ms/step - loss: 0.6000 - accuracy: 0.7914 - val_loss: 0.7521 - val_accuracy: 0.7675\n",
      "Epoch 232/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6140 - accuracy: 0.7878\n",
      "Epoch 00232: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 15s 684ms/step - loss: 0.6140 - accuracy: 0.7878 - val_loss: 0.7488 - val_accuracy: 0.7642\n",
      "Epoch 233/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.7908\n",
      "Epoch 00233: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 15s 670ms/step - loss: 0.6153 - accuracy: 0.7908 - val_loss: 0.7606 - val_accuracy: 0.7623\n",
      "Epoch 234/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.7946\n",
      "Epoch 00234: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 18s 797ms/step - loss: 0.6025 - accuracy: 0.7946 - val_loss: 0.7510 - val_accuracy: 0.7580\n",
      "Epoch 235/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6005 - accuracy: 0.7895\n",
      "Epoch 00235: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 15s 683ms/step - loss: 0.6005 - accuracy: 0.7895 - val_loss: 0.7517 - val_accuracy: 0.7632\n",
      "Epoch 236/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5944 - accuracy: 0.7954\n",
      "Epoch 00236: val_loss did not improve from 0.74144\n",
      "22/22 [==============================] - 19s 877ms/step - loss: 0.5944 - accuracy: 0.7954 - val_loss: 0.7460 - val_accuracy: 0.7703\n",
      "Epoch 237/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6115 - accuracy: 0.7922\n",
      "Epoch 00237: val_loss improved from 0.74144 to 0.73841, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 688ms/step - loss: 0.6115 - accuracy: 0.7922 - val_loss: 0.7384 - val_accuracy: 0.7727\n",
      "Epoch 238/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6005 - accuracy: 0.7910\n",
      "Epoch 00238: val_loss did not improve from 0.73841\n",
      "22/22 [==============================] - 17s 784ms/step - loss: 0.6005 - accuracy: 0.7910 - val_loss: 0.7392 - val_accuracy: 0.7708\n",
      "Epoch 239/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6082 - accuracy: 0.7891\n",
      "Epoch 00239: val_loss did not improve from 0.73841\n",
      "22/22 [==============================] - 19s 855ms/step - loss: 0.6082 - accuracy: 0.7891 - val_loss: 0.7385 - val_accuracy: 0.7670\n",
      "Epoch 240/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.7899\n",
      "Epoch 00240: val_loss did not improve from 0.73841\n",
      "22/22 [==============================] - 18s 829ms/step - loss: 0.5990 - accuracy: 0.7899 - val_loss: 0.7479 - val_accuracy: 0.7675\n",
      "Epoch 241/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6047 - accuracy: 0.7894\n",
      "Epoch 00241: val_loss did not improve from 0.73841\n",
      "22/22 [==============================] - 19s 868ms/step - loss: 0.6047 - accuracy: 0.7894 - val_loss: 0.7390 - val_accuracy: 0.7698\n",
      "Epoch 242/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5886 - accuracy: 0.8040\n",
      "Epoch 00242: val_loss did not improve from 0.73841\n",
      "22/22 [==============================] - 19s 841ms/step - loss: 0.5886 - accuracy: 0.8040 - val_loss: 0.7467 - val_accuracy: 0.7713\n",
      "Epoch 243/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6004 - accuracy: 0.7971\n",
      "Epoch 00243: val_loss improved from 0.73841 to 0.73683, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 735ms/step - loss: 0.6004 - accuracy: 0.7971 - val_loss: 0.7368 - val_accuracy: 0.7703\n",
      "Epoch 244/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5831 - accuracy: 0.7963\n",
      "Epoch 00244: val_loss did not improve from 0.73683\n",
      "22/22 [==============================] - 20s 903ms/step - loss: 0.5831 - accuracy: 0.7963 - val_loss: 0.7498 - val_accuracy: 0.7661\n",
      "Epoch 245/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.7913\n",
      "Epoch 00245: val_loss did not improve from 0.73683\n",
      "22/22 [==============================] - 20s 896ms/step - loss: 0.6028 - accuracy: 0.7913 - val_loss: 0.7378 - val_accuracy: 0.7694\n",
      "Epoch 246/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5959 - accuracy: 0.7960\n",
      "Epoch 00246: val_loss did not improve from 0.73683\n",
      "22/22 [==============================] - 16s 712ms/step - loss: 0.5959 - accuracy: 0.7960 - val_loss: 0.7378 - val_accuracy: 0.7717\n",
      "Epoch 247/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.7862\n",
      "Epoch 00247: val_loss improved from 0.73683 to 0.72984, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 17s 770ms/step - loss: 0.6074 - accuracy: 0.7862 - val_loss: 0.7298 - val_accuracy: 0.7727\n",
      "Epoch 248/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5853 - accuracy: 0.7947\n",
      "Epoch 00248: val_loss did not improve from 0.72984\n",
      "22/22 [==============================] - 18s 834ms/step - loss: 0.5853 - accuracy: 0.7947 - val_loss: 0.7348 - val_accuracy: 0.7661\n",
      "Epoch 249/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5962 - accuracy: 0.7958\n",
      "Epoch 00249: val_loss did not improve from 0.72984\n",
      "22/22 [==============================] - 19s 840ms/step - loss: 0.5962 - accuracy: 0.7958 - val_loss: 0.7334 - val_accuracy: 0.7732\n",
      "Epoch 250/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.7930\n",
      "Epoch 00250: val_loss did not improve from 0.72984\n",
      "22/22 [==============================] - 19s 849ms/step - loss: 0.5999 - accuracy: 0.7930 - val_loss: 0.7446 - val_accuracy: 0.7632\n",
      "Epoch 251/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5874 - accuracy: 0.7977\n",
      "Epoch 00251: val_loss did not improve from 0.72984\n",
      "22/22 [==============================] - 15s 689ms/step - loss: 0.5874 - accuracy: 0.7977 - val_loss: 0.7317 - val_accuracy: 0.7694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5949 - accuracy: 0.7888\n",
      "Epoch 00252: val_loss did not improve from 0.72984\n",
      "22/22 [==============================] - 20s 907ms/step - loss: 0.5949 - accuracy: 0.7888 - val_loss: 0.7340 - val_accuracy: 0.7647\n",
      "Epoch 253/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5843 - accuracy: 0.8003\n",
      "Epoch 00253: val_loss did not improve from 0.72984\n",
      "22/22 [==============================] - 16s 707ms/step - loss: 0.5843 - accuracy: 0.8003 - val_loss: 0.7369 - val_accuracy: 0.7684\n",
      "Epoch 254/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.7977\n",
      "Epoch 00254: val_loss improved from 0.72984 to 0.72878, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 720ms/step - loss: 0.5778 - accuracy: 0.7977 - val_loss: 0.7288 - val_accuracy: 0.7732\n",
      "Epoch 255/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5898 - accuracy: 0.7951\n",
      "Epoch 00255: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 16s 722ms/step - loss: 0.5898 - accuracy: 0.7951 - val_loss: 0.7352 - val_accuracy: 0.7717\n",
      "Epoch 256/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5873 - accuracy: 0.7957\n",
      "Epoch 00256: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 16s 741ms/step - loss: 0.5873 - accuracy: 0.7957 - val_loss: 0.7463 - val_accuracy: 0.7661\n",
      "Epoch 257/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5782 - accuracy: 0.7982\n",
      "Epoch 00257: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 17s 754ms/step - loss: 0.5782 - accuracy: 0.7982 - val_loss: 0.7458 - val_accuracy: 0.7717\n",
      "Epoch 258/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5665 - accuracy: 0.8061\n",
      "Epoch 00258: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 20s 909ms/step - loss: 0.5665 - accuracy: 0.8061 - val_loss: 0.7357 - val_accuracy: 0.7732\n",
      "Epoch 259/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5843 - accuracy: 0.7969\n",
      "Epoch 00259: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 18s 827ms/step - loss: 0.5843 - accuracy: 0.7969 - val_loss: 0.7351 - val_accuracy: 0.7717\n",
      "Epoch 260/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5883 - accuracy: 0.7979\n",
      "Epoch 00260: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 20s 898ms/step - loss: 0.5883 - accuracy: 0.7979 - val_loss: 0.7352 - val_accuracy: 0.7755\n",
      "Epoch 261/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5936 - accuracy: 0.7941\n",
      "Epoch 00261: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 20s 903ms/step - loss: 0.5936 - accuracy: 0.7941 - val_loss: 0.7347 - val_accuracy: 0.7713\n",
      "Epoch 262/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5916 - accuracy: 0.7928\n",
      "Epoch 00262: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 17s 752ms/step - loss: 0.5916 - accuracy: 0.7928 - val_loss: 0.7349 - val_accuracy: 0.7722\n",
      "Epoch 263/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5811 - accuracy: 0.7957\n",
      "Epoch 00263: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 17s 754ms/step - loss: 0.5811 - accuracy: 0.7957 - val_loss: 0.7376 - val_accuracy: 0.7665\n",
      "Epoch 264/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.7991\n",
      "Epoch 00264: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 15s 668ms/step - loss: 0.5754 - accuracy: 0.7991 - val_loss: 0.7347 - val_accuracy: 0.7765\n",
      "Epoch 265/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.8004\n",
      "Epoch 00265: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 16s 737ms/step - loss: 0.5756 - accuracy: 0.8004 - val_loss: 0.7316 - val_accuracy: 0.7732\n",
      "Epoch 266/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5832 - accuracy: 0.7949\n",
      "Epoch 00266: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 15s 662ms/step - loss: 0.5832 - accuracy: 0.7949 - val_loss: 0.7325 - val_accuracy: 0.7736\n",
      "Epoch 267/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5717 - accuracy: 0.8032\n",
      "Epoch 00267: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 15s 666ms/step - loss: 0.5717 - accuracy: 0.8032 - val_loss: 0.7474 - val_accuracy: 0.7689\n",
      "Epoch 268/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5839 - accuracy: 0.8055\n",
      "Epoch 00268: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 16s 745ms/step - loss: 0.5839 - accuracy: 0.8055 - val_loss: 0.7295 - val_accuracy: 0.7817\n",
      "Epoch 269/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.7960\n",
      "Epoch 00269: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 16s 706ms/step - loss: 0.5784 - accuracy: 0.7960 - val_loss: 0.7333 - val_accuracy: 0.7784\n",
      "Epoch 270/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5834 - accuracy: 0.7993\n",
      "Epoch 00270: val_loss did not improve from 0.72878\n",
      "22/22 [==============================] - 15s 669ms/step - loss: 0.5834 - accuracy: 0.7993 - val_loss: 0.7352 - val_accuracy: 0.7741\n",
      "Epoch 271/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5690 - accuracy: 0.8073\n",
      "Epoch 00271: val_loss improved from 0.72878 to 0.72827, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 18s 817ms/step - loss: 0.5690 - accuracy: 0.8073 - val_loss: 0.7283 - val_accuracy: 0.7727\n",
      "Epoch 272/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.8083\n",
      "Epoch 00272: val_loss did not improve from 0.72827\n",
      "22/22 [==============================] - 15s 681ms/step - loss: 0.5644 - accuracy: 0.8083 - val_loss: 0.7360 - val_accuracy: 0.7717\n",
      "Epoch 273/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5868 - accuracy: 0.7957\n",
      "Epoch 00273: val_loss did not improve from 0.72827\n",
      "22/22 [==============================] - 19s 851ms/step - loss: 0.5868 - accuracy: 0.7957 - val_loss: 0.7357 - val_accuracy: 0.7774\n",
      "Epoch 274/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5700 - accuracy: 0.7993\n",
      "Epoch 00274: val_loss did not improve from 0.72827\n",
      "22/22 [==============================] - 16s 734ms/step - loss: 0.5700 - accuracy: 0.7993 - val_loss: 0.7433 - val_accuracy: 0.7732\n",
      "Epoch 275/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5595 - accuracy: 0.8031\n",
      "Epoch 00275: val_loss did not improve from 0.72827\n",
      "22/22 [==============================] - 17s 783ms/step - loss: 0.5595 - accuracy: 0.8031 - val_loss: 0.7304 - val_accuracy: 0.7741\n",
      "Epoch 276/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5676 - accuracy: 0.8031\n",
      "Epoch 00276: val_loss did not improve from 0.72827\n",
      "22/22 [==============================] - 16s 698ms/step - loss: 0.5676 - accuracy: 0.8031 - val_loss: 0.7306 - val_accuracy: 0.7760\n",
      "Epoch 277/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5698 - accuracy: 0.8047\n",
      "Epoch 00277: val_loss did not improve from 0.72827\n",
      "22/22 [==============================] - 15s 687ms/step - loss: 0.5698 - accuracy: 0.8047 - val_loss: 0.7347 - val_accuracy: 0.7765\n",
      "Epoch 278/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5592 - accuracy: 0.8048\n",
      "Epoch 00278: val_loss did not improve from 0.72827\n",
      "22/22 [==============================] - 14s 648ms/step - loss: 0.5592 - accuracy: 0.8048 - val_loss: 0.7397 - val_accuracy: 0.7755\n",
      "Epoch 279/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.8039\n",
      "Epoch 00279: val_loss did not improve from 0.72827\n",
      "22/22 [==============================] - 16s 721ms/step - loss: 0.5645 - accuracy: 0.8039 - val_loss: 0.7419 - val_accuracy: 0.7698\n",
      "Epoch 280/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5566 - accuracy: 0.8083\n",
      "Epoch 00280: val_loss did not improve from 0.72827\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.5566 - accuracy: 0.8083 - val_loss: 0.7400 - val_accuracy: 0.7689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5488 - accuracy: 0.8140\n",
      "Epoch 00281: val_loss improved from 0.72827 to 0.72367, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 17s 759ms/step - loss: 0.5488 - accuracy: 0.8140 - val_loss: 0.7237 - val_accuracy: 0.7765\n",
      "Epoch 282/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5662 - accuracy: 0.8037\n",
      "Epoch 00282: val_loss did not improve from 0.72367\n",
      "22/22 [==============================] - 17s 761ms/step - loss: 0.5662 - accuracy: 0.8037 - val_loss: 0.7361 - val_accuracy: 0.7755\n",
      "Epoch 283/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5717 - accuracy: 0.8061\n",
      "Epoch 00283: val_loss did not improve from 0.72367\n",
      "22/22 [==============================] - 16s 741ms/step - loss: 0.5717 - accuracy: 0.8061 - val_loss: 0.7312 - val_accuracy: 0.7774\n",
      "Epoch 284/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.8072\n",
      "Epoch 00284: val_loss did not improve from 0.72367\n",
      "22/22 [==============================] - 20s 897ms/step - loss: 0.5572 - accuracy: 0.8072 - val_loss: 0.7263 - val_accuracy: 0.7793\n",
      "Epoch 285/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.8086\n",
      "Epoch 00285: val_loss did not improve from 0.72367\n",
      "22/22 [==============================] - 17s 764ms/step - loss: 0.5607 - accuracy: 0.8086 - val_loss: 0.7281 - val_accuracy: 0.7779\n",
      "Epoch 286/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5557 - accuracy: 0.8080\n",
      "Epoch 00286: val_loss did not improve from 0.72367\n",
      "22/22 [==============================] - 19s 876ms/step - loss: 0.5557 - accuracy: 0.8080 - val_loss: 0.7275 - val_accuracy: 0.7784\n",
      "Epoch 287/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.8088\n",
      "Epoch 00287: val_loss did not improve from 0.72367\n",
      "22/22 [==============================] - 15s 691ms/step - loss: 0.5569 - accuracy: 0.8088 - val_loss: 0.7240 - val_accuracy: 0.7769\n",
      "Epoch 288/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.8042\n",
      "Epoch 00288: val_loss did not improve from 0.72367\n",
      "22/22 [==============================] - 14s 651ms/step - loss: 0.5623 - accuracy: 0.8042 - val_loss: 0.7345 - val_accuracy: 0.7765\n",
      "Epoch 289/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5553 - accuracy: 0.8050\n",
      "Epoch 00289: val_loss did not improve from 0.72367\n",
      "22/22 [==============================] - 15s 669ms/step - loss: 0.5553 - accuracy: 0.8050 - val_loss: 0.7315 - val_accuracy: 0.7755\n",
      "Epoch 290/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5568 - accuracy: 0.8014\n",
      "Epoch 00290: val_loss did not improve from 0.72367\n",
      "22/22 [==============================] - 17s 765ms/step - loss: 0.5568 - accuracy: 0.8014 - val_loss: 0.7352 - val_accuracy: 0.7765\n",
      "Epoch 291/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5565 - accuracy: 0.8099\n",
      "Epoch 00291: val_loss improved from 0.72367 to 0.72288, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 15s 682ms/step - loss: 0.5565 - accuracy: 0.8099 - val_loss: 0.7229 - val_accuracy: 0.7788\n",
      "Epoch 292/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5456 - accuracy: 0.8114\n",
      "Epoch 00292: val_loss did not improve from 0.72288\n",
      "22/22 [==============================] - 16s 714ms/step - loss: 0.5456 - accuracy: 0.8114 - val_loss: 0.7275 - val_accuracy: 0.7774\n",
      "Epoch 293/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5484 - accuracy: 0.8111\n",
      "Epoch 00293: val_loss improved from 0.72288 to 0.71805, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 16s 723ms/step - loss: 0.5484 - accuracy: 0.8111 - val_loss: 0.7181 - val_accuracy: 0.7784\n",
      "Epoch 294/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.8097\n",
      "Epoch 00294: val_loss did not improve from 0.71805\n",
      "22/22 [==============================] - 20s 899ms/step - loss: 0.5569 - accuracy: 0.8097 - val_loss: 0.7203 - val_accuracy: 0.7854\n",
      "Epoch 295/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5722 - accuracy: 0.8042\n",
      "Epoch 00295: val_loss did not improve from 0.71805\n",
      "22/22 [==============================] - 15s 680ms/step - loss: 0.5722 - accuracy: 0.8042 - val_loss: 0.7247 - val_accuracy: 0.7784\n",
      "Epoch 296/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5478 - accuracy: 0.8144\n",
      "Epoch 00296: val_loss did not improve from 0.71805\n",
      "22/22 [==============================] - 14s 656ms/step - loss: 0.5478 - accuracy: 0.8144 - val_loss: 0.7335 - val_accuracy: 0.7769\n",
      "Epoch 297/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5436 - accuracy: 0.8111\n",
      "Epoch 00297: val_loss did not improve from 0.71805\n",
      "22/22 [==============================] - 15s 663ms/step - loss: 0.5436 - accuracy: 0.8111 - val_loss: 0.7341 - val_accuracy: 0.7765\n",
      "Epoch 298/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5531 - accuracy: 0.8048\n",
      "Epoch 00298: val_loss did not improve from 0.71805\n",
      "22/22 [==============================] - 14s 644ms/step - loss: 0.5531 - accuracy: 0.8048 - val_loss: 0.7305 - val_accuracy: 0.7750\n",
      "Epoch 299/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5446 - accuracy: 0.8069\n",
      "Epoch 00299: val_loss improved from 0.71805 to 0.70381, saving model to weights17.best.hdf5\n",
      "22/22 [==============================] - 14s 640ms/step - loss: 0.5446 - accuracy: 0.8069 - val_loss: 0.7038 - val_accuracy: 0.7850\n",
      "Epoch 300/300\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.5600 - accuracy: 0.8029\n",
      "Epoch 00300: val_loss did not improve from 0.70381\n",
      "22/22 [==============================] - 14s 648ms/step - loss: 0.5600 - accuracy: 0.8029 - val_loss: 0.7336 - val_accuracy: 0.7769\n",
      "Training completed in time:  1:17:37.877504\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False)\n",
    "checkpointer = ModelCheckpoint(filepath='weights17.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "model.fit(X_train2, y_train,\n",
    "          batch_size = 300, \n",
    "          epochs = 300,\n",
    "          validation_data = (X_val2, y_val), \n",
    "          callbacks=[checkpointer, es],verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "F_Modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
